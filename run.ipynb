{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path='/root/projects/mmcot/gqa/train_all_questions/train_all_questions_0.json'\n",
    "path_bal='/root/projects/mmcot/gqa/val_balanced_questions.json'\n",
    "path_bal='/root/projects/mmcot/gqa/testdev_balanced_questions.json'\n",
    "# with open(path,'r') as f:\n",
    "#     data=json.load(f)\n",
    "with open(path_bal,'r') as f:\n",
    "    data_bal=json.load(f)\n",
    "\n",
    "cnt=0\n",
    "infos=[]\n",
    "\n",
    "for j,(key, value) in enumerate(data_bal.items()):\n",
    "\n",
    "    # if check_nouns(value['question']):\n",
    "    if j%6==0 and cnt<600: \n",
    "        cnt+=1\n",
    "        continue\n",
    "    if j%6==0 and cnt<800:\n",
    "        info={}\n",
    "        cnt+=1\n",
    "\n",
    "        \n",
    "        info['question']=value['question']\n",
    "        info['answer']=value['answer']\n",
    "        info['imageId']=value['imageId']\n",
    "        infos.append(info)\n",
    "# import pickle\n",
    "# with open('/root/projects/code_lihan/Open_vocab_det/QAI.pkl','wb') as f:\n",
    "#     pickle.dump(infos,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "# with open(\"/root/projects/code_lihan/experiment_questions_data/QAI.pkl\",'rb') as f:\n",
    "#     data=pickle.load(f)\n",
    "# print(len(data))\n",
    "questions=[]\n",
    "imageIds=[]\n",
    "answers=[]\n",
    "for i in range(200):\n",
    "    d=infos[i]\n",
    "    questions.append(d['question'])\n",
    "    imageIds.append(d['imageId']) \n",
    "    answers.append(d['answer']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate nodes according to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Entity and Question ===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/root/projects/code_lihan/')\n",
    "import graph_generation\n",
    "import importlib\n",
    "importlib.reload(graph_generation)\n",
    "from graph_generation import entity_generation\n",
    "nodes,_=entity_generation(questions)\n",
    "# tmpN=[]\n",
    "# tmpQ=[]\n",
    "# tmpI=[]\n",
    "# tmpA=[]\n",
    "# for i in range(len(nodes)):\n",
    "#     node=nodes[i]\n",
    "#     question=questions[i]\n",
    "#     imageId=imageIds[i]\n",
    "#     answer=answers[i]\n",
    "#     if len(node)>=0:\n",
    "#         tmpN.append(node)\n",
    "#         tmpQ.append(question)\n",
    "#         tmpI.append(imageId)\n",
    "#         tmpA.append(answer)\n",
    "# nodes=tmpN\n",
    "# questions=tmpQ\n",
    "# answers=tmpA\n",
    "# imageIds=tmpI\n",
    "# # for i in range(len(nodes)):\n",
    "# for i in range(20):\n",
    "#     node=nodes[i]\n",
    "#     question=questions[i]\n",
    "#     imageId=imageIds[i]\n",
    "#     answer=answers[i]\n",
    "#     print(node,question,imageId,answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=600\n",
    "with open(f'{num}-{num+200}_Q.txt','w') as f:\n",
    "    for q in questions:\n",
    "        f.write(q+'\\n')\n",
    "# with open('MoreThanTwoGoodImg.txt','w') as f:\n",
    "with open(f'{num}-{num+200}_Img.txt','w') as f:\n",
    "    for q in imageIds:\n",
    "        f.write(q+'\\n')\n",
    "with open(f'{num}-{num+200}_A.txt','w') as f:\n",
    "    for a in answers:\n",
    "        f.write(a+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'{num}-{num+200}_nodes.json',\"w\") as f:\n",
    "    json.dump(nodes,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['person', 'playing'],\n",
       " ['umbrella'],\n",
       " ['bookcase', 'desk'],\n",
       " ['furniture', 'spoon', 'her'],\n",
       " ['wristband'],\n",
       " ['basket', 'train'],\n",
       " ['animal', 'grass'],\n",
       " ['person', 'woman'],\n",
       " ['furniture'],\n",
       " ['vehicle'],\n",
       " ['curtain'],\n",
       " ['stairs'],\n",
       " ['woman', 'bag'],\n",
       " ['beans'],\n",
       " ['lamps', 'mirrors'],\n",
       " ['device', 'speaker'],\n",
       " ['sidewalk', 'concrete'],\n",
       " ['person', 'crouching person'],\n",
       " ['jeans', 'sneakers'],\n",
       " ['animal'],\n",
       " ['building', 'sign'],\n",
       " ['old woman', 'man', 'giraffe'],\n",
       " ['man'],\n",
       " ['pants'],\n",
       " ['mountains'],\n",
       " ['helmet', 'bench'],\n",
       " ['person', 'bending', 'resting', 'reading'],\n",
       " ['red bricks'],\n",
       " ['bottle cap'],\n",
       " ['drink', 'cup'],\n",
       " ['telephone pole', 'train', 'bus'],\n",
       " ['curtains'],\n",
       " ['appliance', 'knives'],\n",
       " ['lamp', 'metal'],\n",
       " ['motorcycle', 'person'],\n",
       " ['ground'],\n",
       " ['soccer player', 'goal', 'ball'],\n",
       " ['fast food', 'mug'],\n",
       " ['vegetable', 'stove top'],\n",
       " ['appliance', 'oil', 'wine'],\n",
       " ['pasture', 'horse'],\n",
       " ['towel'],\n",
       " ['glass', 'glass', 'object'],\n",
       " ['couch', 'cloth'],\n",
       " ['skiing'],\n",
       " ['person', 'shirt'],\n",
       " ['person'],\n",
       " ['oven', 'stove'],\n",
       " ['leaning object', 'glass window'],\n",
       " ['meat'],\n",
       " ['baseball mitt'],\n",
       " ['notebooks', 'cans'],\n",
       " ['man', 'woman', 'sitting'],\n",
       " ['black speaker'],\n",
       " ['trousers'],\n",
       " ['red tables', 'ottomans'],\n",
       " ['doll', 'ladder', 'plastic'],\n",
       " ['jersey', 'cap'],\n",
       " ['glasses'],\n",
       " ['shelves'],\n",
       " ['coat'],\n",
       " ['trees', 'rock'],\n",
       " ['sky'],\n",
       " ['curtain', 'pillow'],\n",
       " ['polo shirt'],\n",
       " ['material', 'laptop', 'glass'],\n",
       " ['soap dispenser'],\n",
       " ['toy', 'girl'],\n",
       " ['faucet', 'light fixture'],\n",
       " ['woman', 'middle', 'top'],\n",
       " ['beds', 'lamps'],\n",
       " ['she', 'pillow'],\n",
       " ['furniture'],\n",
       " ['flower', 'surfboard'],\n",
       " ['mouse', 'keyboard'],\n",
       " ['men', 'vegetable'],\n",
       " ['flowers'],\n",
       " ['ceiling', 'plate'],\n",
       " ['notebook', 'napkin'],\n",
       " ['fence'],\n",
       " ['computer', 'Wii controller'],\n",
       " ['furniture'],\n",
       " ['skateboarder', 'lamp'],\n",
       " ['animal', 'grass'],\n",
       " ['man'],\n",
       " ['person', 'trash can'],\n",
       " ['road', 'building'],\n",
       " ['whipped cream'],\n",
       " ['device', 'keyboard'],\n",
       " [],\n",
       " ['fire hydrants', 'sidewalk'],\n",
       " ['driver'],\n",
       " ['bat', 'gray', 'red', 'wood'],\n",
       " ['pillows', 'towels'],\n",
       " ['person', 'blond', 'brunette'],\n",
       " ['light fixture', 'stop sign'],\n",
       " ['bath tub'],\n",
       " ['road', 'asphalt', 'cobblestone'],\n",
       " ['towel'],\n",
       " ['appliance', 'plastic'],\n",
       " ['dessert'],\n",
       " ['outlets', 'wood'],\n",
       " ['person', 'other person'],\n",
       " ['pot', 'motorbike', 'stone'],\n",
       " ['device'],\n",
       " ['teddy bear'],\n",
       " ['thin person'],\n",
       " ['man', 'skateboard'],\n",
       " ['doll'],\n",
       " ['boy'],\n",
       " ['animal', 'horses'],\n",
       " [],\n",
       " ['skateboard'],\n",
       " ['person', 'motorbike'],\n",
       " ['drink', 'top'],\n",
       " ['oil'],\n",
       " ['luggage'],\n",
       " ['floor'],\n",
       " ['onion', 'serving dish'],\n",
       " ['shoes'],\n",
       " ['elephant'],\n",
       " ['sign', 'road'],\n",
       " ['chair'],\n",
       " ['boy'],\n",
       " ['animal', 'male person'],\n",
       " ['furniture', 'white table'],\n",
       " ['trousers'],\n",
       " ['t-shirt'],\n",
       " ['table', 'top'],\n",
       " ['people', 'dog'],\n",
       " ['she', 'bottle'],\n",
       " ['woman', 'vehicles'],\n",
       " ['fence'],\n",
       " ['shirt'],\n",
       " ['bath tub'],\n",
       " ['window'],\n",
       " ['kid', 'skateboarder'],\n",
       " ['person', 'hat'],\n",
       " ['pipe'],\n",
       " ['round bowl', 'man', 'left', 'right'],\n",
       " ['table'],\n",
       " ['white animal'],\n",
       " ['truck', 'street'],\n",
       " ['umbrella', 'other umbrella'],\n",
       " ['fork'],\n",
       " ['umpire', 'person', 'racket'],\n",
       " ['glasses', 'remote control'],\n",
       " ['bed', 'cloth', 'leather'],\n",
       " ['fried food', 'top', 'bottom'],\n",
       " ['appliance', 'table', 'black color'],\n",
       " ['sink', 'mirror'],\n",
       " ['hat'],\n",
       " ['person'],\n",
       " ['chickens', 'trash cans'],\n",
       " ['flower'],\n",
       " ['shirt'],\n",
       " ['tall windows', 'white color'],\n",
       " ['floor'],\n",
       " ['napkin', 'plate'],\n",
       " ['toilet brush'],\n",
       " ['grassy field'],\n",
       " ['flag', 'fire truck'],\n",
       " ['clothing'],\n",
       " ['spoon', 'fork', 'plastic'],\n",
       " ['sky'],\n",
       " ['trees'],\n",
       " ['shape', 'swimming pool', 'man'],\n",
       " ['animal', 'fence'],\n",
       " ['furniture', 'mirror', 'television'],\n",
       " ['road'],\n",
       " ['window'],\n",
       " ['jacket'],\n",
       " ['pink umbrella'],\n",
       " ['chair', 'soccer ball'],\n",
       " ['bottle', 'napkin'],\n",
       " ['glazed food'],\n",
       " ['bed'],\n",
       " ['clothing'],\n",
       " ['kite', 'girl', 'round', 'small'],\n",
       " ['man', 'crate', 'beer'],\n",
       " ['young boy', 'jacket'],\n",
       " ['person'],\n",
       " ['furniture', 'plate'],\n",
       " ['red bench', 'painting'],\n",
       " ['man', 'plants'],\n",
       " ['pants'],\n",
       " ['device', 'water bottle'],\n",
       " ['item of furniture', 'storage box'],\n",
       " ['fences', 'bus'],\n",
       " ['chair'],\n",
       " ['helmet'],\n",
       " ['elephant'],\n",
       " ['person'],\n",
       " ['cheese', 'spoon'],\n",
       " ['animal'],\n",
       " ['pasture', 'grassy'],\n",
       " ['glasses'],\n",
       " ['window', 'train'],\n",
       " ['toilets', 'wall'],\n",
       " ['fences', 'bench', 'man']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import json\n",
    "num=600\n",
    "with open(f'{num}-{num+200}_nodes.json', 'r') as file:\n",
    "    nodes = json.load(file)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "with open(f'{num}-{num+200}_Q.txt','r') as f:\n",
    "# with open('MoreThanTwoGoodQ.txt','r') as f:\n",
    "    questions=[]\n",
    "    for line in f:\n",
    "        questions.append(line[:-1])\n",
    "print(len(questions))\n",
    "# with open('MoreThanTwoGoodImg.txt','r') as f:\n",
    "with open(f'{num}-{num+200}_Img.txt','r') as f:\n",
    "    imageIds=[]\n",
    "    for line in f:\n",
    "        imageIds.append(line[:-1])\n",
    "print(len(imageIds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n",
      "==========loading model successfully=============\n",
      "img_name:  0_n314171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxes_filt-- tensor([[  0.0000, 386.6590, 320.5542, 427.0000]])\n",
      "pred_phrase-- ['red table(0.56)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[176.0974, 226.0165, 217.2541, 263.7041],\n",
      "        [134.9921, 226.7801, 176.3076, 294.6703],\n",
      "        [167.3370, 256.6122, 243.7745, 283.2394],\n",
      "        [ 34.0076, 415.3821,  70.9305, 427.0000],\n",
      "        [208.4385, 225.2156, 237.2335, 250.4910],\n",
      "        [210.6879, 218.0341, 256.4754, 244.8178],\n",
      "        [ 63.7150, 417.0414, 105.2563, 427.0000],\n",
      "        [240.0275, 226.0587, 279.6311, 246.0557]])\n",
      "pred_phrase-- ['bottle(0.34)', 'bottle(0.34)', 'bottle(0.34)', 'bottle(0.32)', 'bottle(0.32)', 'bottle(0.31)', 'bottle(0.30)', 'bottle(0.27)']\n",
      "=======================\n",
      "after sort [(321, 0, 0), (160, 176, 1), (208, 134, 2), (256, 167, 3), (369, 34, 4), (112, 208, 5), (82, 210, 6), (417, 63, 7), (208, 240, 8)]\n",
      "Adjusted collect_boxes_filt [tensor([[  0.0000, 321.0000, 320.5542, 427.0000]]), tensor([[176.0974, 160.0000, 217.2541, 263.7041],\n",
      "        [134.9921, 208.0000, 176.3076, 294.6703],\n",
      "        [167.3370, 256.0000, 243.7745, 283.2394],\n",
      "        [ 34.0076, 369.0000,  70.9305, 427.0000],\n",
      "        [208.4385, 112.0000, 237.2335, 250.4910],\n",
      "        [210.6879,  82.0000, 256.4754, 244.8178],\n",
      "        [ 63.7150, 417.0000, 105.2563, 427.0000],\n",
      "        [240.0275, 208.0000, 279.6311, 246.0557]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'red table': {'red table_1': {'number': 1, 'bbox': [0, 321, 320, 427], 'score': 0.56}}, 'bottle': {'bottle_1': {'number': 2, 'bbox': [176, 160, 217, 263], 'score': 0.34}, 'bottle_2': {'number': 3, 'bbox': [134, 208, 176, 294], 'score': 0.34}, 'bottle_3': {'number': 4, 'bbox': [167, 256, 243, 283], 'score': 0.34}, 'bottle_4': {'number': 5, 'bbox': [34, 369, 70, 427], 'score': 0.32}, 'bottle_5': {'number': 6, 'bbox': [208, 112, 237, 250], 'score': 0.32}}})\n",
      "img_name:  1_n160664\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000, 486.7852, 250.5460]])\n",
      "pred_phrase-- ['animal(0.77)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[185.4751, 361.7746, 377.1751, 427.0000]])\n",
      "pred_phrase-- ['brick walkway(0.40)']\n",
      "=======================\n",
      "after sort [(0, 0, 0), (361, 185, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[  0.0000,   0.0000, 486.7852, 250.5460]]), tensor([[185.4751, 361.0000, 377.1751, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'animal': {'animal_1': {'number': 1, 'bbox': [0, 0, 486, 250], 'score': 0.77}}, 'brick walkway': {'brick walkway_1': {'number': 2, 'bbox': [185, 361, 377, 427], 'score': 0.4}}})\n",
      "img_name:  2_n400036\n",
      "boxes_filt-- tensor([[227.8682, 125.7275, 399.9127, 419.1111]])\n",
      "pred_phrase-- ['she(0.91)']\n",
      "=======================\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  frisbee\n",
      "obj_dict defaultdict(<class 'dict'>, {'she': {'she_1': {'number': 1, 'bbox': [227, 125, 399, 419], 'score': 0.91}}})\n",
      "img_name:  3_n400036\n",
      "boxes_filt-- tensor([[227.3352, 125.4263, 399.0215, 419.1887]])\n",
      "pred_phrase-- ['person(0.93)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[127.2099,   0.0000, 176.4935,  48.5278]])\n",
      "pred_phrase-- ['soccer ball(0.53)']\n",
      "=======================\n",
      "after sort [(125, 227, 0), (0, 127, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[227.3352, 125.0000, 399.0215, 419.1887]]), tensor([[127.2099,   0.0000, 176.4935,  48.5278]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'person': {'person_1': {'number': 1, 'bbox': [227, 125, 399, 419], 'score': 0.93}}, 'soccer ball': {'soccer ball_1': {'number': 2, 'bbox': [127, 0, 176, 48], 'score': 0.53}}})\n",
      "img_name:  4_n167552\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  shelf\n",
      "obj_dict defaultdict(<class 'dict'>, {})\n",
      "img_name:  5_n356822\n",
      "boxes_filt-- tensor([[  0.0000,  15.2315, 267.0972, 424.1144]])\n",
      "pred_phrase-- ['man(0.42)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'man': {'man_1': {'number': 1, 'bbox': [0, 15, 267, 424], 'score': 0.42}}})\n",
      "img_name:  6_n279173\n",
      "boxes_filt-- tensor([[ 79.8350,  79.2461, 265.3036, 418.9518]])\n",
      "pred_phrase-- ['he(0.54)']\n",
      "=======================\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  snowboard\n",
      "obj_dict defaultdict(<class 'dict'>, {'he': {'he_1': {'number': 1, 'bbox': [79, 79, 265, 418], 'score': 0.54}}})\n",
      "img_name:  7_n216553\n",
      "boxes_filt-- tensor([[  0.0000, 182.8731, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['ground(0.56)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'ground': {'ground_1': {'number': 1, 'bbox': [0, 182, 640, 427], 'score': 0.56}}})\n",
      "img_name:  8_n460556\n",
      "boxes_filt-- tensor([[552.2070, 323.5935, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['bicycles(0.53)']\n",
      "=======================\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  scooters\n",
      "obj_dict defaultdict(<class 'dict'>, {'bicycles': {'bicycles_1': {'number': 1, 'bbox': [552, 323, 640, 427], 'score': 0.53}}})\n",
      "img_name:  9_n530733\n",
      "boxes_filt-- tensor([[481.5352, 364.6611, 571.9811, 427.0000]])\n",
      "pred_phrase-- ['trash can(0.85)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[106.5684, 331.8451, 292.3210, 427.0000],\n",
      "        [  8.4166,  32.0121, 185.0561, 293.1018]])\n",
      "pred_phrase-- ['left(0.41)', 'left(0.33)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[331.7982, 270.9321, 443.7968, 320.4531]])\n",
      "pred_phrase-- ['right(0.46)']\n",
      "=======================\n",
      "after sort [(364, 481, 0), (331, 106, 1), (32, 8, 2), (270, 331, 3)]\n",
      "Adjusted collect_boxes_filt [tensor([[481.5352, 364.0000, 571.9811, 427.0000]]), tensor([[106.5684, 331.0000, 292.3210, 427.0000],\n",
      "        [  8.4166,  32.0000, 185.0561, 293.1018]]), tensor([[331.7982, 270.0000, 443.7968, 320.4531]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'trash can': {'trash can_1': {'number': 1, 'bbox': [481, 364, 571, 427], 'score': 0.85}}, 'left': {'left_1': {'number': 2, 'bbox': [106, 331, 292, 427], 'score': 0.41}, 'left_2': {'number': 3, 'bbox': [8, 32, 185, 293], 'score': 0.33}}, 'right': {'right_1': {'number': 4, 'bbox': [331, 270, 443, 320], 'score': 0.46}}})\n",
      "img_name:  10_n71728\n",
      "boxes_filt-- tensor([[322.0377, 267.5704, 379.7278, 299.7862],\n",
      "        [388.3712, 271.9533, 425.2626, 297.7090],\n",
      "        [262.7967, 269.2684, 300.6898, 291.9086],\n",
      "        [315.2015, 235.8677, 361.5109, 256.1232],\n",
      "        [398.7328, 258.9034, 427.5880, 278.5661]])\n",
      "pred_phrase-- ['bowl(0.45)', 'bowl(0.38)', 'bowl(0.29)', 'bowl(0.27)', 'bowl(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  0.0000, 206.2410, 191.6324, 427.0000]])\n",
      "pred_phrase-- ['bench(0.56)']\n",
      "=======================\n",
      "after sort [(221, 322, 0), (271, 388, 1), (269, 262, 2), (173, 315, 3), (223, 398, 4), (206, 0, 5)]\n",
      "Adjusted collect_boxes_filt [tensor([[322.0377, 221.0000, 379.7278, 299.7862],\n",
      "        [388.3712, 271.0000, 425.2626, 297.7090],\n",
      "        [262.7967, 269.0000, 300.6898, 291.9086],\n",
      "        [315.2015, 173.0000, 361.5109, 256.1232],\n",
      "        [398.7328, 223.0000, 427.5880, 278.5661]]), tensor([[  0.0000, 206.0000, 191.6324, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'bowl': {'bowl_1': {'number': 1, 'bbox': [322, 221, 379, 299], 'score': 0.45}, 'bowl_2': {'number': 2, 'bbox': [388, 271, 425, 297], 'score': 0.38}, 'bowl_3': {'number': 3, 'bbox': [262, 269, 300, 291], 'score': 0.29}, 'bowl_4': {'number': 4, 'bbox': [315, 173, 361, 256], 'score': 0.27}, 'bowl_5': {'number': 5, 'bbox': [398, 223, 427, 278], 'score': 0.26}}, 'bench': {'bench_1': {'number': 6, 'bbox': [0, 206, 191, 427], 'score': 0.56}}})\n",
      "img_name:  11_n154160\n",
      "boxes_filt-- tensor([[  0.0000,  16.3548, 409.7872, 426.8978]])\n",
      "pred_phrase-- ['umpire(0.71)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'umpire': {'umpire_1': {'number': 1, 'bbox': [0, 16, 409, 426], 'score': 0.71}}})\n",
      "img_name:  12_n578564\n",
      "boxes_filt-- tensor([[310.4093, 287.1515, 466.3677, 397.9310],\n",
      "        [478.8114,  98.4686, 640.0000, 237.3122],\n",
      "        [407.8422, 299.1113, 597.7391, 411.8877]])\n",
      "pred_phrase-- ['rolling pins(0.31)', 'rolling pins(0.25)', 'rolling pins(0.25)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[143.0100, 339.1819, 334.8932, 409.1836]])\n",
      "pred_phrase-- ['microwaves(0.70)']\n",
      "=======================\n",
      "after sort [(287, 310, 0), (98, 478, 1), (299, 407, 2), (339, 143, 3)]\n",
      "Adjusted collect_boxes_filt [tensor([[310.4093, 287.0000, 466.3677, 397.9310],\n",
      "        [478.8114,  98.0000, 640.0000, 237.3122],\n",
      "        [407.8422, 299.0000, 597.7391, 411.8877]]), tensor([[143.0100, 339.0000, 334.8932, 409.1836]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'rolling pins': {'rolling pins_1': {'number': 1, 'bbox': [310, 287, 466, 397], 'score': 0.31}, 'rolling pins_2': {'number': 2, 'bbox': [478, 98, 640, 237], 'score': 0.25}, 'rolling pins_3': {'number': 3, 'bbox': [407, 299, 597, 411], 'score': 0.25}}, 'microwaves': {'microwaves_1': {'number': 4, 'bbox': [143, 339, 334, 409], 'score': 0.7}}})\n",
      "img_name:  13_n355339\n",
      "boxes_filt-- tensor([[  0.0000, 303.3182, 640.0000, 427.0000],\n",
      "        [422.5532, 144.5174, 535.4302, 187.6830]])\n",
      "pred_phrase-- ['table(0.63)', 'table(0.27)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 61.4138,  75.8975, 397.4110, 356.8673],\n",
      "        [  0.0000,  33.1525, 115.1553, 200.9818],\n",
      "        [317.9333,  25.0383, 433.2733, 191.0243],\n",
      "        [ 75.4559,  35.7369, 194.4492, 164.4303]])\n",
      "pred_phrase-- ['people(0.44)', 'people(0.43)', 'people(0.42)', 'people(0.40)']\n",
      "=======================\n",
      "after sort [(303, 0, 0), (144, 422, 1), (75, 61, 2), (27, 0, 3), (25, 317, 4), (27, 75, 5)]\n",
      "Adjusted collect_boxes_filt [tensor([[  0.0000, 303.0000, 640.0000, 427.0000],\n",
      "        [422.5532, 144.0000, 535.4302, 187.6830]]), tensor([[ 61.4138,  75.0000, 397.4110, 356.8673],\n",
      "        [  0.0000,  27.0000, 115.1553, 200.9818],\n",
      "        [317.9333,  25.0000, 433.2733, 191.0243],\n",
      "        [ 75.4559,  27.0000, 194.4492, 164.4303]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'table': {'table_1': {'number': 1, 'bbox': [0, 303, 640, 427], 'score': 0.63}, 'table_2': {'number': 2, 'bbox': [422, 144, 535, 187], 'score': 0.27}}, 'people': {'people_1': {'number': 3, 'bbox': [61, 75, 397, 356], 'score': 0.44}, 'people_2': {'number': 4, 'bbox': [0, 27, 115, 200], 'score': 0.43}, 'people_3': {'number': 5, 'bbox': [317, 25, 433, 191], 'score': 0.42}, 'people_4': {'number': 6, 'bbox': [75, 27, 194, 164], 'score': 0.4}}})\n",
      "img_name:  14_n233607\n",
      "boxes_filt-- tensor([[523.2211, 102.3709, 640.0000, 190.7837]])\n",
      "pred_phrase-- ['computer(0.50)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'computer': {'computer_1': {'number': 1, 'bbox': [523, 102, 640, 190], 'score': 0.5}}})\n",
      "img_name:  15_n28792\n",
      "boxes_filt-- tensor([[119.2521,  79.6775, 279.6276, 198.2391],\n",
      "        [280.3826,  87.4924, 426.1234, 169.4488],\n",
      "        [  9.7503, 157.3650,  47.5176, 188.0621]])\n",
      "pred_phrase-- ['umbrella(0.54)', 'umbrella(0.53)', 'umbrella(0.28)']\n",
      "=======================\n",
      "after sort [(79, 119, 0), (87, 280, 1), (157, 9, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[119.2521,  79.0000, 279.6276, 198.2391],\n",
      "        [280.3826,  87.0000, 426.1234, 169.4488],\n",
      "        [  9.7503, 157.0000,  47.5176, 188.0621]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'umbrella': {'umbrella_1': {'number': 1, 'bbox': [119, 79, 279, 198], 'score': 0.54}, 'umbrella_2': {'number': 2, 'bbox': [280, 87, 426, 169], 'score': 0.53}, 'umbrella_3': {'number': 3, 'bbox': [9, 157, 47, 188], 'score': 0.28}}})\n",
      "img_name:  16_n88933\n",
      "boxes_filt-- tensor([[ 96.3055, 222.6196, 142.6584, 260.3547],\n",
      "        [218.0739, 246.2766, 256.5702, 277.2806],\n",
      "        [239.2333, 263.3714, 294.9650, 290.8026],\n",
      "        [364.1502, 232.4898, 403.8736, 260.1790],\n",
      "        [ 54.7993, 212.9150, 104.4454, 233.6136],\n",
      "        [383.4077, 253.6446, 421.0760, 277.8469],\n",
      "        [363.2762, 231.7344, 423.7283, 278.6968],\n",
      "        [217.4751, 245.8714, 296.7815, 290.8644]])\n",
      "pred_phrase-- ['food(0.55)', 'food(0.46)', 'food(0.45)', 'food(0.44)', 'food(0.43)', 'food(0.34)', 'food(0.32)', 'food(0.29)']\n",
      "=======================\n",
      "after sort [(222, 96, 0), (215, 218, 1), (263, 239, 2), (205, 364, 3), (174, 54, 4), (253, 383, 5)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 96.3055, 222.0000, 142.6584, 260.3547],\n",
      "        [218.0739, 215.0000, 256.5702, 277.2806],\n",
      "        [239.2333, 263.0000, 294.9650, 290.8026],\n",
      "        [364.1502, 205.0000, 403.8736, 260.1790],\n",
      "        [ 54.7993, 174.0000, 104.4454, 233.6136],\n",
      "        [383.4077, 253.0000, 421.0760, 277.8469]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'food': {'food_1': {'number': 1, 'bbox': [96, 222, 142, 260], 'score': 0.55}, 'food_2': {'number': 2, 'bbox': [218, 215, 256, 277], 'score': 0.46}, 'food_3': {'number': 3, 'bbox': [239, 263, 294, 290], 'score': 0.45}, 'food_4': {'number': 4, 'bbox': [364, 205, 403, 260], 'score': 0.44}, 'food_5': {'number': 5, 'bbox': [54, 174, 104, 233], 'score': 0.43}}})\n",
      "img_name:  17_n25275\n",
      "obj_dict {}\n",
      "img_name:  18_n24526\n",
      "boxes_filt-- tensor([[549.6086, 258.9896, 640.0000, 427.0000],\n",
      "        [129.4692,   0.0000, 553.4291, 426.2133]])\n",
      "pred_phrase-- ['woman(0.32)', 'woman(0.30)']\n",
      "=======================\n",
      "after sort [(258, 549, 0), (0, 129, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[549.6086, 258.0000, 640.0000, 427.0000],\n",
      "        [129.4692,   0.0000, 553.4291, 426.2133]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'woman': {'woman_1': {'number': 1, 'bbox': [549, 258, 640, 427], 'score': 0.32}, 'woman_2': {'number': 2, 'bbox': [129, 0, 553, 426], 'score': 0.3}}})\n",
      "img_name:  19_n501609\n",
      "boxes_filt-- tensor([[196.5935, 178.8685, 256.8998, 320.6056]])\n",
      "pred_phrase-- ['oven(0.37)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[371.1550, 234.1133, 436.4538, 252.3711]])\n",
      "pred_phrase-- ['stove(0.54)']\n",
      "=======================\n",
      "after sort [(178, 196, 0), (234, 371, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[196.5935, 178.0000, 256.8998, 320.6056]]), tensor([[371.1550, 234.0000, 436.4538, 252.3711]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'oven': {'oven_1': {'number': 1, 'bbox': [196, 178, 256, 320], 'score': 0.37}}, 'stove': {'stove_1': {'number': 2, 'bbox': [371, 234, 436, 252], 'score': 0.54}}})\n",
      "img_name:  20_n279581\n",
      "boxes_filt-- tensor([[105.2281,  87.8249, 266.7129, 357.6139]])\n",
      "pred_phrase-- ['shoes(0.29)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'shoes': {'shoes_1': {'number': 1, 'bbox': [105, 87, 266, 357], 'score': 0.29}}})\n",
      "img_name:  21_n49438\n",
      "boxes_filt-- tensor([[232.1411,  20.7788, 485.9492, 308.2369]])\n",
      "pred_phrase-- ['boys(0.80)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 10.3374,  16.0733, 126.9387, 199.1046]])\n",
      "pred_phrase-- ['tall dresser(0.80)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 10.8093, 141.0579, 632.4664, 420.2984]])\n",
      "pred_phrase-- ['bed(0.73)']\n",
      "=======================\n",
      "after sort [(20, 232, 0), (16, 10, 1), (141, 10, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[232.1411,  20.0000, 485.9492, 308.2369]]), tensor([[ 10.3374,  16.0000, 126.9387, 199.1046]]), tensor([[ 10.8093, 141.0000, 632.4664, 420.2984]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'boys': {'boys_1': {'number': 1, 'bbox': [232, 20, 485, 308], 'score': 0.8}}, 'tall dresser': {'tall dresser_1': {'number': 2, 'bbox': [10, 16, 126, 199], 'score': 0.8}}, 'bed': {'bed_1': {'number': 3, 'bbox': [10, 141, 632, 420], 'score': 0.73}}})\n",
      "img_name:  22_n315887\n",
      "boxes_filt-- tensor([[ 25.2177, 208.7204, 422.8220, 375.3333],\n",
      "        [ 22.6834,   0.0000, 467.0294, 215.1051]])\n",
      "pred_phrase-- ['device(0.38)', 'device(0.36)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[488.3286, 162.5478, 567.8344, 266.4443]])\n",
      "pred_phrase-- ['speaker(0.34)']\n",
      "=======================\n",
      "after sort [(208, 25, 0), (0, 22, 1), (162, 488, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 25.2177, 208.0000, 422.8220, 375.3333],\n",
      "        [ 22.6834,   0.0000, 467.0294, 215.1051]]), tensor([[488.3286, 162.0000, 567.8344, 266.4443]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'device': {'device_1': {'number': 1, 'bbox': [25, 208, 422, 375], 'score': 0.38}, 'device_2': {'number': 2, 'bbox': [22, 0, 467, 215], 'score': 0.36}}, 'speaker': {'speaker_1': {'number': 3, 'bbox': [488, 162, 567, 266], 'score': 0.34}}})\n",
      "img_name:  23_n489190\n",
      "boxes_filt-- tensor([[312.1042,  82.7505, 457.1720, 134.0938]])\n",
      "pred_phrase-- ['fence(0.25)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'fence': {'fence_1': {'number': 1, 'bbox': [312, 82, 457, 134], 'score': 0.25}}})\n",
      "img_name:  24_n184551\n",
      "boxes_filt-- tensor([[269.9009, 123.5190, 444.0305, 179.8309]])\n",
      "pred_phrase-- ['umbrella(0.53)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[255.9049, 160.8209, 365.4697, 320.1588]])\n",
      "pred_phrase-- ['man(0.90)']\n",
      "=======================\n",
      "after sort [(112, 269, 0), (160, 255, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[269.9009, 112.0000, 444.0305, 179.8309]]), tensor([[255.9049, 160.0000, 365.4697, 320.1588]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'umbrella': {'umbrella_1': {'number': 1, 'bbox': [269, 112, 444, 179], 'score': 0.53}}, 'man': {'man_1': {'number': 2, 'bbox': [255, 160, 365, 320], 'score': 0.9}}})\n",
      "img_name:  25_n278312\n",
      "boxes_filt-- tensor([[115.4484,  88.3051, 199.3884, 347.9662],\n",
      "        [258.0699,  93.0911, 356.2257, 181.3944],\n",
      "        [210.2119, 219.4334, 378.7580, 418.6259],\n",
      "        [404.1683, 218.1488, 496.2981, 269.9792],\n",
      "        [229.5005, 170.4570, 281.2083, 226.2003],\n",
      "        [210.2578, 218.3683, 378.2410, 279.7265],\n",
      "        [559.0586, 271.4391, 599.0406, 309.1738]])\n",
      "pred_phrase-- ['appliance(0.54)', 'appliance(0.47)', 'appliance(0.41)', 'appliance(0.30)', 'appliance(0.27)', 'appliance(0.26)', 'appliance(0.25)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[410.3403, 220.1852, 468.4603, 270.2816]])\n",
      "pred_phrase-- ['tea kettle(0.35)']\n",
      "=======================\n",
      "after sort [(88, 115, 0), (93, 258, 1), (172, 404, 2), (170, 229, 3), (218, 210, 4), (271, 559, 5), (220, 410, 6)]\n",
      "Adjusted collect_boxes_filt [tensor([[115.4484,  88.0000, 199.3884, 347.9662],\n",
      "        [258.0699,  93.0000, 356.2257, 181.3944],\n",
      "        [404.1683, 172.0000, 496.2981, 269.9792],\n",
      "        [229.5005, 170.0000, 281.2083, 226.2003],\n",
      "        [210.2578, 218.0000, 378.2410, 279.7265],\n",
      "        [559.0586, 271.0000, 599.0406, 309.1738]]), tensor([[410.3403, 220.0000, 468.4603, 270.2816]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'appliance': {'appliance_1': {'number': 1, 'bbox': [115, 88, 199, 347], 'score': 0.54}, 'appliance_2': {'number': 2, 'bbox': [258, 93, 356, 181], 'score': 0.47}, 'appliance_3': {'number': 3, 'bbox': [404, 172, 496, 269], 'score': 0.3}, 'appliance_4': {'number': 4, 'bbox': [229, 170, 281, 226], 'score': 0.27}, 'appliance_5': {'number': 5, 'bbox': [210, 218, 378, 279], 'score': 0.26}}, 'tea kettle': {'tea kettle_1': {'number': 6, 'bbox': [410, 220, 468, 270], 'score': 0.35}}})\n",
      "img_name:  26_n309148\n",
      "boxes_filt-- tensor([[111.7731,  97.6761, 565.9852, 402.4679],\n",
      "        [  0.0000, 185.4374, 131.7729, 351.0181],\n",
      "        [597.4391, 262.0460, 640.0000, 340.1862],\n",
      "        [552.1024, 308.0424, 589.2700, 335.8531],\n",
      "        [587.6110, 306.9257, 611.5577, 335.4427]])\n",
      "pred_phrase-- ['vehicle(0.71)', 'vehicle(0.42)', 'vehicle(0.30)', 'vehicle(0.27)', 'vehicle(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[597.1908, 272.1417, 636.6791, 340.5787],\n",
      "        [440.6385, 154.1936, 519.0759, 219.7041],\n",
      "        [190.3554,  88.0124, 218.5339, 117.1066]])\n",
      "pred_phrase-- ['pedestrian(0.52)', 'pedestrian(0.32)', 'pedestrian(0.27)']\n",
      "=======================\n",
      "after sort [(97, 111, 0), (185, 0, 1), (164, 597, 2), (308, 552, 3), (260, 587, 4), (212, 597, 5), (154, 440, 6), (88, 190, 7)]\n",
      "Adjusted collect_boxes_filt [tensor([[111.7731,  97.0000, 565.9852, 402.4679],\n",
      "        [  0.0000, 185.0000, 131.7729, 351.0181],\n",
      "        [597.4391, 164.0000, 640.0000, 340.1862],\n",
      "        [552.1024, 308.0000, 589.2700, 335.8531],\n",
      "        [587.6110, 260.0000, 611.5577, 335.4427]]), tensor([[597.1908, 212.0000, 636.6791, 340.5787],\n",
      "        [440.6385, 154.0000, 519.0759, 219.7041],\n",
      "        [190.3554,  88.0000, 218.5339, 117.1066]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'vehicle': {'vehicle_1': {'number': 1, 'bbox': [111, 97, 565, 402], 'score': 0.71}, 'vehicle_2': {'number': 2, 'bbox': [0, 185, 131, 351], 'score': 0.42}, 'vehicle_3': {'number': 3, 'bbox': [597, 164, 640, 340], 'score': 0.3}, 'vehicle_4': {'number': 4, 'bbox': [552, 308, 589, 335], 'score': 0.27}, 'vehicle_5': {'number': 5, 'bbox': [587, 260, 611, 335], 'score': 0.26}}, 'pedestrian': {'pedestrian_1': {'number': 6, 'bbox': [597, 212, 636, 340], 'score': 0.52}, 'pedestrian_2': {'number': 7, 'bbox': [440, 154, 519, 219], 'score': 0.32}, 'pedestrian_3': {'number': 8, 'bbox': [190, 88, 218, 117], 'score': 0.27}}})\n",
      "img_name:  27_n534106\n",
      "boxes_filt-- tensor([[ 87.2834,   0.0000, 394.4826, 427.0000]])\n",
      "pred_phrase-- ['woman(0.95)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'woman': {'woman_1': {'number': 1, 'bbox': [87, 0, 394, 427], 'score': 0.95}}})\n",
      "img_name:  28_n380113\n",
      "boxes_filt-- tensor([[440.1048, 264.6782, 497.5389, 322.1085]])\n",
      "pred_phrase-- ['soccer ball(0.56)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 60.4342,   0.0000, 225.0301, 244.8307]])\n",
      "pred_phrase-- ['red(0.76)']\n",
      "=======================\n",
      "after sort [(264, 440, 0), (0, 60, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[440.1048, 264.0000, 497.5389, 322.1085]]), tensor([[ 60.4342,   0.0000, 225.0301, 244.8307]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'soccer ball': {'soccer ball_1': {'number': 1, 'bbox': [440, 264, 497, 322], 'score': 0.56}}, 'red': {'red_1': {'number': 2, 'bbox': [60, 0, 225, 244], 'score': 0.76}}})\n",
      "img_name:  29_n234722\n",
      "boxes_filt-- tensor([[218.0477,  53.2031, 609.5505, 180.3517]])\n",
      "pred_phrase-- ['cooking utensil(0.41)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 18.0146,  35.5278, 629.4410, 396.5253]])\n",
      "pred_phrase-- ['pizza(0.71)']\n",
      "=======================\n",
      "after sort [(53, 218, 0), (35, 18, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[218.0477,  53.0000, 609.5505, 180.3517]]), tensor([[ 18.0146,  35.0000, 629.4410, 396.5253]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'cooking utensil': {'cooking utensil_1': {'number': 1, 'bbox': [218, 53, 609, 180], 'score': 0.41}}, 'pizza': {'pizza_1': {'number': 2, 'bbox': [18, 35, 629, 396], 'score': 0.71}}})\n",
      "img_name:  30_n518912\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000, 640.0000, 233.1849]])\n",
      "pred_phrase-- ['curtains(0.32)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'curtains': {'curtains_1': {'number': 1, 'bbox': [0, 0, 640, 233], 'score': 0.32}}})\n",
      "img_name:  31_n315887\n",
      "boxes_filt-- tensor([[565.2098, 276.2382, 640.0000, 382.1890]])\n",
      "pred_phrase-- ['telephone(0.46)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[423.8217, 247.3225, 512.0759, 320.2853]])\n",
      "pred_phrase-- ['mouse(0.48)']\n",
      "=======================\n",
      "after sort [(276, 565, 0), (247, 423, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[565.2098, 276.0000, 640.0000, 382.1890]]), tensor([[423.8217, 247.0000, 512.0759, 320.2853]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'telephone': {'telephone_1': {'number': 1, 'bbox': [565, 276, 640, 382], 'score': 0.46}}, 'mouse': {'mouse_1': {'number': 2, 'bbox': [423, 247, 512, 320], 'score': 0.48}}})\n",
      "img_name:  32_n9181\n",
      "boxes_filt-- tensor([[  0.0000,  53.8118, 123.4921, 215.4260],\n",
      "        [ 10.2297,  53.6091,  69.8482,  84.2742]])\n",
      "pred_phrase-- ['brown cowboy hat(0.40)', 'brown cowboy hat(0.30)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 26.2888,   9.1507,  59.4297,  42.8503],\n",
      "        [175.6062, 100.5705, 535.4097, 350.2052]])\n",
      "pred_phrase-- ['small sign(0.38)', 'small sign(0.25)']\n",
      "=======================\n",
      "after sort [(53, 10, 0), (5, 26, 1), (100, 175, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[10.2297, 53.0000, 69.8482, 84.2742]]), tensor([[ 26.2888,   5.0000,  59.4297,  42.8503],\n",
      "        [175.6062, 100.0000, 535.4097, 350.2052]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'brown cowboy hat': {'brown cowboy hat_1': {'number': 1, 'bbox': [10, 53, 69, 84], 'score': 0.3}}, 'small sign': {'small sign_1': {'number': 2, 'bbox': [26, 5, 59, 42], 'score': 0.38}, 'small sign_2': {'number': 3, 'bbox': [175, 100, 535, 350], 'score': 0.25}}})\n",
      "img_name:  33_n118102\n",
      "boxes_filt-- tensor([[418.1300, 253.0405, 585.8031, 417.9951],\n",
      "        [494.9925, 254.4941, 581.6599, 313.8551]])\n",
      "pred_phrase-- ['candles(0.33)', 'candles(0.28)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'candles': {'candles_1': {'number': 1, 'bbox': [494, 254, 581, 313], 'score': 0.28}}})\n",
      "img_name:  34_n83784\n",
      "boxes_filt-- tensor([[  0.0000, 193.3348, 165.0506, 297.7394]])\n",
      "pred_phrase-- ['small side table(0.50)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'small side table': {'small side table_1': {'number': 1, 'bbox': [0, 193, 165, 297], 'score': 0.5}}})\n",
      "img_name:  35_n530733\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000, 395.6072, 340.6439]])\n",
      "pred_phrase-- ['mirror(0.41)']\n",
      "=======================\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  sofa\n",
      "obj_dict defaultdict(<class 'dict'>, {'mirror': {'mirror_1': {'number': 1, 'bbox': [0, 0, 395, 340], 'score': 0.41}}})\n",
      "img_name:  36_n298104\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000, 640.0000, 121.5292]])\n",
      "pred_phrase-- ['sky(0.64)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'sky': {'sky_1': {'number': 1, 'bbox': [0, 0, 640, 121], 'score': 0.64}}})\n",
      "img_name:  37_n195925\n",
      "boxes_filt-- tensor([[  0.0000, 311.8894, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['water(0.61)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'water': {'water_1': {'number': 1, 'bbox': [0, 311, 640, 427], 'score': 0.61}}})\n",
      "img_name:  38_n125122\n",
      "boxes_filt-- tensor([[485.1566, 111.2880, 640.0000, 335.8984],\n",
      "        [  0.0000, 248.1055, 384.5319, 427.0000],\n",
      "        [ 64.6998, 161.1019, 411.1138, 302.0133],\n",
      "        [455.3922, 113.4102, 530.7580, 229.1015]])\n",
      "pred_phrase-- ['furniture(0.40)', 'furniture(0.40)', 'furniture(0.36)', 'furniture(0.30)']\n",
      "=======================\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  phone\n",
      "boxes_filt-- tensor([[485.0904, 109.6548, 640.0000, 335.8063],\n",
      "        [  0.0000, 252.8034, 384.4265, 427.0000]])\n",
      "pred_phrase-- ['table(0.60)', 'table(0.31)']\n",
      "=======================\n",
      "after sort [(65, 485, 0), (204, 0, 1), (156, 64, 2), (113, 455, 3), (17, 485, 4), (252, 0, 5)]\n",
      "Adjusted collect_boxes_filt [tensor([[485.1566,  65.0000, 640.0000, 335.8984],\n",
      "        [  0.0000, 204.0000, 384.5319, 427.0000],\n",
      "        [ 64.6998, 156.0000, 411.1138, 302.0133],\n",
      "        [455.3922, 113.0000, 530.7580, 229.1015]]), tensor([], size=(0, 4)), tensor([[485.0904,  17.0000, 640.0000, 335.8063],\n",
      "        [  0.0000, 252.0000, 384.4265, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'furniture': {'furniture_1': {'number': 1, 'bbox': [485, 65, 640, 335], 'score': 0.4}, 'furniture_2': {'number': 2, 'bbox': [0, 204, 384, 427], 'score': 0.4}, 'furniture_3': {'number': 3, 'bbox': [64, 156, 411, 302], 'score': 0.36}, 'furniture_4': {'number': 4, 'bbox': [455, 113, 530, 229], 'score': 0.3}}, 'table': {'table_1': {'number': 5, 'bbox': [485, 17, 640, 335], 'score': 0.6}, 'table_2': {'number': 6, 'bbox': [0, 252, 384, 427], 'score': 0.31}}})\n",
      "img_name:  39_n250821\n",
      "boxes_filt-- tensor([[373.4008, 109.9155, 640.0000, 253.5740]])\n",
      "pred_phrase-- ['cat(0.95)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'cat': {'cat_1': {'number': 1, 'bbox': [373, 109, 640, 253], 'score': 0.95}}})\n",
      "img_name:  40_n12404\n",
      "boxes_filt-- tensor([[256.5682, 321.9176, 299.4980, 353.9161],\n",
      "        [234.4199, 331.8595, 262.8279, 364.5678]])\n",
      "pred_phrase-- ['shoe(0.65)', 'shoe(0.63)']\n",
      "=======================\n",
      "after sort [(283, 256, 0), (331, 234, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[256.5682, 283.0000, 299.4980, 353.9161],\n",
      "        [234.4199, 331.0000, 262.8279, 364.5678]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'shoe': {'shoe_1': {'number': 1, 'bbox': [256, 283, 299, 353], 'score': 0.65}, 'shoe_2': {'number': 2, 'bbox': [234, 331, 262, 364], 'score': 0.63}}})\n",
      "img_name:  41_n119944\n",
      "boxes_filt-- tensor([[364.0117, 215.2674, 555.4058, 426.6947]])\n",
      "pred_phrase-- ['green shirt(0.92)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'green shirt': {'green shirt_1': {'number': 1, 'bbox': [364, 215, 555, 426], 'score': 0.92}}})\n",
      "img_name:  42_n196058\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  brush\n",
      "boxes_filt-- tensor([[ 27.8010, 109.9003, 630.2520, 414.5790]])\n",
      "pred_phrase-- ['horses(0.37)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'horses': {'horses_1': {'number': 1, 'bbox': [27, 109, 630, 414], 'score': 0.37}}})\n",
      "img_name:  43_n556604\n",
      "boxes_filt-- tensor([[119.0754, 276.0163, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['skate park(0.40)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[151.5361,  81.3424, 587.5280, 303.0614],\n",
      "        [252.3879, 253.5757, 282.6105, 292.7892],\n",
      "        [304.5218, 256.3433, 354.3326, 299.5999]])\n",
      "pred_phrase-- ['man(0.75)', 'man(0.26)', 'man(0.25)']\n",
      "=======================\n",
      "after sort [(276, 119, 0), (208, 252, 1), (256, 304, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[119.0754, 276.0000, 640.0000, 427.0000]]), tensor([[252.3879, 208.0000, 282.6105, 292.7892],\n",
      "        [304.5218, 256.0000, 354.3326, 299.5999]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'skate park': {'skate park_1': {'number': 1, 'bbox': [119, 276, 640, 427], 'score': 0.4}}, 'man': {'man_1': {'number': 2, 'bbox': [252, 208, 282, 292], 'score': 0.26}, 'man_2': {'number': 3, 'bbox': [304, 256, 354, 299], 'score': 0.25}}})\n",
      "img_name:  44_n35676\n",
      "boxes_filt-- tensor([[ 90.6988,  80.4550, 223.2358, 152.0687]])\n",
      "pred_phrase-- ['microwave(0.68)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'microwave': {'microwave_1': {'number': 1, 'bbox': [90, 80, 223, 152], 'score': 0.68}}})\n",
      "img_name:  45_n16936\n",
      "boxes_filt-- tensor([[ 97.2482,   0.0000, 172.1673, 247.2610],\n",
      "        [  3.7481,  18.6245,  80.8131, 218.5963],\n",
      "        [  3.4099,   0.0000, 241.7180, 247.9482]])\n",
      "pred_phrase-- ['snowboarders(0.43)', 'snowboarders(0.28)', 'snowboarders(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 97.0365,   0.0000, 172.3371, 247.2826],\n",
      "        [  3.6596,  18.5077,  81.3158, 218.6758],\n",
      "        [  3.2621,   0.0000, 241.4521, 248.1186]])\n",
      "pred_phrase-- ['skateboarders(0.41)', 'skateboarders(0.30)', 'skateboarders(0.27)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 98.3635,   0.0000, 171.4287, 247.2615]])\n",
      "pred_phrase-- ['jumping(0.66)']\n",
      "=======================\n",
      "after sort [(0, 97, 0), (0, 3, 1), (0, 97, 2), (18, 3, 3), (0, 98, 4)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 97.2482,   0.0000, 172.1673, 247.2610],\n",
      "        [  3.7481,   0.0000,  80.8131, 218.5963]]), tensor([[ 97.0365,   0.0000, 172.3371, 247.2826],\n",
      "        [  3.6596,  18.0000,  81.3158, 218.6758]]), tensor([[ 98.3635,   0.0000, 171.4287, 247.2615]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'snowboarders': {'snowboarders_1': {'number': 1, 'bbox': [97, 0, 172, 247], 'score': 0.43}, 'snowboarders_2': {'number': 2, 'bbox': [3, 0, 80, 218], 'score': 0.28}}, 'skateboarders': {'skateboarders_1': {'number': 3, 'bbox': [97, 0, 172, 247], 'score': 0.41}, 'skateboarders_2': {'number': 4, 'bbox': [3, 18, 81, 218], 'score': 0.3}}, 'jumping': {'jumping_1': {'number': 5, 'bbox': [98, 0, 171, 247], 'score': 0.66}}})\n",
      "img_name:  46_n507959\n",
      "boxes_filt-- tensor([[133.2247,  63.2261, 184.8751, 201.9770]])\n",
      "pred_phrase-- ['umbrellas(0.50)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[515.0646, 126.2961, 544.0811, 201.6369],\n",
      "        [536.8140, 132.5196, 563.0043, 195.3717],\n",
      "        [438.9817, 129.4202, 464.6733, 196.7188],\n",
      "        [460.8505, 127.9448, 488.1866, 194.5194],\n",
      "        [480.1319, 125.0986, 505.1055, 195.6065],\n",
      "        [282.2964, 124.3917, 361.4573, 251.4581],\n",
      "        [252.6341, 146.2938, 275.8555, 188.4259],\n",
      "        [328.3812, 123.5251, 391.0866, 257.5326],\n",
      "        [422.6418, 131.4904, 436.8640, 159.0168]])\n",
      "pred_phrase-- ['person(0.55)', 'person(0.53)', 'person(0.51)', 'person(0.50)', 'person(0.50)', 'person(0.45)', 'person(0.39)', 'person(0.37)', 'person(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[342.9090,   0.0000, 421.3015, 133.7885],\n",
      "        [124.7405,   0.0000, 273.6442,  88.8168],\n",
      "        [303.5874,   0.0000, 360.2824, 130.4315],\n",
      "        [286.9016,   4.8731, 337.0370,  94.0298]])\n",
      "pred_phrase-- ['palm(0.33)', 'palm(0.32)', 'palm(0.30)', 'palm(0.29)']\n",
      "=======================\n",
      "after sort [(63, 133, 0), (0, 515, 1), (132, 536, 2), (83, 438, 3), (35, 460, 4), (0, 480, 5), (98, 282, 6), (146, 252, 7), (50, 328, 8), (131, 422, 9), (0, 342, 10), (0, 124, 11), (0, 303, 12), (4, 286, 13)]\n",
      "Adjusted collect_boxes_filt [tensor([[133.2247,  63.0000, 184.8751, 201.9770]]), tensor([[515.0646,   0.0000, 544.0811, 201.6369],\n",
      "        [536.8140, 132.0000, 563.0043, 195.3717],\n",
      "        [438.9817,  83.0000, 464.6733, 196.7188],\n",
      "        [460.8505,  35.0000, 488.1866, 194.5194],\n",
      "        [480.1319,   0.0000, 505.1055, 195.6065],\n",
      "        [282.2964,  98.0000, 361.4573, 251.4581],\n",
      "        [252.6341, 146.0000, 275.8555, 188.4259],\n",
      "        [328.3812,  50.0000, 391.0866, 257.5326],\n",
      "        [422.6418, 131.0000, 436.8640, 159.0168]]), tensor([[342.9090,   0.0000, 421.3015, 133.7885],\n",
      "        [124.7405,   0.0000, 273.6442,  88.8168],\n",
      "        [303.5874,   0.0000, 360.2824, 130.4315],\n",
      "        [286.9016,   4.0000, 337.0370,  94.0298]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'umbrellas': {'umbrellas_1': {'number': 1, 'bbox': [133, 63, 184, 201], 'score': 0.5}}, 'person': {'person_1': {'number': 2, 'bbox': [515, 0, 544, 201], 'score': 0.55}, 'person_2': {'number': 3, 'bbox': [536, 132, 563, 195], 'score': 0.53}, 'person_3': {'number': 4, 'bbox': [438, 83, 464, 196], 'score': 0.51}, 'person_4': {'number': 5, 'bbox': [460, 35, 488, 194], 'score': 0.5}, 'person_5': {'number': 6, 'bbox': [480, 0, 505, 195], 'score': 0.5}}, 'palm': {'palm_1': {'number': 7, 'bbox': [342, 0, 421, 133], 'score': 0.33}, 'palm_2': {'number': 8, 'bbox': [124, 0, 273, 88], 'score': 0.32}, 'palm_3': {'number': 9, 'bbox': [303, 0, 360, 130], 'score': 0.3}, 'palm_4': {'number': 10, 'bbox': [286, 4, 337, 94], 'score': 0.29}}})\n",
      "img_name:  47_n35676\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000, 640.0000, 170.4663],\n",
      "        [ 15.5695, 231.0497, 116.4047, 381.1824],\n",
      "        [212.9907,  18.5158, 346.0549, 158.9463],\n",
      "        [501.9418,   0.0000, 622.2072, 115.3579],\n",
      "        [454.6305, 231.8280, 640.0000, 427.0000],\n",
      "        [ 13.1251, 201.3479, 640.0000, 427.0000],\n",
      "        [  0.0000,   0.0000,  98.4472, 161.4834],\n",
      "        [ 90.8768,   9.1143, 221.2908,  92.2685],\n",
      "        [609.1852,   0.0000, 640.0000, 175.3222],\n",
      "        [311.7197,   1.4764, 460.0800, 159.3282],\n",
      "        [409.7036,   9.7433, 544.3339, 163.3217]])\n",
      "pred_phrase-- ['cabinets(0.40)', 'cabinets(0.29)', 'cabinets(0.29)', 'cabinets(0.29)', 'cabinets(0.28)', 'cabinets(0.28)', 'cabinets(0.27)', 'cabinets(0.27)', 'cabinets(0.27)', 'cabinets(0.27)', 'cabinets(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 84.9255, 186.1128, 245.6864, 373.8066],\n",
      "        [387.9843, 222.6360, 465.4057, 360.5152]])\n",
      "pred_phrase-- ['right(0.44)', 'right(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[236.0961, 220.7663, 287.1100, 339.6659],\n",
      "        [287.8058, 218.6561, 337.5449, 330.2862],\n",
      "        [288.9464, 218.7638, 336.4795, 248.0714],\n",
      "        [ 22.7216, 234.0002, 113.0493, 269.9357],\n",
      "        [237.2681, 221.9361, 285.4106, 252.6076],\n",
      "        [237.1157, 245.5010, 285.0363, 274.7198],\n",
      "        [288.8758, 240.3883, 334.8029, 269.1955],\n",
      "        [237.3569, 265.5628, 284.3731, 296.1281],\n",
      "        [289.2495, 261.5948, 334.8471, 289.1176],\n",
      "        [455.8496, 231.5186, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['drawers(0.30)', 'drawers(0.29)', 'drawers(0.29)', 'drawers(0.29)', 'drawers(0.28)', 'drawers(0.28)', 'drawers(0.27)', 'drawers(0.27)', 'drawers(0.25)', 'drawers(0.25)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 84.3288, 184.1307, 245.7919, 373.6554]])\n",
      "pred_phrase-- ['open(0.60)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[455.2044, 232.7808, 640.0000, 427.0000],\n",
      "        [ 13.0841, 189.4822, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['wooden(0.26)', 'wooden(0.26)']\n",
      "=======================\n",
      "after sort [(186, 15, 0), (18, 212, 1), (0, 501, 2), (136, 454, 3), (0, 0, 4), (9, 90, 5), (0, 609, 6), (1, 311, 7), (9, 409, 8), (138, 84, 9), (88, 387, 10), (125, 288, 11), (234, 22, 12), (217, 237, 13), (125, 237, 14), (121, 288, 15), (265, 237, 16), (173, 289, 17), (184, 455, 18), (138, 84, 19), (232, 455, 20)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 15.5695, 186.0000, 116.4047, 381.1824],\n",
      "        [212.9907,  18.0000, 346.0549, 158.9463],\n",
      "        [501.9418,   0.0000, 622.2072, 115.3579],\n",
      "        [454.6305, 136.0000, 640.0000, 427.0000],\n",
      "        [  0.0000,   0.0000,  98.4472, 161.4834],\n",
      "        [ 90.8768,   9.0000, 221.2908,  92.2685],\n",
      "        [609.1852,   0.0000, 640.0000, 175.3222],\n",
      "        [311.7197,   1.0000, 460.0800, 159.3282],\n",
      "        [409.7036,   9.0000, 544.3339, 163.3217]]), tensor([[ 84.9255, 138.0000, 245.6864, 373.8066],\n",
      "        [387.9843,  88.0000, 465.4057, 360.5152]]), tensor([[288.9464, 125.0000, 336.4795, 248.0714],\n",
      "        [ 22.7216, 234.0000, 113.0493, 269.9357],\n",
      "        [237.2681, 217.0000, 285.4106, 252.6076],\n",
      "        [237.1157, 125.0000, 285.0363, 274.7198],\n",
      "        [288.8758, 121.0000, 334.8029, 269.1955],\n",
      "        [237.3569, 265.0000, 284.3731, 296.1281],\n",
      "        [289.2495, 173.0000, 334.8471, 289.1176],\n",
      "        [455.8496, 184.0000, 640.0000, 427.0000]]), tensor([[ 84.3288, 138.0000, 245.7919, 373.6554]]), tensor([[455.2044, 232.0000, 640.0000, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'cabinets': {'cabinets_1': {'number': 1, 'bbox': [15, 186, 116, 381], 'score': 0.29}, 'cabinets_2': {'number': 2, 'bbox': [212, 18, 346, 158], 'score': 0.29}, 'cabinets_3': {'number': 3, 'bbox': [501, 0, 622, 115], 'score': 0.29}, 'cabinets_4': {'number': 4, 'bbox': [454, 136, 640, 427], 'score': 0.28}, 'cabinets_5': {'number': 5, 'bbox': [0, 0, 98, 161], 'score': 0.27}}, 'right': {'right_1': {'number': 6, 'bbox': [84, 138, 245, 373], 'score': 0.44}, 'right_2': {'number': 7, 'bbox': [387, 88, 465, 360], 'score': 0.26}}, 'drawers': {'drawers_1': {'number': 8, 'bbox': [288, 125, 336, 248], 'score': 0.29}, 'drawers_2': {'number': 9, 'bbox': [22, 234, 113, 269], 'score': 0.29}, 'drawers_3': {'number': 10, 'bbox': [237, 217, 285, 252], 'score': 0.28}, 'drawers_4': {'number': 11, 'bbox': [237, 125, 285, 274], 'score': 0.28}, 'drawers_5': {'number': 12, 'bbox': [288, 121, 334, 269], 'score': 0.27}}, 'open': {'open_1': {'number': 13, 'bbox': [84, 138, 245, 373], 'score': 0.6}}, 'wooden': {'wooden_1': {'number': 14, 'bbox': [455, 232, 640, 427], 'score': 0.26}}})\n",
      "img_name:  48_n16656\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000, 511.3455, 315.8426]])\n",
      "pred_phrase-- ['animal(0.45)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'animal': {'animal_1': {'number': 1, 'bbox': [0, 0, 511, 315], 'score': 0.45}}})\n",
      "img_name:  49_n355339\n",
      "boxes_filt-- tensor([[ 60.6965,  76.1112, 396.9752, 357.0223],\n",
      "        [  0.0000,  33.3963, 114.4403, 201.0192],\n",
      "        [318.1030,  25.1275, 432.9559, 190.8642],\n",
      "        [ 75.6939,  35.9723, 194.5285, 163.9504]])\n",
      "pred_phrase-- ['man(0.60)', 'man(0.57)', 'man(0.52)', 'man(0.50)']\n",
      "=======================\n",
      "after sort [(76, 60, 0), (28, 0, 1), (25, 318, 2), (28, 75, 3)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 60.6965,  76.0000, 396.9752, 357.0223],\n",
      "        [  0.0000,  28.0000, 114.4403, 201.0192],\n",
      "        [318.1030,  25.0000, 432.9559, 190.8642],\n",
      "        [ 75.6939,  28.0000, 194.5285, 163.9504]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'man': {'man_1': {'number': 1, 'bbox': [60, 76, 396, 357], 'score': 0.6}, 'man_2': {'number': 2, 'bbox': [0, 28, 114, 201], 'score': 0.57}, 'man_3': {'number': 3, 'bbox': [318, 25, 432, 190], 'score': 0.52}, 'man_4': {'number': 4, 'bbox': [75, 28, 194, 163], 'score': 0.5}}})\n",
      "img_name:  50_n260521\n",
      "boxes_filt-- tensor([[469.3211,  30.1282, 592.9194, 391.5376]])\n",
      "pred_phrase-- ['coat(0.50)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'coat': {'coat_1': {'number': 1, 'bbox': [469, 30, 592, 391], 'score': 0.5}}})\n",
      "img_name:  51_n573460\n",
      "obj_dict {}\n",
      "img_name:  52_n88366\n",
      "boxes_filt-- tensor([[158.2195, 207.9178, 254.0247, 367.6483]])\n",
      "pred_phrase-- ['pants(0.49)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'pants': {'pants_1': {'number': 1, 'bbox': [158, 207, 254, 367], 'score': 0.49}}})\n",
      "img_name:  53_n413319\n",
      "boxes_filt-- tensor([[147.3856, 133.9733, 348.9250, 334.5588]])\n",
      "pred_phrase-- ['player(0.51)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[145.3237,  83.2515, 357.6435, 335.6231]])\n",
      "pred_phrase-- ['jersey(0.36)']\n",
      "=======================\n",
      "after sort [(133, 147, 0), (83, 145, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[147.3856, 133.0000, 348.9250, 334.5588]]), tensor([[145.3237,  83.0000, 357.6435, 335.6231]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'player': {'player_1': {'number': 1, 'bbox': [147, 133, 348, 334], 'score': 0.51}}, 'jersey': {'jersey_1': {'number': 2, 'bbox': [145, 83, 357, 335], 'score': 0.36}}})\n",
      "img_name:  54_n141939\n",
      "boxes_filt-- tensor([[ 20.4053, 182.6475, 132.3026, 353.8891]])\n",
      "pred_phrase-- ['bath towel(0.38)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 68.7892, 349.0287, 213.7372, 427.0000]])\n",
      "pred_phrase-- ['toilet(0.64)']\n",
      "=======================\n",
      "after sort [(182, 20, 0), (349, 68, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 20.4053, 182.0000, 132.3026, 353.8891]]), tensor([[ 68.7892, 349.0000, 213.7372, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'bath towel': {'bath towel_1': {'number': 1, 'bbox': [20, 182, 132, 353], 'score': 0.38}}, 'toilet': {'toilet_1': {'number': 2, 'bbox': [68, 349, 213, 427], 'score': 0.64}}})\n",
      "img_name:  55_n311910\n",
      "boxes_filt-- tensor([[130.9645,  43.8357, 572.8039, 397.9136]])\n",
      "pred_phrase-- ['sign(0.58)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  0.0000, 276.4146, 192.9590, 427.0000]])\n",
      "pred_phrase-- ['sidewalk(0.32)']\n",
      "=======================\n",
      "after sort [(43, 130, 0), (276, 0, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[130.9645,  43.0000, 572.8039, 397.9136]]), tensor([[  0.0000, 276.0000, 192.9590, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'sign': {'sign_1': {'number': 1, 'bbox': [130, 43, 572, 397], 'score': 0.58}}, 'sidewalk': {'sidewalk_1': {'number': 2, 'bbox': [0, 276, 192, 427], 'score': 0.32}}})\n",
      "img_name:  56_n541482\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  scarf\n",
      "obj_dict defaultdict(<class 'dict'>, {})\n",
      "img_name:  57_n532191\n",
      "boxes_filt-- tensor([[135.5330,  46.9085, 572.9392, 371.0057]])\n",
      "pred_phrase-- ['charger(0.35)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'charger': {'charger_1': {'number': 1, 'bbox': [135, 46, 572, 371], 'score': 0.35}}})\n",
      "img_name:  58_n382416\n",
      "boxes_filt-- tensor([[152.8347, 285.0040, 358.3174, 389.0596],\n",
      "        [261.8159, 326.8752, 418.8790, 399.2014],\n",
      "        [  0.0000, 129.4681,  96.8142, 255.4154]])\n",
      "pred_phrase-- ['bag(0.30)', 'bag(0.29)', 'bag(0.28)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 30.1228,  75.4794, 239.0145, 418.8376]])\n",
      "pred_phrase-- ['person(0.86)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[487.3905, 290.9827, 640.0000, 374.3555],\n",
      "        [152.3767, 285.1125, 358.0913, 388.5862],\n",
      "        [366.3567, 279.0558, 498.2542, 384.5026],\n",
      "        [262.3796, 327.1667, 418.4608, 398.9636],\n",
      "        [495.3307, 337.0911, 628.6266, 393.2881],\n",
      "        [ 52.3970, 296.2467, 122.5461, 358.1210]])\n",
      "pred_phrase-- ['suitcase(0.44)', 'suitcase(0.43)', 'suitcase(0.43)', 'suitcase(0.42)', 'suitcase(0.40)', 'suitcase(0.27)']\n",
      "=======================\n",
      "after sort [(237, 152, 0), (279, 261, 1), (129, 0, 2), (75, 30, 3), (289, 487, 4), (285, 152, 5), (279, 366, 6), (327, 262, 7), (337, 495, 8), (296, 52, 9)]\n",
      "Adjusted collect_boxes_filt [tensor([[152.8347, 237.0000, 358.3174, 389.0596],\n",
      "        [261.8159, 279.0000, 418.8790, 399.2014],\n",
      "        [  0.0000, 129.0000,  96.8142, 255.4154]]), tensor([[ 30.1228,  75.0000, 239.0145, 418.8376]]), tensor([[487.3905, 289.0000, 640.0000, 374.3555],\n",
      "        [152.3767, 285.0000, 358.0913, 388.5862],\n",
      "        [366.3567, 279.0000, 498.2542, 384.5026],\n",
      "        [262.3796, 327.0000, 418.4608, 398.9636],\n",
      "        [495.3307, 337.0000, 628.6266, 393.2881],\n",
      "        [ 52.3970, 296.0000, 122.5461, 358.1210]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'bag': {'bag_1': {'number': 1, 'bbox': [152, 237, 358, 389], 'score': 0.3}, 'bag_2': {'number': 2, 'bbox': [261, 279, 418, 399], 'score': 0.29}, 'bag_3': {'number': 3, 'bbox': [0, 129, 96, 255], 'score': 0.28}}, 'person': {'person_1': {'number': 4, 'bbox': [30, 75, 239, 418], 'score': 0.86}}, 'suitcase': {'suitcase_1': {'number': 5, 'bbox': [487, 289, 640, 374], 'score': 0.44}, 'suitcase_2': {'number': 6, 'bbox': [152, 285, 358, 388], 'score': 0.43}, 'suitcase_3': {'number': 7, 'bbox': [366, 279, 498, 384], 'score': 0.43}, 'suitcase_4': {'number': 8, 'bbox': [262, 327, 418, 398], 'score': 0.42}, 'suitcase_5': {'number': 9, 'bbox': [495, 337, 628, 393], 'score': 0.4}}})\n",
      "img_name:  59_n170941\n",
      "boxes_filt-- tensor([[  0.0000,  68.9965, 367.1544, 320.4542],\n",
      "        [354.6153,  66.6655, 588.2338, 233.2085],\n",
      "        [321.8388, 171.2314, 640.0000, 308.6098],\n",
      "        [150.9159, 131.3349, 248.2449, 211.3800]])\n",
      "pred_phrase-- ['food(0.49)', 'food(0.47)', 'food(0.46)', 'food(0.25)']\n",
      "=======================\n",
      "after sort [(66, 354, 0), (171, 321, 1), (131, 150, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[354.6153,  66.0000, 588.2338, 233.2085],\n",
      "        [321.8388, 171.0000, 640.0000, 308.6098],\n",
      "        [150.9159, 131.0000, 248.2449, 211.3800]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'food': {'food_1': {'number': 1, 'bbox': [354, 66, 588, 233], 'score': 0.47}, 'food_2': {'number': 2, 'bbox': [321, 171, 640, 308], 'score': 0.46}, 'food_3': {'number': 3, 'bbox': [150, 131, 248, 211], 'score': 0.25}}})\n",
      "img_name:  60_n334278\n",
      "boxes_filt-- tensor([[304.0082, 135.6800, 406.4924, 336.0264],\n",
      "        [246.3315, 124.6785, 318.8149, 208.2958],\n",
      "        [ 58.6936, 124.7353, 106.8032, 187.2628],\n",
      "        [ 21.7894, 115.9424,  59.7845, 179.8852],\n",
      "        [156.1615, 119.5642, 172.9907, 155.9376],\n",
      "        [368.2015, 153.4077, 586.2881, 369.0339],\n",
      "        [508.7280, 125.8060, 544.8760, 178.0725],\n",
      "        [446.6163, 276.7946, 587.5488, 356.9668]])\n",
      "pred_phrase-- ['person(0.71)', 'person(0.71)', 'person(0.71)', 'person(0.68)', 'person(0.63)', 'person(0.63)', 'person(0.63)', 'person(0.27)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[302.6807, 135.5916, 407.1252, 336.7988],\n",
      "        [245.6902, 124.4041, 319.1778, 208.8552]])\n",
      "pred_phrase-- ['pitcher(0.40)', 'pitcher(0.37)']\n",
      "=======================\n",
      "after sort [(87, 304, 0), (0, 246, 1), (124, 58, 2), (76, 21, 3), (119, 156, 4), (125, 508, 5), (276, 446, 6), (135, 302, 7), (39, 245, 8)]\n",
      "Adjusted collect_boxes_filt [tensor([[304.0082,  87.0000, 406.4924, 336.0264],\n",
      "        [246.3315,   0.0000, 318.8149, 208.2958],\n",
      "        [ 58.6936, 124.0000, 106.8032, 187.2628],\n",
      "        [ 21.7894,  76.0000,  59.7845, 179.8852],\n",
      "        [156.1615, 119.0000, 172.9907, 155.9376],\n",
      "        [508.7280, 125.0000, 544.8760, 178.0725],\n",
      "        [446.6163, 276.0000, 587.5488, 356.9668]]), tensor([[302.6807, 135.0000, 407.1252, 336.7988],\n",
      "        [245.6902,  39.0000, 319.1778, 208.8552]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'person': {'person_1': {'number': 1, 'bbox': [304, 87, 406, 336], 'score': 0.71}, 'person_2': {'number': 2, 'bbox': [246, 0, 318, 208], 'score': 0.71}, 'person_3': {'number': 3, 'bbox': [58, 124, 106, 187], 'score': 0.71}, 'person_4': {'number': 4, 'bbox': [21, 76, 59, 179], 'score': 0.68}, 'person_5': {'number': 5, 'bbox': [156, 119, 172, 155], 'score': 0.63}}, 'pitcher': {'pitcher_1': {'number': 6, 'bbox': [302, 135, 407, 336], 'score': 0.4}, 'pitcher_2': {'number': 7, 'bbox': [245, 39, 319, 208], 'score': 0.37}}})\n",
      "img_name:  61_n538684\n",
      "obj_dict {}\n",
      "img_name:  62_n293477\n",
      "boxes_filt-- tensor([[195.4667,   4.5275, 449.4830, 139.7173]])\n",
      "pred_phrase-- ['food(0.31)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[296.6660, 199.8553, 503.6133, 295.2900]])\n",
      "pred_phrase-- ['hair clip(0.33)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  0.,   0., 640., 427.]])\n",
      "pred_phrase-- ['bed(0.72)']\n",
      "=======================\n",
      "after sort [(4, 195, 0), (199, 296, 1), (0, 0, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[195.4667,   4.0000, 449.4830, 139.7173]]), tensor([[296.6660, 199.0000, 503.6133, 295.2900]]), tensor([[  0.,   0., 640., 427.]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'food': {'food_1': {'number': 1, 'bbox': [195, 4, 449, 139], 'score': 0.31}}, 'hair clip': {'hair clip_1': {'number': 2, 'bbox': [296, 199, 503, 295], 'score': 0.33}}, 'bed': {'bed_1': {'number': 3, 'bbox': [0, 0, 640, 427], 'score': 0.72}}})\n",
      "img_name:  63_n6908\n",
      "boxes_filt-- tensor([[  0.0000, 294.4985, 424.3775, 427.0000],\n",
      "        [450.7296, 305.8335, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['furniture(0.47)', 'furniture(0.44)']\n",
      "=======================\n",
      "after sort [(294, 0, 0), (305, 450, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[  0.0000, 294.0000, 424.3775, 427.0000],\n",
      "        [450.7296, 305.0000, 640.0000, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'furniture': {'furniture_1': {'number': 1, 'bbox': [0, 294, 424, 427], 'score': 0.47}, 'furniture_2': {'number': 2, 'bbox': [450, 305, 640, 427], 'score': 0.44}}})\n",
      "img_name:  64_n501951\n",
      "boxes_filt-- tensor([[113.9772, 137.9841, 303.6975, 334.5336]])\n",
      "pred_phrase-- ['bench(0.71)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[114.2137, 139.3727, 303.9368, 334.2935],\n",
      "        [449.6571,  78.7363, 577.7502, 257.3169]])\n",
      "pred_phrase-- ['tool(0.29)', 'tool(0.25)']\n",
      "=======================\n",
      "after sort [(91, 113, 0), (139, 114, 1), (78, 449, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[113.9772,  91.0000, 303.6975, 334.5336]]), tensor([[114.2137, 139.0000, 303.9368, 334.2935],\n",
      "        [449.6571,  78.0000, 577.7502, 257.3169]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'bench': {'bench_1': {'number': 1, 'bbox': [113, 91, 303, 334], 'score': 0.71}}, 'tool': {'tool_1': {'number': 2, 'bbox': [114, 139, 303, 334], 'score': 0.29}, 'tool_2': {'number': 3, 'bbox': [449, 78, 577, 257], 'score': 0.25}}})\n",
      "img_name:  65_n324908\n",
      "boxes_filt-- tensor([[ 96.5894, 159.3906, 348.3060, 426.3024]])\n",
      "pred_phrase-- ['tail(0.56)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'tail': {'tail_1': {'number': 1, 'bbox': [96, 159, 348, 426], 'score': 0.56}}})\n",
      "img_name:  66_n278312\n",
      "boxes_filt-- tensor([[410.3403, 220.1852, 468.4603, 270.2816]])\n",
      "pred_phrase-- ['tea kettle(0.35)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'tea kettle': {'tea kettle_1': {'number': 1, 'bbox': [410, 220, 468, 270], 'score': 0.35}}})\n",
      "img_name:  67_n200692\n",
      "boxes_filt-- tensor([[108.2655,  16.1523, 569.6741, 397.9780]])\n",
      "pred_phrase-- ['food(0.73)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'food': {'food_1': {'number': 1, 'bbox': [108, 16, 569, 397], 'score': 0.73}}})\n",
      "img_name:  68_n126087\n",
      "boxes_filt-- tensor([[  9.8328,  33.4301, 285.8651, 377.4450],\n",
      "        [  0.0000,   0.0000, 640.0000, 427.0000],\n",
      "        [268.5948, 107.1922, 484.6182, 387.7532]])\n",
      "pred_phrase-- ['courtyard(0.35)', 'courtyard(0.27)', 'courtyard(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[268.8668, 106.9823, 485.0715, 387.0770],\n",
      "        [ 10.6635,  33.8095, 285.5624, 377.4183]])\n",
      "pred_phrase-- ['wristband(0.49)', 'wristband(0.38)']\n",
      "=======================\n",
      "after sort [(0, 9, 0), (107, 268, 1), (59, 268, 2), (33, 10, 3)]\n",
      "Adjusted collect_boxes_filt [tensor([[  9.8328,   0.0000, 285.8651, 377.4450],\n",
      "        [268.5948, 107.0000, 484.6182, 387.7532]]), tensor([[268.8668,  59.0000, 485.0715, 387.0770],\n",
      "        [ 10.6635,  33.0000, 285.5624, 377.4183]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'courtyard': {'courtyard_1': {'number': 1, 'bbox': [9, 0, 285, 377], 'score': 0.35}, 'courtyard_2': {'number': 2, 'bbox': [268, 107, 484, 387], 'score': 0.26}}, 'wristband': {'wristband_1': {'number': 3, 'bbox': [268, 59, 485, 387], 'score': 0.49}, 'wristband_2': {'number': 4, 'bbox': [10, 33, 285, 377], 'score': 0.38}}})\n",
      "img_name:  69_n579928\n",
      "boxes_filt-- tensor([[ 31.0584,  20.7691, 627.2238, 408.9085]])\n",
      "pred_phrase-- ['animal(0.93)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'animal': {'animal_1': {'number': 1, 'bbox': [31, 20, 627, 408], 'score': 0.93}}})\n",
      "img_name:  70_n488874\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000, 640.0000, 223.8182]])\n",
      "pred_phrase-- ['trees(0.40)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'trees': {'trees_1': {'number': 1, 'bbox': [0, 0, 640, 223], 'score': 0.4}}})\n",
      "img_name:  71_n97485\n",
      "boxes_filt-- tensor([[ 31.4080, 266.3917, 412.3090, 427.0000],\n",
      "        [  0.0000,   0.0000, 209.2546, 179.8918],\n",
      "        [115.4397, 352.7969, 361.9661, 427.0000]])\n",
      "pred_phrase-- ['furniture(0.59)', 'furniture(0.28)', 'furniture(0.25)']\n",
      "=======================\n",
      "after sort [(0, 0, 0), (352, 115, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[  0.0000,   0.0000, 209.2546, 179.8918],\n",
      "        [115.4397, 352.0000, 361.9661, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'furniture': {'furniture_1': {'number': 1, 'bbox': [0, 0, 209, 179], 'score': 0.28}, 'furniture_2': {'number': 2, 'bbox': [115, 352, 361, 427], 'score': 0.25}}})\n",
      "img_name:  72_n249903\n",
      "boxes_filt-- tensor([[208.9650,  39.3259, 336.6570, 164.1535]])\n",
      "pred_phrase-- ['poster(0.45)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'poster': {'poster_1': {'number': 1, 'bbox': [208, 39, 336, 164], 'score': 0.45}}})\n",
      "img_name:  73_n507959\n",
      "boxes_filt-- tensor([[344.7257,   0.0000, 420.4689, 134.5546],\n",
      "        [311.3355,   0.0000, 359.5614, 131.1316],\n",
      "        [286.6884,   4.4536, 336.4366,  96.5382]])\n",
      "pred_phrase-- ['palm tree(0.42)', 'palm tree(0.32)', 'palm tree(0.32)']\n",
      "=======================\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  flag\n",
      "after sort [(0, 344, 0), (0, 311, 1), (4, 286, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[344.7257,   0.0000, 420.4689, 134.5546],\n",
      "        [311.3355,   0.0000, 359.5614, 131.1316],\n",
      "        [286.6884,   4.0000, 336.4366,  96.5382]]), tensor([], size=(0, 4))]\n",
      "obj_dict defaultdict(<class 'dict'>, {'palm tree': {'palm tree_1': {'number': 1, 'bbox': [344, 0, 420, 134], 'score': 0.42}, 'palm tree_2': {'number': 2, 'bbox': [311, 0, 359, 131], 'score': 0.32}, 'palm tree_3': {'number': 3, 'bbox': [286, 4, 336, 96], 'score': 0.32}}})\n",
      "img_name:  74_n98544\n",
      "boxes_filt-- tensor([[464.0812,  92.0126, 551.4758, 133.1527],\n",
      "        [ 82.8170, 107.4485, 413.7474, 342.2457]])\n",
      "pred_phrase-- ['toilet paper(0.47)', 'toilet paper(0.28)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 82.6958, 108.3913, 413.9698, 342.7056]])\n",
      "pred_phrase-- ['right(0.70)']\n",
      "=======================\n",
      "after sort [(92, 464, 0), (60, 82, 1), (108, 82, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[464.0812,  92.0000, 551.4758, 133.1527],\n",
      "        [ 82.8170,  60.0000, 413.7474, 342.2457]]), tensor([[ 82.6958, 108.0000, 413.9698, 342.7056]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'toilet paper': {'toilet paper_1': {'number': 1, 'bbox': [464, 92, 551, 133], 'score': 0.47}, 'toilet paper_2': {'number': 2, 'bbox': [82, 60, 413, 342], 'score': 0.28}}, 'right': {'right_1': {'number': 3, 'bbox': [82, 108, 413, 342], 'score': 0.7}}})\n",
      "img_name:  75_n302358\n",
      "boxes_filt-- tensor([[  5.4665, 132.1399, 346.5328, 413.8495]])\n",
      "pred_phrase-- ['small scooter(0.54)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  5.6554, 131.0683, 345.0367, 413.8971]])\n",
      "pred_phrase-- ['motorcycle(0.91)']\n",
      "=======================\n",
      "after sort [(132, 5, 0), (84, 5, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[  5.4665, 132.0000, 346.5328, 413.8495]]), tensor([[  5.6554,  84.0000, 345.0367, 413.8971]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'small scooter': {'small scooter_1': {'number': 1, 'bbox': [5, 132, 346, 413], 'score': 0.54}}, 'motorcycle': {'motorcycle_1': {'number': 2, 'bbox': [5, 84, 345, 413], 'score': 0.91}}})\n",
      "img_name:  76_n151768\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  skirt\n",
      "obj_dict defaultdict(<class 'dict'>, {})\n",
      "img_name:  77_n318370\n",
      "boxes_filt-- tensor([[274.9928, 354.3998, 344.1509, 427.0000],\n",
      "        [147.1237, 295.8155, 272.1360, 427.0000]])\n",
      "pred_phrase-- ['drink(0.47)', 'drink(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[274.8427, 354.1791, 344.0601, 427.0000]])\n",
      "pred_phrase-- ['bottle(0.63)']\n",
      "=======================\n",
      "after sort [(306, 274, 0), (295, 147, 1), (354, 274, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[274.9928, 306.0000, 344.1509, 427.0000],\n",
      "        [147.1237, 295.0000, 272.1360, 427.0000]]), tensor([[274.8427, 354.0000, 344.0601, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'drink': {'drink_1': {'number': 1, 'bbox': [274, 306, 344, 427], 'score': 0.47}, 'drink_2': {'number': 2, 'bbox': [147, 295, 272, 427], 'score': 0.26}}, 'bottle': {'bottle_1': {'number': 3, 'bbox': [274, 354, 344, 427], 'score': 0.63}}})\n",
      "img_name:  78_n412144\n",
      "boxes_filt-- tensor([[467.4019, 102.3018, 544.6735, 225.7940],\n",
      "        [418.3547, 122.0966, 486.5552, 240.8378],\n",
      "        [ 90.1281,  24.3132, 367.0777, 236.5556],\n",
      "        [572.7740,  60.0058, 636.5184, 205.2775],\n",
      "        [297.3606, 157.7119, 348.7856, 282.6664],\n",
      "        [143.8226, 215.6602, 234.7622, 318.4012],\n",
      "        [243.3023, 186.8790, 299.5335, 289.5008],\n",
      "        [211.8156, 206.4294, 261.5291, 302.4576],\n",
      "        [ 79.1311, 216.9939, 120.0375, 293.5720],\n",
      "        [  4.8910, 229.2292,  46.0615, 316.0157],\n",
      "        [ 31.5785, 230.9812,  72.3309, 309.1087],\n",
      "        [616.8536,  65.4379, 640.0000, 194.8044],\n",
      "        [  0.0000, 261.5667,  21.5899, 322.7496]])\n",
      "pred_phrase-- ['person(0.78)', 'person(0.74)', 'person(0.74)', 'person(0.71)', 'person(0.70)', 'person(0.63)', 'person(0.61)', 'person(0.57)', 'person(0.54)', 'person(0.54)', 'person(0.52)', 'person(0.40)', 'person(0.36)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[571.3859,  59.0369, 636.6599, 204.7924],\n",
      "        [572.0012,  61.1656, 607.5396, 100.5247]])\n",
      "pred_phrase-- ['video camera(0.38)', 'video camera(0.32)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 89.4847,  23.6171, 367.3903, 238.1548]])\n",
      "pred_phrase-- ['ethnicity(0.52)']\n",
      "=======================\n",
      "after sort [(74, 467, 0), (122, 418, 1), (24, 90, 2), (71, 297, 3), (168, 143, 4), (72, 243, 5), (120, 211, 6), (216, 79, 7), (120, 4, 8), (168, 31, 9), (65, 616, 10), (261, 0, 11), (17, 572, 12), (0, 89, 13)]\n",
      "Adjusted collect_boxes_filt [tensor([[467.4019,  74.0000, 544.6735, 225.7940],\n",
      "        [418.3547, 122.0000, 486.5552, 240.8378],\n",
      "        [ 90.1281,  24.0000, 367.0777, 236.5556],\n",
      "        [297.3606,  71.0000, 348.7856, 282.6664],\n",
      "        [143.8226, 168.0000, 234.7622, 318.4012],\n",
      "        [243.3023,  72.0000, 299.5335, 289.5008],\n",
      "        [211.8156, 120.0000, 261.5291, 302.4576],\n",
      "        [ 79.1311, 216.0000, 120.0375, 293.5720],\n",
      "        [  4.8910, 120.0000,  46.0615, 316.0157],\n",
      "        [ 31.5785, 168.0000,  72.3309, 309.1087],\n",
      "        [616.8536,  65.0000, 640.0000, 194.8044],\n",
      "        [  0.0000, 261.0000,  21.5899, 322.7496]]), tensor([[572.0012,  17.0000, 607.5396, 100.5247]]), tensor([[ 89.4847,   0.0000, 367.3903, 238.1548]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'person': {'person_1': {'number': 1, 'bbox': [467, 74, 544, 225], 'score': 0.78}, 'person_2': {'number': 2, 'bbox': [418, 122, 486, 240], 'score': 0.74}, 'person_3': {'number': 3, 'bbox': [90, 24, 367, 236], 'score': 0.74}, 'person_4': {'number': 4, 'bbox': [297, 71, 348, 282], 'score': 0.7}, 'person_5': {'number': 5, 'bbox': [143, 168, 234, 318], 'score': 0.63}}, 'video camera': {'video camera_1': {'number': 6, 'bbox': [572, 17, 607, 100], 'score': 0.32}}, 'ethnicity': {'ethnicity_1': {'number': 7, 'bbox': [89, 0, 367, 238], 'score': 0.52}}})\n",
      "img_name:  79_n262929\n",
      "boxes_filt-- tensor([[261.6653,  26.2571, 405.4501,  75.9112],\n",
      "        [427.4729,  19.5506, 501.9619,  44.9572],\n",
      "        [ 82.0115,  12.0838, 153.7478,  41.3747],\n",
      "        [208.0836,  25.7193, 512.0664, 343.4302]])\n",
      "pred_phrase-- ['hat(0.39)', 'hat(0.38)', 'hat(0.35)', 'hat(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[208.3210,  26.3618, 512.2597, 343.8399]])\n",
      "pred_phrase-- ['tie(0.39)']\n",
      "=======================\n",
      "after sort [(0, 261, 0), (19, 427, 1), (12, 82, 2), (26, 208, 3)]\n",
      "Adjusted collect_boxes_filt [tensor([[261.6653,   0.0000, 405.4501,  75.9112],\n",
      "        [427.4729,  19.0000, 501.9619,  44.9572],\n",
      "        [ 82.0115,  12.0000, 153.7478,  41.3747]]), tensor([[208.3210,  26.0000, 512.2597, 343.8399]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'hat': {'hat_1': {'number': 1, 'bbox': [261, 0, 405, 75], 'score': 0.39}, 'hat_2': {'number': 2, 'bbox': [427, 19, 501, 44], 'score': 0.38}, 'hat_3': {'number': 3, 'bbox': [82, 12, 153, 41], 'score': 0.35}}, 'tie': {'tie_1': {'number': 4, 'bbox': [208, 26, 512, 343], 'score': 0.39}}})\n",
      "img_name:  80_n356822\n",
      "boxes_filt-- tensor([[354.2312, 311.2461, 632.0178, 427.0000],\n",
      "        [236.3496, 235.8929, 392.2170, 427.0000],\n",
      "        [113.1253, 119.1756, 255.6633, 206.9444],\n",
      "        [  0.0000, 119.1181, 256.7841, 427.0000],\n",
      "        [  4.3776,  64.5519,  28.2588, 104.0766],\n",
      "        [  0.0000, 202.9724,  71.2970, 427.0000],\n",
      "        [133.1016,  56.1225, 169.6102,  92.9222]])\n",
      "pred_phrase-- ['clothing(0.49)', 'clothing(0.36)', 'clothing(0.33)', 'clothing(0.30)', 'clothing(0.30)', 'clothing(0.29)', 'clothing(0.28)']\n",
      "=======================\n",
      "after sort [(311, 354, 0), (235, 236, 1), (119, 113, 2), (64, 4, 3), (202, 0, 4), (56, 133, 5)]\n",
      "Adjusted collect_boxes_filt [tensor([[354.2312, 311.0000, 632.0178, 427.0000],\n",
      "        [236.3496, 235.0000, 392.2170, 427.0000],\n",
      "        [113.1253, 119.0000, 255.6633, 206.9444],\n",
      "        [  4.3776,  64.0000,  28.2588, 104.0766],\n",
      "        [  0.0000, 202.0000,  71.2970, 427.0000],\n",
      "        [133.1016,  56.0000, 169.6102,  92.9222]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'clothing': {'clothing_1': {'number': 1, 'bbox': [354, 311, 632, 427], 'score': 0.49}, 'clothing_2': {'number': 2, 'bbox': [236, 235, 392, 427], 'score': 0.36}, 'clothing_3': {'number': 3, 'bbox': [113, 119, 255, 206], 'score': 0.33}, 'clothing_4': {'number': 4, 'bbox': [4, 64, 28, 104], 'score': 0.3}, 'clothing_5': {'number': 5, 'bbox': [0, 202, 71, 427], 'score': 0.29}}})\n",
      "img_name:  81_n527290\n",
      "boxes_filt-- tensor([[  0.0000, 293.0740,  86.9569, 373.0280],\n",
      "        [  0.0000, 172.0074, 103.2612, 373.2571]])\n",
      "pred_phrase-- ['handbag(0.39)', 'handbag(0.37)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'handbag': {'handbag_1': {'number': 1, 'bbox': [0, 293, 86, 373], 'score': 0.39}}})\n",
      "img_name:  82_n460385\n",
      "boxes_filt-- tensor([[124.1600,   1.7719, 541.5160, 327.4174]])\n",
      "pred_phrase-- ['woman(0.86)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'woman': {'woman_1': {'number': 1, 'bbox': [124, 1, 541, 327], 'score': 0.86}}})\n",
      "img_name:  83_n25275\n",
      "boxes_filt-- tensor([[164.1422,  30.1191, 322.5178, 266.0824],\n",
      "        [313.4342,   1.1412, 462.9495, 277.9189]])\n",
      "pred_phrase-- ['boy(0.60)', 'boy(0.36)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[163.6784,  30.6254, 323.4808, 266.3869]])\n",
      "pred_phrase-- ['other boy(0.66)']\n",
      "=======================\n",
      "after sort [(0, 164, 0), (1, 313, 1), (30, 163, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[164.1422,   0.0000, 322.5178, 266.0824],\n",
      "        [313.4342,   1.0000, 462.9495, 277.9189]]), tensor([[163.6784,  30.0000, 323.4808, 266.3869]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'boy': {'boy_1': {'number': 1, 'bbox': [164, 0, 322, 266], 'score': 0.6}, 'boy_2': {'number': 2, 'bbox': [313, 1, 462, 277], 'score': 0.36}}, 'other boy': {'other boy_1': {'number': 3, 'bbox': [163, 30, 323, 266], 'score': 0.66}}})\n",
      "img_name:  84_n382416\n",
      "boxes_filt-- tensor([[153.0039, 284.5976, 358.5238, 389.4402]])\n",
      "pred_phrase-- ['square box(0.48)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'square box': {'square box_1': {'number': 1, 'bbox': [153, 284, 358, 389], 'score': 0.48}}})\n",
      "img_name:  85_n115614\n",
      "boxes_filt-- tensor([[  0.0000, 229.9610,  78.8226, 273.3000]])\n",
      "pred_phrase-- ['red sign(0.31)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'red sign': {'red sign_1': {'number': 1, 'bbox': [0, 229, 78, 273], 'score': 0.31}}})\n",
      "img_name:  86_n167164\n",
      "boxes_filt-- tensor([[  0.0000, 140.2684, 396.7187, 272.9613]])\n",
      "pred_phrase-- ['wire(0.25)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'wire': {'wire_1': {'number': 1, 'bbox': [0, 140, 396, 272], 'score': 0.25}}})\n",
      "img_name:  87_n141939\n",
      "boxes_filt-- tensor([[431.0213, 335.6221, 532.4976, 422.1675]])\n",
      "pred_phrase-- ['garbage can(0.73)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[534.1657, 280.6339, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['radiator(0.60)']\n",
      "=======================\n",
      "after sort [(335, 431, 0), (280, 534, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[431.0213, 335.0000, 532.4976, 422.1675]]), tensor([[534.1657, 280.0000, 640.0000, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'garbage can': {'garbage can_1': {'number': 1, 'bbox': [431, 335, 532, 422], 'score': 0.73}}, 'radiator': {'radiator_1': {'number': 2, 'bbox': [534, 280, 640, 427], 'score': 0.6}}})\n",
      "img_name:  88_n511913\n",
      "boxes_filt-- tensor([[330.5970, 341.2666, 414.0426, 394.0490]])\n",
      "pred_phrase-- ['small books(0.34)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[459.5399,  11.0300, 640.0000, 426.1627]])\n",
      "pred_phrase-- ['right side(0.82)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 18.6637,  35.4003, 404.1563, 426.2141]])\n",
      "pred_phrase-- ['left(0.93)']\n",
      "=======================\n",
      "after sort [(341, 330, 0), (11, 459, 1), (35, 18, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[330.5970, 341.0000, 414.0426, 394.0490]]), tensor([[459.5399,  11.0000, 640.0000, 426.1627]]), tensor([[ 18.6637,  35.0000, 404.1563, 426.2141]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'small books': {'small books_1': {'number': 1, 'bbox': [330, 341, 414, 394], 'score': 0.34}}, 'right side': {'right side_1': {'number': 2, 'bbox': [459, 11, 640, 426], 'score': 0.82}}, 'left': {'left_1': {'number': 3, 'bbox': [18, 35, 404, 426], 'score': 0.93}}})\n",
      "img_name:  89_n312206\n",
      "boxes_filt-- tensor([[225.2896, 243.3577, 451.6199, 421.9146]])\n",
      "pred_phrase-- ['chocolate dessert(0.81)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[398.0431,  29.7714, 550.2205, 253.3173]])\n",
      "pred_phrase-- ['right side(0.44)']\n",
      "=======================\n",
      "after sort [(243, 225, 0), (29, 398, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[225.2896, 243.0000, 451.6199, 421.9146]]), tensor([[398.0431,  29.0000, 550.2205, 253.3173]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'chocolate dessert': {'chocolate dessert_1': {'number': 1, 'bbox': [225, 243, 451, 421], 'score': 0.81}}, 'right side': {'right side_1': {'number': 2, 'bbox': [398, 29, 550, 253], 'score': 0.44}}})\n",
      "img_name:  90_n526228\n",
      "boxes_filt-- tensor([[418.3573, 399.9332, 484.8300, 427.0000]])\n",
      "pred_phrase-- ['remote(0.30)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 53.4390, 137.0407, 395.5801, 424.5967]])\n",
      "pred_phrase-- ['phone(0.27)']\n",
      "=======================\n",
      "after sort [(399, 418, 0), (137, 53, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[418.3573, 399.0000, 484.8300, 427.0000]]), tensor([[ 53.4390, 137.0000, 395.5801, 424.5967]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'remote': {'remote_1': {'number': 1, 'bbox': [418, 399, 484, 427], 'score': 0.3}}, 'phone': {'phone_1': {'number': 2, 'bbox': [53, 137, 395, 424], 'score': 0.27}}})\n",
      "img_name:  91_n86120\n",
      "boxes_filt-- tensor([[215.7441,   0.0000, 535.0880, 384.0659]])\n",
      "pred_phrase-- ['shirt(0.28)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[493.7121,   0.0000, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['wall(0.64)']\n",
      "=======================\n",
      "after sort [(0, 215, 0), (0, 493, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[215.7441,   0.0000, 535.0880, 384.0659]]), tensor([[493.7121,   0.0000, 640.0000, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'shirt': {'shirt_1': {'number': 1, 'bbox': [215, 0, 535, 384], 'score': 0.28}}, 'wall': {'wall_1': {'number': 2, 'bbox': [493, 0, 640, 427], 'score': 0.64}}})\n",
      "img_name:  92_n477215\n",
      "boxes_filt-- tensor([[  6.7654,  38.3387, 575.0071, 331.7706],\n",
      "        [ 87.7022,  41.5313, 101.4967,  56.6037]])\n",
      "pred_phrase-- ['animals(0.32)', 'animals(0.25)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'animals': {'animals_1': {'number': 1, 'bbox': [87, 41, 101, 56], 'score': 0.25}}})\n",
      "img_name:  93_n216553\n",
      "boxes_filt-- tensor([[411.0814, 273.4205, 517.2872, 372.0975]])\n",
      "pred_phrase-- ['bag(0.45)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[509.5963,   0.0000, 640.0000, 121.9044]])\n",
      "pred_phrase-- ['sky(0.36)']\n",
      "=======================\n",
      "after sort [(273, 411, 0), (0, 509, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[411.0814, 273.0000, 517.2872, 372.0975]]), tensor([[509.5963,   0.0000, 640.0000, 121.9044]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'bag': {'bag_1': {'number': 1, 'bbox': [411, 273, 517, 372], 'score': 0.45}}, 'sky': {'sky_1': {'number': 2, 'bbox': [509, 0, 640, 121], 'score': 0.36}}})\n",
      "img_name:  94_n118102\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  appliance\n",
      "obj_dict defaultdict(<class 'dict'>, {})\n",
      "img_name:  95_n473688\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  soap dispenser\n",
      "obj_dict defaultdict(<class 'dict'>, {})\n",
      "img_name:  96_n116329\n",
      "boxes_filt-- tensor([[186.5870, 177.3766, 640.0000, 242.6947]])\n",
      "pred_phrase-- ['metal fence(0.44)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'metal fence': {'metal fence_1': {'number': 1, 'bbox': [186, 177, 640, 242], 'score': 0.44}}})\n",
      "img_name:  97_n51658\n",
      "boxes_filt-- tensor([[187.4460, 153.0972, 514.7137, 401.4892],\n",
      "        [551.4777, 247.2584, 640.0000, 342.1560],\n",
      "        [401.9371, 156.9830, 491.5562, 224.7633],\n",
      "        [180.8286, 144.9342, 278.7136, 209.0565],\n",
      "        [289.6382,  35.3956, 383.4185,  96.8281],\n",
      "        [384.8551,  94.8782, 469.0815, 176.9033],\n",
      "        [ 47.9240,  59.7328, 132.2785, 151.0724],\n",
      "        [107.9715,   0.0000, 225.3611,  73.7058],\n",
      "        [ 95.1236, 126.6643, 181.0128, 187.0008],\n",
      "        [124.1363,  49.3922, 216.9600, 132.4697],\n",
      "        [368.3651, 183.9866, 432.5609, 226.2460],\n",
      "        [ 40.7838,   8.4133, 129.2541,  80.0690],\n",
      "        [163.9954, 117.9979, 239.8840, 185.1251],\n",
      "        [197.7260,  27.1835, 270.6111,  92.6768],\n",
      "        [  0.0000,   0.0000,  62.8499,  66.5199],\n",
      "        [181.8735,  58.1057, 262.6971, 128.1785],\n",
      "        [311.4869, 117.7700, 409.8160, 201.7708],\n",
      "        [329.1951,  62.1189, 413.3632, 111.1673],\n",
      "        [  0.0000,  49.7239,  58.2278, 164.7900],\n",
      "        [330.0428,  82.7105, 419.1388, 129.5157],\n",
      "        [ 72.5932,   0.0000, 143.8934,  30.5841],\n",
      "        [246.3915, 118.8933, 340.7601, 188.6618],\n",
      "        [ 33.9588,   0.0000,  84.8546,  28.4360],\n",
      "        [251.2219,   0.0000, 333.3638,  37.8231],\n",
      "        [  0.0000,   0.0000, 485.6848, 224.0347]])\n",
      "pred_phrase-- ['person(0.69)', 'person(0.59)', 'person(0.47)', 'person(0.47)', 'person(0.46)', 'person(0.44)', 'person(0.42)', 'person(0.42)', 'person(0.40)', 'person(0.39)', 'person(0.39)', 'person(0.38)', 'person(0.37)', 'person(0.37)', 'person(0.37)', 'person(0.36)', 'person(0.36)', 'person(0.34)', 'person(0.34)', 'person(0.32)', 'person(0.30)', 'person(0.29)', 'person(0.28)', 'person(0.27)', 'person(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[187.4964, 153.2524, 515.1659, 402.0539]])\n",
      "pred_phrase-- ['shirt(0.37)']\n",
      "=======================\n",
      "after sort [(247, 551, 0), (135, 401, 1), (30, 180, 2), (0, 289, 3), (0, 384, 4), (59, 47, 5), (0, 107, 6), (126, 95, 7), (10, 124, 8), (183, 368, 9), (0, 40, 10), (78, 163, 11), (0, 197, 12), (0, 0, 13), (21, 181, 14), (9, 311, 15), (0, 329, 16), (11, 0, 17), (0, 330, 18), (0, 72, 19), (10, 246, 20), (0, 33, 21), (0, 251, 22), (153, 187, 23)]\n",
      "Adjusted collect_boxes_filt [tensor([[551.4777, 247.0000, 640.0000, 342.1560],\n",
      "        [401.9371, 135.0000, 491.5562, 224.7633],\n",
      "        [180.8286,  30.0000, 278.7136, 209.0565],\n",
      "        [289.6382,   0.0000, 383.4185,  96.8281],\n",
      "        [384.8551,   0.0000, 469.0815, 176.9033],\n",
      "        [ 47.9240,  59.0000, 132.2785, 151.0724],\n",
      "        [107.9715,   0.0000, 225.3611,  73.7058],\n",
      "        [ 95.1236, 126.0000, 181.0128, 187.0008],\n",
      "        [124.1363,  10.0000, 216.9600, 132.4697],\n",
      "        [368.3651, 183.0000, 432.5609, 226.2460],\n",
      "        [ 40.7838,   0.0000, 129.2541,  80.0690],\n",
      "        [163.9954,  78.0000, 239.8840, 185.1251],\n",
      "        [197.7260,   0.0000, 270.6111,  92.6768],\n",
      "        [  0.0000,   0.0000,  62.8499,  66.5199],\n",
      "        [181.8735,  21.0000, 262.6971, 128.1785],\n",
      "        [311.4869,   9.0000, 409.8160, 201.7708],\n",
      "        [329.1951,   0.0000, 413.3632, 111.1673],\n",
      "        [  0.0000,  11.0000,  58.2278, 164.7900],\n",
      "        [330.0428,   0.0000, 419.1388, 129.5157],\n",
      "        [ 72.5932,   0.0000, 143.8934,  30.5841],\n",
      "        [246.3915,  10.0000, 340.7601, 188.6618],\n",
      "        [ 33.9588,   0.0000,  84.8546,  28.4360],\n",
      "        [251.2219,   0.0000, 333.3638,  37.8231]]), tensor([[187.4964, 153.0000, 515.1659, 402.0539]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'person': {'person_1': {'number': 1, 'bbox': [551, 247, 640, 342], 'score': 0.59}, 'person_2': {'number': 2, 'bbox': [401, 135, 491, 224], 'score': 0.47}, 'person_3': {'number': 3, 'bbox': [180, 30, 278, 209], 'score': 0.47}, 'person_4': {'number': 4, 'bbox': [289, 0, 383, 96], 'score': 0.46}, 'person_5': {'number': 5, 'bbox': [384, 0, 469, 176], 'score': 0.44}}, 'shirt': {'shirt_1': {'number': 6, 'bbox': [187, 153, 515, 402], 'score': 0.37}}})\n",
      "img_name:  98_n309148\n",
      "boxes_filt-- tensor([[111.9310,  99.2763, 566.9241, 402.8721],\n",
      "        [  0.0000, 186.0453, 131.5623, 350.9294]])\n",
      "pred_phrase-- ['taxi(0.48)', 'taxi(0.28)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[113.1430,  99.7341, 567.2689, 401.7504]])\n",
      "pred_phrase-- ['sticker(0.62)']\n",
      "=======================\n",
      "after sort [(51, 111, 0), (186, 0, 1), (99, 113, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[111.9310,  51.0000, 566.9241, 402.8721],\n",
      "        [  0.0000, 186.0000, 131.5623, 350.9294]]), tensor([[113.1430,  99.0000, 567.2689, 401.7504]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'taxi': {'taxi_1': {'number': 1, 'bbox': [111, 51, 566, 402], 'score': 0.48}, 'taxi_2': {'number': 2, 'bbox': [0, 186, 131, 350], 'score': 0.28}}, 'sticker': {'sticker_1': {'number': 3, 'bbox': [113, 99, 567, 401], 'score': 0.62}}})\n",
      "img_name:  99_n473688\n",
      "boxes_filt-- tensor([[209.8957, 233.6864, 461.0579, 427.0000],\n",
      "        [395.2596, 392.1535, 481.6765, 427.0000]])\n",
      "pred_phrase-- ['clothing(0.65)', 'clothing(0.26)']\n",
      "=======================\n",
      "after sort [(233, 209, 0), (392, 395, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[209.8957, 233.0000, 461.0579, 427.0000],\n",
      "        [395.2596, 392.0000, 481.6765, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'clothing': {'clothing_1': {'number': 1, 'bbox': [209, 233, 461, 427], 'score': 0.65}, 'clothing_2': {'number': 2, 'bbox': [395, 392, 481, 427], 'score': 0.26}}})\n",
      "img_name:  100_n54424\n",
      "boxes_filt-- tensor([[302.0278, 252.6153, 475.1224, 422.5522],\n",
      "        [302.5159, 339.6927, 443.6366, 420.8030]])\n",
      "pred_phrase-- ['device(0.37)', 'device(0.28)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 91.4941,   0.0000, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['boy(0.48)']\n",
      "=======================\n",
      "after sort [(339, 302, 0), (0, 91, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[302.5159, 339.0000, 443.6366, 420.8030]]), tensor([[ 91.4941,   0.0000, 640.0000, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'device': {'device_1': {'number': 1, 'bbox': [302, 339, 443, 420], 'score': 0.28}}, 'boy': {'boy_1': {'number': 2, 'bbox': [91, 0, 640, 427], 'score': 0.48}}})\n",
      "img_name:  101_n16378\n",
      "boxes_filt-- tensor([[207.1717,  42.2390, 467.9390, 427.0000],\n",
      "        [ 60.7721, 125.6691, 147.6841, 368.5885],\n",
      "        [164.1642, 132.1987, 253.6529, 340.0842]])\n",
      "pred_phrase-- ['person(0.75)', 'person(0.56)', 'person(0.55)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[207.3332,  42.8639, 464.1176, 425.6140]])\n",
      "pred_phrase-- ['papers(0.37)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[365.2520,  92.0284, 529.9529, 388.0447],\n",
      "        [209.3111, 352.9015, 300.7203, 427.0000]])\n",
      "pred_phrase-- ['bag(0.41)', 'bag(0.27)']\n",
      "=======================\n",
      "after sort [(0, 207, 0), (125, 60, 1), (132, 164, 2), (42, 207, 3), (92, 365, 4), (352, 209, 5)]\n",
      "Adjusted collect_boxes_filt [tensor([[207.1717,   0.0000, 467.9390, 427.0000],\n",
      "        [ 60.7721, 125.0000, 147.6841, 368.5885],\n",
      "        [164.1642, 132.0000, 253.6529, 340.0842]]), tensor([[207.3332,  42.0000, 464.1176, 425.6140]]), tensor([[365.2520,  92.0000, 529.9529, 388.0447],\n",
      "        [209.3111, 352.0000, 300.7203, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'person': {'person_1': {'number': 1, 'bbox': [207, 0, 467, 427], 'score': 0.75}, 'person_2': {'number': 2, 'bbox': [60, 125, 147, 368], 'score': 0.56}, 'person_3': {'number': 3, 'bbox': [164, 132, 253, 340], 'score': 0.55}}, 'papers': {'papers_1': {'number': 4, 'bbox': [207, 42, 464, 425], 'score': 0.37}}, 'bag': {'bag_1': {'number': 5, 'bbox': [365, 92, 529, 388], 'score': 0.41}, 'bag_2': {'number': 6, 'bbox': [209, 352, 300, 427], 'score': 0.27}}})\n",
      "img_name:  102_n140421\n",
      "boxes_filt-- tensor([[  6.5651, 288.7460, 173.4310, 353.9735],\n",
      "        [111.4589, 261.6854, 241.7215, 307.4200]])\n",
      "pred_phrase-- ['sink(0.52)', 'sink(0.51)']\n",
      "=======================\n",
      "after sort [(288, 6, 0), (261, 111, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[  6.5651, 288.0000, 173.4310, 353.9735],\n",
      "        [111.4589, 261.0000, 241.7215, 307.4200]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'sink': {'sink_1': {'number': 1, 'bbox': [6, 288, 173, 353], 'score': 0.52}, 'sink_2': {'number': 2, 'bbox': [111, 261, 241, 307], 'score': 0.51}}})\n",
      "img_name:  103_n498712\n",
      "boxes_filt-- tensor([[144.4930,   0.0000, 289.3325, 325.6591],\n",
      "        [448.9842,   0.0000, 576.5048, 345.7604]])\n",
      "pred_phrase-- ['woman(0.73)', 'woman(0.57)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[146.6019,   0.0000, 289.3676, 326.0456],\n",
      "        [449.4615,   0.0000, 576.2039, 345.6620]])\n",
      "pred_phrase-- ['girl(0.51)', 'girl(0.35)']\n",
      "=======================\n",
      "after sort [(0, 144, 0), (0, 448, 1), (0, 146, 2), (0, 449, 3)]\n",
      "Adjusted collect_boxes_filt [tensor([[144.4930,   0.0000, 289.3325, 325.6591],\n",
      "        [448.9842,   0.0000, 576.5048, 345.7604]]), tensor([[146.6019,   0.0000, 289.3676, 326.0456],\n",
      "        [449.4615,   0.0000, 576.2039, 345.6620]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'woman': {'woman_1': {'number': 1, 'bbox': [144, 0, 289, 325], 'score': 0.73}, 'woman_2': {'number': 2, 'bbox': [448, 0, 576, 345], 'score': 0.57}}, 'girl': {'girl_1': {'number': 3, 'bbox': [146, 0, 289, 326], 'score': 0.51}, 'girl_2': {'number': 4, 'bbox': [449, 0, 576, 345], 'score': 0.35}}})\n",
      "img_name:  104_n39114\n",
      "boxes_filt-- tensor([[173.8354,  66.3962, 472.5459, 416.7763]])\n",
      "pred_phrase-- ['person(0.88)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  0.0000,  57.9614, 287.9917, 106.1502],\n",
      "        [474.9526,  22.6779, 548.4489,  99.0674],\n",
      "        [399.2190,  22.1510, 482.1262,  99.4771]])\n",
      "pred_phrase-- ['bench(0.42)', 'bench(0.30)', 'bench(0.30)']\n",
      "=======================\n",
      "after sort [(66, 173, 0), (57, 0, 1), (22, 474, 2), (22, 399, 3)]\n",
      "Adjusted collect_boxes_filt [tensor([[173.8354,  66.0000, 472.5459, 416.7763]]), tensor([[  0.0000,  57.0000, 287.9917, 106.1502],\n",
      "        [474.9526,  22.0000, 548.4489,  99.0674],\n",
      "        [399.2190,  22.0000, 482.1262,  99.4771]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'person': {'person_1': {'number': 1, 'bbox': [173, 66, 472, 416], 'score': 0.88}}, 'bench': {'bench_1': {'number': 2, 'bbox': [0, 57, 287, 106], 'score': 0.42}, 'bench_2': {'number': 3, 'bbox': [474, 22, 548, 99], 'score': 0.3}, 'bench_3': {'number': 4, 'bbox': [399, 22, 482, 99], 'score': 0.3}}})\n",
      "img_name:  105_n415215\n",
      "boxes_filt-- tensor([[375.0670,   3.5650, 623.7808, 309.9006]])\n",
      "pred_phrase-- ['purple hair(0.78)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'purple hair': {'purple hair_1': {'number': 1, 'bbox': [375, 3, 623, 309], 'score': 0.78}}})\n",
      "img_name:  106_n479092\n",
      "boxes_filt-- tensor([[439.0294,  83.2273, 504.8018, 201.1816]])\n",
      "pred_phrase-- ['silver forks(0.44)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  0.0000,  29.0334, 640.0000, 404.6159]])\n",
      "pred_phrase-- ['side(0.37)']\n",
      "=======================\n",
      "after sort [(83, 439, 0), (29, 0, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[439.0294,  83.0000, 504.8018, 201.1816]]), tensor([[  0.0000,  29.0000, 640.0000, 404.6159]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'silver forks': {'silver forks_1': {'number': 1, 'bbox': [439, 83, 504, 201], 'score': 0.44}}, 'side': {'side_1': {'number': 2, 'bbox': [0, 29, 640, 404], 'score': 0.37}}})\n",
      "img_name:  107_n283587\n",
      "boxes_filt-- tensor([[  0.0000, 176.5571, 336.0521, 427.0000],\n",
      "        [264.2997, 158.1135, 478.0792, 240.9521],\n",
      "        [ 22.3142, 278.6052, 171.6448, 427.0000],\n",
      "        [337.5316, 191.7121, 429.1735, 245.9527],\n",
      "        [113.1799, 253.3486, 229.7531, 409.6976],\n",
      "        [464.1104, 192.3230, 514.8772, 234.5917]])\n",
      "pred_phrase-- ['furniture(0.44)', 'furniture(0.41)', 'furniture(0.27)', 'furniture(0.27)', 'furniture(0.26)', 'furniture(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  0.0000, 176.4597, 334.4934, 427.0000],\n",
      "        [265.3871, 156.2435, 481.1916, 239.8044]])\n",
      "pred_phrase-- ['staircase(0.31)', 'staircase(0.25)']\n",
      "=======================\n",
      "after sort [(143, 264, 0), (278, 22, 1), (191, 337, 2), (253, 113, 3), (192, 464, 4), (176, 0, 5), (95, 265, 6)]\n",
      "Adjusted collect_boxes_filt [tensor([[264.2997, 143.0000, 478.0792, 240.9521],\n",
      "        [ 22.3142, 278.0000, 171.6448, 427.0000],\n",
      "        [337.5316, 191.0000, 429.1735, 245.9527],\n",
      "        [113.1799, 253.0000, 229.7531, 409.6976],\n",
      "        [464.1104, 192.0000, 514.8772, 234.5917]]), tensor([[  0.0000, 176.0000, 334.4934, 427.0000],\n",
      "        [265.3871,  95.0000, 481.1916, 239.8044]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'furniture': {'furniture_1': {'number': 1, 'bbox': [264, 143, 478, 240], 'score': 0.41}, 'furniture_2': {'number': 2, 'bbox': [22, 278, 171, 427], 'score': 0.27}, 'furniture_3': {'number': 3, 'bbox': [337, 191, 429, 245], 'score': 0.27}, 'furniture_4': {'number': 4, 'bbox': [113, 253, 229, 409], 'score': 0.26}, 'furniture_5': {'number': 5, 'bbox': [464, 192, 514, 234], 'score': 0.26}}, 'staircase': {'staircase_1': {'number': 6, 'bbox': [0, 176, 334, 427], 'score': 0.31}, 'staircase_2': {'number': 7, 'bbox': [265, 95, 481, 239], 'score': 0.25}}})\n",
      "img_name:  108_n211324\n",
      "boxes_filt-- tensor([[502.2095, 137.4389, 640.0000, 336.2199]])\n",
      "pred_phrase-- ['garden(0.46)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'garden': {'garden_1': {'number': 1, 'bbox': [502, 137, 640, 336], 'score': 0.46}}})\n",
      "img_name:  109_n184385\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000, 640.0000, 426.9803]])\n",
      "pred_phrase-- ['appliance(0.47)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[421.4469,  47.7484, 503.7416, 136.5907]])\n",
      "pred_phrase-- ['utensil(0.53)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000, 301.3477, 146.5205],\n",
      "        [  0.0000,   0.0000, 640.0000, 424.7521]])\n",
      "pred_phrase-- ['stainless steel(0.44)', 'stainless steel(0.27)']\n",
      "=======================\n",
      "after sort [(0, 0, 0), (47, 421, 1), (0, 0, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[  0.0000,   0.0000, 640.0000, 426.9803]]), tensor([[421.4469,  47.0000, 503.7416, 136.5907]]), tensor([[  0.0000,   0.0000, 301.3477, 146.5205]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'appliance': {'appliance_1': {'number': 1, 'bbox': [0, 0, 640, 426], 'score': 0.47}}, 'utensil': {'utensil_1': {'number': 2, 'bbox': [421, 47, 503, 136], 'score': 0.53}}, 'stainless steel': {'stainless steel_1': {'number': 3, 'bbox': [0, 0, 301, 146], 'score': 0.44}}})\n",
      "img_name:  110_n500209\n",
      "boxes_filt-- tensor([[101.1246,   0.0000, 505.4959, 425.5645]])\n",
      "pred_phrase-- ['frame(0.43)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'frame': {'frame_1': {'number': 1, 'bbox': [101, 0, 505, 425], 'score': 0.43}}})\n",
      "img_name:  111_n125122\n",
      "boxes_filt-- tensor([[ 66.4449, 160.9159, 411.8765, 301.4879],\n",
      "        [  0.0000, 253.7581, 384.5639, 427.0000]])\n",
      "pred_phrase-- ['sinks(0.43)', 'sinks(0.33)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[187.5221,  48.4527, 311.0308, 193.2202]])\n",
      "pred_phrase-- ['mirror(0.32)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[455.7104, 113.4223, 529.8223, 229.1947],\n",
      "        [246.2286, 134.7568, 295.3158, 189.8716],\n",
      "        [232.7419, 166.1893, 259.7764, 192.4331]])\n",
      "pred_phrase-- ['chair(0.62)', 'chair(0.35)', 'chair(0.27)']\n",
      "=======================\n",
      "after sort [(160, 66, 0), (253, 0, 1), (48, 187, 2), (113, 455, 3), (118, 246, 4), (166, 232, 5)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 66.4449, 160.0000, 411.8765, 301.4879],\n",
      "        [  0.0000, 253.0000, 384.5639, 427.0000]]), tensor([[187.5221,  48.0000, 311.0308, 193.2202]]), tensor([[455.7104, 113.0000, 529.8223, 229.1947],\n",
      "        [246.2286, 118.0000, 295.3158, 189.8716],\n",
      "        [232.7419, 166.0000, 259.7764, 192.4331]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'sinks': {'sinks_1': {'number': 1, 'bbox': [66, 160, 411, 301], 'score': 0.43}, 'sinks_2': {'number': 2, 'bbox': [0, 253, 384, 427], 'score': 0.33}}, 'mirror': {'mirror_1': {'number': 3, 'bbox': [187, 48, 311, 193], 'score': 0.32}}, 'chair': {'chair_1': {'number': 4, 'bbox': [455, 113, 529, 229], 'score': 0.62}, 'chair_2': {'number': 5, 'bbox': [246, 118, 295, 189], 'score': 0.35}, 'chair_3': {'number': 6, 'bbox': [232, 166, 259, 192], 'score': 0.27}}})\n",
      "img_name:  112_n299528\n",
      "boxes_filt-- tensor([[283.1286, 273.2330, 384.1246, 323.4017]])\n",
      "pred_phrase-- ['skateboard(0.69)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'skateboard': {'skateboard_1': {'number': 1, 'bbox': [283, 273, 384, 323], 'score': 0.69}}})\n",
      "img_name:  113_n77818\n",
      "boxes_filt-- tensor([[  0.0000, 236.2688, 148.6577, 318.0713]])\n",
      "pred_phrase-- ['towels(0.38)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'towels': {'towels_1': {'number': 1, 'bbox': [0, 236, 148, 318], 'score': 0.38}}})\n",
      "img_name:  114_n356822\n",
      "boxes_filt-- tensor([[354.2312, 311.2461, 632.0178, 427.0000],\n",
      "        [236.3496, 235.8929, 392.2170, 427.0000],\n",
      "        [113.1253, 119.1756, 255.6633, 206.9444],\n",
      "        [  0.0000, 119.1181, 256.7841, 427.0000],\n",
      "        [  4.3776,  64.5519,  28.2588, 104.0766],\n",
      "        [  0.0000, 202.9724,  71.2970, 427.0000],\n",
      "        [133.1016,  56.1225, 169.6102,  92.9222]])\n",
      "pred_phrase-- ['clothing(0.49)', 'clothing(0.36)', 'clothing(0.33)', 'clothing(0.30)', 'clothing(0.30)', 'clothing(0.29)', 'clothing(0.28)']\n",
      "=======================\n",
      "after sort [(311, 354, 0), (235, 236, 1), (119, 113, 2), (64, 4, 3), (202, 0, 4), (56, 133, 5)]\n",
      "Adjusted collect_boxes_filt [tensor([[354.2312, 311.0000, 632.0178, 427.0000],\n",
      "        [236.3496, 235.0000, 392.2170, 427.0000],\n",
      "        [113.1253, 119.0000, 255.6633, 206.9444],\n",
      "        [  4.3776,  64.0000,  28.2588, 104.0766],\n",
      "        [  0.0000, 202.0000,  71.2970, 427.0000],\n",
      "        [133.1016,  56.0000, 169.6102,  92.9222]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'clothing': {'clothing_1': {'number': 1, 'bbox': [354, 311, 632, 427], 'score': 0.49}, 'clothing_2': {'number': 2, 'bbox': [236, 235, 392, 427], 'score': 0.36}, 'clothing_3': {'number': 3, 'bbox': [113, 119, 255, 206], 'score': 0.33}, 'clothing_4': {'number': 4, 'bbox': [4, 64, 28, 104], 'score': 0.3}, 'clothing_5': {'number': 5, 'bbox': [0, 202, 71, 427], 'score': 0.29}}})\n",
      "img_name:  115_n125122\n",
      "boxes_filt-- tensor([[187.5221,  48.4527, 311.0308, 193.2202]])\n",
      "pred_phrase-- ['mirror(0.32)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  0.0000,  32.4385, 161.7390, 202.6303],\n",
      "        [ 18.3541,  80.9943, 127.5509, 150.0385]])\n",
      "pred_phrase-- ['paintings(0.30)', 'paintings(0.29)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[485.0904, 109.6548, 640.0000, 335.8063],\n",
      "        [  0.0000, 252.8034, 384.4265, 427.0000]])\n",
      "pred_phrase-- ['table(0.60)', 'table(0.31)']\n",
      "=======================\n",
      "after sort [(48, 187, 0), (80, 18, 1), (109, 485, 2), (252, 0, 3)]\n",
      "Adjusted collect_boxes_filt [tensor([[187.5221,  48.0000, 311.0308, 193.2202]]), tensor([[ 18.3541,  80.0000, 127.5509, 150.0385]]), tensor([[485.0904, 109.0000, 640.0000, 335.8063],\n",
      "        [  0.0000, 252.0000, 384.4265, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'mirror': {'mirror_1': {'number': 1, 'bbox': [187, 48, 311, 193], 'score': 0.32}}, 'paintings': {'paintings_1': {'number': 2, 'bbox': [18, 80, 127, 150], 'score': 0.29}}, 'table': {'table_1': {'number': 3, 'bbox': [485, 109, 640, 335], 'score': 0.6}, 'table_2': {'number': 4, 'bbox': [0, 252, 384, 427], 'score': 0.31}}})\n",
      "img_name:  116_n562105\n",
      "boxes_filt-- tensor([[ 16.9226,  17.3540, 623.8903, 416.3489]])\n",
      "pred_phrase-- ['stadium(0.53)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'stadium': {'stadium_1': {'number': 1, 'bbox': [16, 17, 623, 416], 'score': 0.53}}})\n",
      "img_name:  117_n310828\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000, 295.2993, 427.0000]])\n",
      "pred_phrase-- ['person(0.86)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'person': {'person_1': {'number': 1, 'bbox': [0, 0, 295, 427], 'score': 0.86}}})\n",
      "img_name:  118_n309148\n",
      "boxes_filt-- tensor([[112.5173,  86.5379, 571.2826, 403.6939]])\n",
      "pred_phrase-- ['fire truck(0.89)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'fire truck': {'fire truck_1': {'number': 1, 'bbox': [112, 86, 571, 403], 'score': 0.89}}})\n",
      "img_name:  119_n65202\n",
      "boxes_filt-- tensor([[275.6355, 114.0954, 436.6399, 236.8098]])\n",
      "pred_phrase-- ['wristwatch(0.70)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'wristwatch': {'wristwatch_1': {'number': 1, 'bbox': [275, 114, 436, 236], 'score': 0.7}}})\n",
      "img_name:  120_n296467\n",
      "boxes_filt-- tensor([[  0.0000,   8.7155, 640.0000, 414.7776]])\n",
      "pred_phrase-- ['pasta(0.42)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[323.2530,  65.3155, 467.5308, 226.0799],\n",
      "        [ 29.4235,  38.4613, 316.5526, 209.7618],\n",
      "        [ 30.0517, 210.7828, 191.4132, 374.2171],\n",
      "        [455.3632,  41.1936, 591.7411, 207.8583],\n",
      "        [184.5606, 210.6210, 317.3149, 375.4305],\n",
      "        [495.0280, 215.6762, 618.4680, 373.4676],\n",
      "        [318.9048, 220.0240, 618.1501, 381.1417],\n",
      "        [318.7887, 225.2897, 424.5338, 381.1781],\n",
      "        [407.1154, 224.4103, 519.1097, 378.2761],\n",
      "        [ 26.3659,  32.2700, 618.9463, 384.4758]])\n",
      "pred_phrase-- ['food(0.47)', 'food(0.45)', 'food(0.43)', 'food(0.42)', 'food(0.41)', 'food(0.38)', 'food(0.36)', 'food(0.35)', 'food(0.34)', 'food(0.32)']\n",
      "=======================\n",
      "after sort [(0, 0, 0), (65, 323, 1), (38, 29, 2), (210, 30, 3), (41, 455, 4), (210, 184, 5), (215, 495, 6), (225, 318, 7), (224, 407, 8)]\n",
      "Adjusted collect_boxes_filt [tensor([[  0.0000,   0.0000, 640.0000, 414.7776]]), tensor([[323.2530,  65.0000, 467.5308, 226.0799],\n",
      "        [ 29.4235,  38.0000, 316.5526, 209.7618],\n",
      "        [ 30.0517, 210.0000, 191.4132, 374.2171],\n",
      "        [455.3632,  41.0000, 591.7411, 207.8583],\n",
      "        [184.5606, 210.0000, 317.3149, 375.4305],\n",
      "        [495.0280, 215.0000, 618.4680, 373.4676],\n",
      "        [318.7887, 225.0000, 424.5338, 381.1781],\n",
      "        [407.1154, 224.0000, 519.1097, 378.2761]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'pasta': {'pasta_1': {'number': 1, 'bbox': [0, 0, 640, 414], 'score': 0.42}}, 'food': {'food_1': {'number': 2, 'bbox': [323, 65, 467, 226], 'score': 0.47}, 'food_2': {'number': 3, 'bbox': [29, 38, 316, 209], 'score': 0.45}, 'food_3': {'number': 4, 'bbox': [30, 210, 191, 374], 'score': 0.43}, 'food_4': {'number': 5, 'bbox': [455, 41, 591, 207], 'score': 0.42}, 'food_5': {'number': 6, 'bbox': [184, 210, 317, 375], 'score': 0.41}}})\n",
      "img_name:  121_n54424\n",
      "boxes_filt-- tensor([[  0.,   0., 640., 427.]])\n",
      "pred_phrase-- ['couch(0.78)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'couch': {'couch_1': {'number': 1, 'bbox': [0, 0, 640, 427], 'score': 0.78}}})\n",
      "img_name:  122_n538039\n",
      "boxes_filt-- tensor([[464.5302, 127.1240, 598.4920, 222.5900],\n",
      "        [  0.0000, 161.9070,  30.7528, 242.6465],\n",
      "        [613.9328, 150.8434, 640.0000, 222.3254],\n",
      "        [561.9968,  90.3823, 614.5153, 116.4888]])\n",
      "pred_phrase-- ['car(0.48)', 'car(0.39)', 'car(0.30)', 'car(0.30)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[464.4050, 127.1233, 598.5396, 222.6855]])\n",
      "pred_phrase-- ['truck(0.50)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[325.6206,  68.8706, 477.0360, 390.5162]])\n",
      "pred_phrase-- ['color(0.54)']\n",
      "=======================\n",
      "after sort [(79, 464, 0), (161, 0, 1), (150, 613, 2), (90, 561, 3), (127, 464, 4), (68, 325, 5)]\n",
      "Adjusted collect_boxes_filt [tensor([[464.5302,  79.0000, 598.4920, 222.5900],\n",
      "        [  0.0000, 161.0000,  30.7528, 242.6465],\n",
      "        [613.9328, 150.0000, 640.0000, 222.3254],\n",
      "        [561.9968,  90.0000, 614.5153, 116.4888]]), tensor([[464.4050, 127.0000, 598.5396, 222.6855]]), tensor([[325.6206,  68.0000, 477.0360, 390.5162]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'car': {'car_1': {'number': 1, 'bbox': [464, 79, 598, 222], 'score': 0.48}, 'car_2': {'number': 2, 'bbox': [0, 161, 30, 242], 'score': 0.39}, 'car_3': {'number': 3, 'bbox': [613, 150, 640, 222], 'score': 0.3}, 'car_4': {'number': 4, 'bbox': [561, 90, 614, 116], 'score': 0.3}}, 'truck': {'truck_1': {'number': 5, 'bbox': [464, 127, 598, 222], 'score': 0.5}}, 'color': {'color_1': {'number': 6, 'bbox': [325, 68, 477, 390], 'score': 0.54}}})\n",
      "img_name:  123_n62458\n",
      "boxes_filt-- tensor([[ 61.3292,  15.2930, 580.2524, 407.3409],\n",
      "        [513.0742, 298.0784, 598.6329, 416.3960]])\n",
      "pred_phrase-- ['clay pots(0.38)', 'clay pots(0.26)']\n",
      "=======================\n",
      "after sort [(15, 61, 0), (298, 513, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 61.3292,  15.0000, 580.2524, 407.3409],\n",
      "        [513.0742, 298.0000, 598.6329, 416.3960]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'clay pots': {'clay pots_1': {'number': 1, 'bbox': [61, 15, 580, 407], 'score': 0.38}, 'clay pots_2': {'number': 2, 'bbox': [513, 298, 598, 416], 'score': 0.26}}})\n",
      "img_name:  124_n531731\n",
      "boxes_filt-- tensor([[124.1441,  80.5166, 200.8903, 134.3762],\n",
      "        [ 93.9330,  83.9570, 279.4017, 367.1024]])\n",
      "pred_phrase-- ['baseball bat(0.44)', 'baseball bat(0.28)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'baseball bat': {'baseball bat_1': {'number': 1, 'bbox': [124, 80, 200, 134], 'score': 0.44}}})\n",
      "img_name:  125_n88366\n",
      "boxes_filt-- tensor([[273.9500, 133.3006, 359.6989, 257.2585],\n",
      "        [157.7926, 207.3873, 254.5109, 372.8269]])\n",
      "pred_phrase-- ['person(0.74)', 'person(0.73)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[193.2945, 207.6478, 227.6191, 246.2098],\n",
      "        [300.0456, 134.0864, 325.4398, 165.6248]])\n",
      "pred_phrase-- ['helmet(0.51)', 'helmet(0.44)']\n",
      "=======================\n",
      "after sort [(86, 273, 0), (159, 157, 1), (207, 193, 2), (134, 300, 3)]\n",
      "Adjusted collect_boxes_filt [tensor([[273.9500,  86.0000, 359.6989, 257.2585],\n",
      "        [157.7926, 159.0000, 254.5109, 372.8269]]), tensor([[193.2945, 207.0000, 227.6191, 246.2098],\n",
      "        [300.0456, 134.0000, 325.4398, 165.6248]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'person': {'person_1': {'number': 1, 'bbox': [273, 86, 359, 257], 'score': 0.74}, 'person_2': {'number': 2, 'bbox': [157, 159, 254, 372], 'score': 0.73}}, 'helmet': {'helmet_1': {'number': 3, 'bbox': [193, 207, 227, 246], 'score': 0.51}, 'helmet_2': {'number': 4, 'bbox': [300, 134, 325, 165], 'score': 0.44}}})\n",
      "img_name:  126_n410476\n",
      "boxes_filt-- tensor([[281.1212,  42.3311, 486.5882, 412.7271],\n",
      "        [163.4903, 130.9296, 294.0754, 170.1126]])\n",
      "pred_phrase-- ['animal(0.59)', 'animal(0.38)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[281.1469,  43.0207, 487.1882, 413.1783]])\n",
      "pred_phrase-- ['middle(0.83)']\n",
      "=======================\n",
      "after sort [(0, 281, 0), (130, 163, 1), (43, 281, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[281.1212,   0.0000, 486.5882, 412.7271],\n",
      "        [163.4903, 130.0000, 294.0754, 170.1126]]), tensor([[281.1469,  43.0000, 487.1882, 413.1783]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'animal': {'animal_1': {'number': 1, 'bbox': [281, 0, 486, 412], 'score': 0.59}, 'animal_2': {'number': 2, 'bbox': [163, 130, 294, 170], 'score': 0.38}}, 'middle': {'middle_1': {'number': 3, 'bbox': [281, 43, 487, 413], 'score': 0.83}}})\n",
      "img_name:  127_n414992\n",
      "boxes_filt-- tensor([[138.1532, 285.7393, 201.5688, 427.0000],\n",
      "        [150.4804, 342.3499, 192.0300, 388.6284],\n",
      "        [426.2498, 347.3133, 471.8921, 375.8484],\n",
      "        [390.7059, 239.7592, 507.0707, 427.0000]])\n",
      "pred_phrase-- ['shorts(0.34)', 'shorts(0.30)', 'shorts(0.30)', 'shorts(0.28)']\n",
      "=======================\n",
      "after sort [(342, 150, 0), (347, 426, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[150.4804, 342.0000, 192.0300, 388.6284],\n",
      "        [426.2498, 347.0000, 471.8921, 375.8484]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'shorts': {'shorts_1': {'number': 1, 'bbox': [150, 342, 192, 388], 'score': 0.3}, 'shorts_2': {'number': 2, 'bbox': [426, 347, 471, 375], 'score': 0.3}}})\n",
      "img_name:  128_n146555\n",
      "boxes_filt-- tensor([[ 85.0306, 219.0694, 399.9952, 392.4630]])\n",
      "pred_phrase-- ['large cow(0.95)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'large cow': {'large cow_1': {'number': 1, 'bbox': [85, 219, 399, 392], 'score': 0.95}}})\n",
      "img_name:  129_n334278\n",
      "boxes_filt-- tensor([[304.7189, 136.0904, 408.5429, 334.4419],\n",
      "        [370.2495, 152.9193, 585.2126, 367.0410]])\n",
      "pred_phrase-- ['uniform(0.33)', 'uniform(0.29)']\n",
      "=======================\n",
      "after sort [(104, 304, 0), (152, 370, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[304.7189, 104.0000, 408.5429, 334.4419],\n",
      "        [370.2495, 152.0000, 585.2126, 367.0410]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'uniform': {'uniform_1': {'number': 1, 'bbox': [304, 104, 408, 334], 'score': 0.33}, 'uniform_2': {'number': 2, 'bbox': [370, 152, 585, 367], 'score': 0.29}}})\n",
      "img_name:  130_n145498\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000, 640.0000, 427.0000],\n",
      "        [346.5400,  53.4323, 456.5993, 180.6000],\n",
      "        [298.9596, 305.2651, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['furniture(0.47)', 'furniture(0.32)', 'furniture(0.27)']\n",
      "=======================\n",
      "after sort [(53, 346, 0), (305, 298, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[346.5400,  53.0000, 456.5993, 180.6000],\n",
      "        [298.9596, 305.0000, 640.0000, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'furniture': {'furniture_1': {'number': 1, 'bbox': [346, 53, 456, 180], 'score': 0.32}, 'furniture_2': {'number': 2, 'bbox': [298, 305, 640, 427], 'score': 0.27}}})\n",
      "img_name:  131_n66756\n",
      "boxes_filt-- tensor([[203.0176, 122.3766, 350.5270, 315.1927],\n",
      "        [ 86.2552, 187.7549, 198.9581, 303.8174],\n",
      "        [ 15.6079, 152.9110, 106.2266, 306.8266],\n",
      "        [417.4129,  53.9215, 469.1182, 162.7132],\n",
      "        [258.0989,  75.5789, 313.7346, 128.3159],\n",
      "        [208.3206,  71.7453, 271.1992, 128.5246],\n",
      "        [310.2437,  70.3311, 359.0264, 128.0076],\n",
      "        [ 48.3356,  95.1677,  85.8489, 128.2120],\n",
      "        [ 90.1057,  87.5043, 120.0815, 128.7407],\n",
      "        [128.2599,  77.8830, 167.4685, 129.6557],\n",
      "        [285.2923,  51.9093, 335.3705, 110.7218],\n",
      "        [210.7182,  52.7461, 260.9186, 102.1412],\n",
      "        [478.7272,  56.3570, 515.1272, 126.0042],\n",
      "        [136.0058,   0.0000, 179.0012,  47.7271],\n",
      "        [183.1186,  84.9093, 213.8188, 128.6789],\n",
      "        [  0.0000,   3.0586,  39.7255,  53.8433],\n",
      "        [ 20.8865,  27.9531,  69.3376,  91.7592],\n",
      "        [  0.0000,  30.4601,  25.7612,  88.9382],\n",
      "        [ 41.6361,   0.0000,  93.5374,  54.7317],\n",
      "        [156.3435,  57.9251, 208.4209, 125.6666],\n",
      "        [609.9343,  13.2333, 640.0000,  59.5193],\n",
      "        [351.2171,   0.0000, 412.2492,  77.9063],\n",
      "        [558.5491,   0.0000, 618.9252,  62.4525],\n",
      "        [222.8862,   0.0000, 263.9346,  30.1256],\n",
      "        [485.9439,   0.0000, 535.7155,  63.8284],\n",
      "        [210.1652,   0.0000, 280.4449,  51.4252],\n",
      "        [309.2778,   0.0000, 353.2658,  34.2225],\n",
      "        [175.5266,  57.3453, 203.1558,  88.6273]])\n",
      "pred_phrase-- ['person(0.76)', 'person(0.68)', 'person(0.66)', 'person(0.54)', 'person(0.52)', 'person(0.51)', 'person(0.49)', 'person(0.48)', 'person(0.48)', 'person(0.48)', 'person(0.47)', 'person(0.47)', 'person(0.46)', 'person(0.45)', 'person(0.44)', 'person(0.41)', 'person(0.38)', 'person(0.38)', 'person(0.34)', 'person(0.31)', 'person(0.30)', 'person(0.29)', 'person(0.29)', 'person(0.28)', 'person(0.28)', 'person(0.26)', 'person(0.26)', 'person(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 86.1181, 187.8739, 202.1836, 304.1790]])\n",
      "pred_phrase-- ['catcher(0.96)']\n",
      "=======================\n",
      "after sort [(122, 203, 0), (139, 86, 1), (91, 15, 2), (8, 417, 3), (75, 258, 4), (4, 208, 5), (27, 310, 6), (43, 48, 7), (47, 90, 8), (0, 128, 9), (0, 285, 10), (26, 210, 11), (56, 478, 12), (0, 136, 13), (74, 183, 14), (0, 0, 15), (0, 20, 16), (30, 0, 17), (0, 41, 18), (13, 609, 19), (0, 351, 20), (0, 558, 21), (0, 222, 22), (0, 485, 23), (0, 309, 24), (0, 175, 25), (187, 86, 26)]\n",
      "Adjusted collect_boxes_filt [tensor([[203.0176, 122.0000, 350.5270, 315.1927],\n",
      "        [ 86.2552, 139.0000, 198.9581, 303.8174],\n",
      "        [ 15.6079,  91.0000, 106.2266, 306.8266],\n",
      "        [417.4129,   8.0000, 469.1182, 162.7132],\n",
      "        [258.0989,  75.0000, 313.7346, 128.3159],\n",
      "        [208.3206,   4.0000, 271.1992, 128.5246],\n",
      "        [310.2437,  27.0000, 359.0264, 128.0076],\n",
      "        [ 48.3356,  43.0000,  85.8489, 128.2120],\n",
      "        [ 90.1057,  47.0000, 120.0815, 128.7407],\n",
      "        [128.2599,   0.0000, 167.4685, 129.6557],\n",
      "        [285.2923,   0.0000, 335.3705, 110.7218],\n",
      "        [210.7182,  26.0000, 260.9186, 102.1412],\n",
      "        [478.7272,  56.0000, 515.1272, 126.0042],\n",
      "        [136.0058,   0.0000, 179.0012,  47.7271],\n",
      "        [183.1186,  74.0000, 213.8188, 128.6789],\n",
      "        [  0.0000,   0.0000,  39.7255,  53.8433],\n",
      "        [ 20.8865,   0.0000,  69.3376,  91.7592],\n",
      "        [  0.0000,  30.0000,  25.7612,  88.9382],\n",
      "        [ 41.6361,   0.0000,  93.5374,  54.7317],\n",
      "        [609.9343,  13.0000, 640.0000,  59.5193],\n",
      "        [351.2171,   0.0000, 412.2492,  77.9063],\n",
      "        [558.5491,   0.0000, 618.9252,  62.4525],\n",
      "        [222.8862,   0.0000, 263.9346,  30.1256],\n",
      "        [485.9439,   0.0000, 535.7155,  63.8284],\n",
      "        [309.2778,   0.0000, 353.2658,  34.2225],\n",
      "        [175.5266,   0.0000, 203.1558,  88.6273]]), tensor([[ 86.1181, 187.0000, 202.1836, 304.1790]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'person': {'person_1': {'number': 1, 'bbox': [203, 122, 350, 315], 'score': 0.76}, 'person_2': {'number': 2, 'bbox': [86, 139, 198, 303], 'score': 0.68}, 'person_3': {'number': 3, 'bbox': [15, 91, 106, 306], 'score': 0.66}, 'person_4': {'number': 4, 'bbox': [417, 8, 469, 162], 'score': 0.54}, 'person_5': {'number': 5, 'bbox': [258, 75, 313, 128], 'score': 0.52}}, 'catcher': {'catcher_1': {'number': 6, 'bbox': [86, 187, 202, 304], 'score': 0.96}}})\n",
      "img_name:  132_n310625\n",
      "boxes_filt-- tensor([[ 66.3823, 110.9306, 146.7875, 193.5057]])\n",
      "pred_phrase-- ['toothpaste(0.61)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'toothpaste': {'toothpaste_1': {'number': 1, 'bbox': [66, 110, 146, 193], 'score': 0.61}}})\n",
      "img_name:  133_n538684\n",
      "obj_dict {}\n",
      "img_name:  134_n243701\n",
      "boxes_filt-- tensor([[  0.0000, 139.2250, 640.0000, 426.4366]])\n",
      "pred_phrase-- ['black road(0.61)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'black road': {'black road_1': {'number': 1, 'bbox': [0, 139, 640, 426], 'score': 0.61}}})\n",
      "img_name:  135_n23181\n",
      "boxes_filt-- tensor([[276.0716, 201.4953, 623.2144, 427.0000],\n",
      "        [ 97.1335, 186.7036, 278.1863, 370.9846],\n",
      "        [349.4397, 146.2018, 368.3157, 198.3021]])\n",
      "pred_phrase-- ['sculpture(0.27)', 'sculpture(0.26)', 'sculpture(0.25)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[279.7952, 177.8636, 396.7456, 372.9943]])\n",
      "pred_phrase-- ['fireplace(0.43)']\n",
      "=======================\n",
      "after sort [(201, 276, 0), (186, 97, 1), (105, 349, 2), (153, 279, 3)]\n",
      "Adjusted collect_boxes_filt [tensor([[276.0716, 201.0000, 623.2144, 427.0000],\n",
      "        [ 97.1335, 186.0000, 278.1863, 370.9846],\n",
      "        [349.4397, 105.0000, 368.3157, 198.3021]]), tensor([[279.7952, 153.0000, 396.7456, 372.9943]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'sculpture': {'sculpture_1': {'number': 1, 'bbox': [276, 201, 623, 427], 'score': 0.27}, 'sculpture_2': {'number': 2, 'bbox': [97, 186, 278, 370], 'score': 0.26}, 'sculpture_3': {'number': 3, 'bbox': [349, 105, 368, 198], 'score': 0.25}}, 'fireplace': {'fireplace_1': {'number': 4, 'bbox': [279, 153, 396, 372], 'score': 0.43}}})\n",
      "img_name:  136_n28996\n",
      "boxes_filt-- tensor([[180.9319,  98.4593, 395.7484, 426.7877],\n",
      "        [359.6661,  59.1302, 640.0000, 426.9172],\n",
      "        [  0.0000,  47.7730, 231.6470, 427.0000]])\n",
      "pred_phrase-- ['food(0.36)', 'food(0.29)', 'food(0.28)']\n",
      "=======================\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  cheese\n",
      "after sort [(98, 180, 0), (59, 359, 1), (47, 0, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[180.9319,  98.0000, 395.7484, 426.7877],\n",
      "        [359.6661,  59.0000, 640.0000, 426.9172],\n",
      "        [  0.0000,  47.0000, 231.6470, 427.0000]]), tensor([], size=(0, 4))]\n",
      "obj_dict defaultdict(<class 'dict'>, {'food': {'food_1': {'number': 1, 'bbox': [180, 98, 395, 426], 'score': 0.36}, 'food_2': {'number': 2, 'bbox': [359, 59, 640, 426], 'score': 0.29}, 'food_3': {'number': 3, 'bbox': [0, 47, 231, 427], 'score': 0.28}}})\n",
      "img_name:  137_n513429\n",
      "boxes_filt-- tensor([[  0.0000, 151.1750, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['desk(0.68)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'desk': {'desk_1': {'number': 1, 'bbox': [0, 151, 640, 427], 'score': 0.68}}})\n",
      "img_name:  138_n513429\n",
      "boxes_filt-- tensor([[  0.0000, 152.9996, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['furniture(0.78)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000, 357.7477, 181.4185]])\n",
      "pred_phrase-- ['curtain(0.43)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[403.4967,  53.2624, 599.4403, 201.3096]])\n",
      "pred_phrase-- ['monitor(0.75)']\n",
      "=======================\n",
      "after sort [(152, 0, 0), (0, 0, 1), (53, 403, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[  0., 152., 640., 427.]]), tensor([[  0.0000,   0.0000, 357.7477, 181.4185]]), tensor([[403.4967,  53.0000, 599.4403, 201.3096]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'furniture': {'furniture_1': {'number': 1, 'bbox': [0, 152, 640, 427], 'score': 0.78}}, 'curtain': {'curtain_1': {'number': 2, 'bbox': [0, 0, 357, 181], 'score': 0.43}}, 'monitor': {'monitor_1': {'number': 3, 'bbox': [403, 53, 599, 201], 'score': 0.75}}})\n",
      "img_name:  139_n413002\n",
      "boxes_filt-- tensor([[131.1862,  41.6993, 367.7420, 368.7590],\n",
      "        [321.9078,  84.6037, 563.6622, 397.0930]])\n",
      "pred_phrase-- ['animal(0.47)', 'animal(0.35)']\n",
      "=======================\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  backpack\n",
      "after sort [(41, 131, 0), (84, 321, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[131.1862,  41.0000, 367.7420, 368.7590],\n",
      "        [321.9078,  84.0000, 563.6622, 397.0930]]), tensor([], size=(0, 4))]\n",
      "obj_dict defaultdict(<class 'dict'>, {'animal': {'animal_1': {'number': 1, 'bbox': [131, 41, 367, 368], 'score': 0.47}, 'animal_2': {'number': 2, 'bbox': [321, 84, 563, 397], 'score': 0.35}}})\n",
      "img_name:  140_n181355\n",
      "boxes_filt-- tensor([[354.4991, 353.1400, 418.9402, 386.3431],\n",
      "        [320.4097, 309.0855, 350.4745, 344.5963]])\n",
      "pred_phrase-- ['remote(0.29)', 'remote(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[324.3640,  95.6089, 549.8693, 424.8501],\n",
      "        [ 45.8754,  90.4719, 232.7142, 300.0885],\n",
      "        [183.8257, 103.5854, 369.0479, 309.6085]])\n",
      "pred_phrase-- ['plastic(0.37)', 'plastic(0.28)', 'plastic(0.26)']\n",
      "=======================\n",
      "after sort [(353, 354, 0), (305, 320, 1), (95, 324, 2), (90, 45, 3), (103, 183, 4)]\n",
      "Adjusted collect_boxes_filt [tensor([[354.4991, 353.0000, 418.9402, 386.3431],\n",
      "        [320.4097, 305.0000, 350.4745, 344.5963]]), tensor([[324.3640,  95.0000, 549.8693, 424.8501],\n",
      "        [ 45.8754,  90.0000, 232.7142, 300.0885],\n",
      "        [183.8257, 103.0000, 369.0479, 309.6085]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'remote': {'remote_1': {'number': 1, 'bbox': [354, 353, 418, 386], 'score': 0.29}, 'remote_2': {'number': 2, 'bbox': [320, 305, 350, 344], 'score': 0.26}}, 'plastic': {'plastic_1': {'number': 3, 'bbox': [324, 95, 549, 424], 'score': 0.37}, 'plastic_2': {'number': 4, 'bbox': [45, 90, 232, 300], 'score': 0.28}, 'plastic_3': {'number': 5, 'bbox': [183, 103, 369, 309], 'score': 0.26}}})\n",
      "img_name:  141_n507959\n",
      "boxes_filt-- tensor([[ 68.2787, 177.8554, 283.1886, 351.1706]])\n",
      "pred_phrase-- ['knife blocks(0.31)']\n",
      "=======================\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  diapers\n",
      "obj_dict defaultdict(<class 'dict'>, {'knife blocks': {'knife blocks_1': {'number': 1, 'bbox': [68, 177, 283, 351], 'score': 0.31}}})\n",
      "img_name:  142_n355567\n",
      "boxes_filt-- tensor([[384.7606, 205.7175, 487.8627, 425.0063]])\n",
      "pred_phrase-- ['lady(0.76)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[384.9178, 205.4961, 532.9865, 425.1837],\n",
      "        [  0.0000, 167.5382, 527.2172, 244.0062]])\n",
      "pred_phrase-- ['antique(0.34)', 'antique(0.25)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[384.5811, 205.6380, 487.6661, 425.3637]])\n",
      "pred_phrase-- ['playing(0.41)']\n",
      "=======================\n",
      "after sort [(109, 384, 0), (157, 384, 1), (167, 0, 2), (205, 384, 3)]\n",
      "Adjusted collect_boxes_filt [tensor([[384.7606, 109.0000, 487.8627, 425.0063]]), tensor([[384.9178, 157.0000, 532.9865, 425.1837],\n",
      "        [  0.0000, 167.0000, 527.2172, 244.0062]]), tensor([[384.5811, 205.0000, 487.6661, 425.3637]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'lady': {'lady_1': {'number': 1, 'bbox': [384, 109, 487, 425], 'score': 0.76}}, 'antique': {'antique_1': {'number': 2, 'bbox': [384, 157, 532, 425], 'score': 0.34}, 'antique_2': {'number': 3, 'bbox': [0, 167, 527, 244], 'score': 0.25}}, 'playing': {'playing_1': {'number': 4, 'bbox': [384, 205, 487, 425], 'score': 0.41}}})\n",
      "img_name:  143_n64959\n",
      "boxes_filt-- tensor([[370.0859, 138.6326, 475.8682, 367.9656],\n",
      "        [266.5477, 181.8516, 351.4087, 316.8570]])\n",
      "pred_phrase-- ['round door(0.34)', 'round door(0.31)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 67.9549, 113.2841, 115.5553, 238.0822]])\n",
      "pred_phrase-- ['window(0.82)']\n",
      "=======================\n",
      "after sort [(138, 370, 0), (181, 266, 1), (113, 67, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[370.0859, 138.0000, 475.8682, 367.9656],\n",
      "        [266.5477, 181.0000, 351.4087, 316.8570]]), tensor([[ 67.9549, 113.0000, 115.5553, 238.0822]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'round door': {'round door_1': {'number': 1, 'bbox': [370, 138, 475, 367], 'score': 0.34}, 'round door_2': {'number': 2, 'bbox': [266, 181, 351, 316], 'score': 0.31}}, 'window': {'window_1': {'number': 3, 'bbox': [67, 113, 115, 238], 'score': 0.82}}})\n",
      "img_name:  144_n326988\n",
      "boxes_filt-- tensor([[  0.0000,  54.6447, 148.2826, 427.0000]])\n",
      "pred_phrase-- ['man(0.94)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'man': {'man_1': {'number': 1, 'bbox': [0, 54, 148, 427], 'score': 0.94}}})\n",
      "img_name:  145_n273901\n",
      "boxes_filt-- tensor([[ 38.7710, 102.1008, 389.7381, 297.8241],\n",
      "        [321.4528, 122.2270, 614.7751, 288.8883]])\n",
      "pred_phrase-- ['bus(0.64)', 'bus(0.42)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 38.7757, 102.2929, 389.8194, 297.9020],\n",
      "        [375.5396, 122.5849, 614.5625, 289.1741]])\n",
      "pred_phrase-- ['other bus(0.55)', 'other bus(0.39)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 38.4578, 102.5936, 391.2924, 297.2176]])\n",
      "pred_phrase-- ['green color(0.82)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 37.9690, 101.9134, 390.4914, 297.9544],\n",
      "        [376.7126, 122.7251, 614.9597, 289.5581]])\n",
      "pred_phrase-- ['rectangular shape(0.48)', 'rectangular shape(0.38)']\n",
      "=======================\n",
      "after sort [(6, 38, 0), (26, 321, 1), (54, 38, 2), (74, 375, 3), (102, 38, 4), (0, 37, 5), (122, 376, 6)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 38.7710,   6.0000, 389.7381, 297.8241],\n",
      "        [321.4528,  26.0000, 614.7751, 288.8883]]), tensor([[ 38.7757,  54.0000, 389.8194, 297.9020],\n",
      "        [375.5396,  74.0000, 614.5625, 289.1741]]), tensor([[ 38.4578, 102.0000, 391.2924, 297.2176]]), tensor([[ 37.9690,   0.0000, 390.4914, 297.9544],\n",
      "        [376.7126, 122.0000, 614.9597, 289.5581]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'bus': {'bus_1': {'number': 1, 'bbox': [38, 6, 389, 297], 'score': 0.64}, 'bus_2': {'number': 2, 'bbox': [321, 26, 614, 288], 'score': 0.42}}, 'other bus': {'other bus_1': {'number': 3, 'bbox': [38, 54, 389, 297], 'score': 0.55}, 'other bus_2': {'number': 4, 'bbox': [375, 74, 614, 289], 'score': 0.39}}, 'green color': {'green color_1': {'number': 5, 'bbox': [38, 102, 391, 297], 'score': 0.82}}, 'rectangular shape': {'rectangular shape_1': {'number': 6, 'bbox': [37, 0, 390, 297], 'score': 0.48}, 'rectangular shape_2': {'number': 7, 'bbox': [376, 122, 614, 289], 'score': 0.38}}})\n",
      "img_name:  146_n526228\n",
      "boxes_filt-- tensor([[ 58.2993, 136.6594, 395.4033, 422.5405]])\n",
      "pred_phrase-- ['man(0.70)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 56.3184, 136.7893, 395.5221, 425.1750],\n",
      "        [294.7042, 147.3009, 591.2556, 416.6150]])\n",
      "pred_phrase-- ['frame(0.36)', 'frame(0.26)']\n",
      "=======================\n",
      "after sort [(88, 58, 0), (136, 56, 1), (147, 294, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 58.2993,  88.0000, 395.4033, 422.5405]]), tensor([[ 56.3184, 136.0000, 395.5221, 425.1750],\n",
      "        [294.7042, 147.0000, 591.2556, 416.6150]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'man': {'man_1': {'number': 1, 'bbox': [58, 88, 395, 422], 'score': 0.7}}, 'frame': {'frame_1': {'number': 2, 'bbox': [56, 136, 395, 425], 'score': 0.36}, 'frame_2': {'number': 3, 'bbox': [294, 147, 591, 416], 'score': 0.26}}})\n",
      "img_name:  147_n234683\n",
      "boxes_filt-- tensor([[  0.0000, 297.3705, 101.2199, 343.2225]])\n",
      "pred_phrase-- ['black phone(0.48)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'black phone': {'black phone_1': {'number': 1, 'bbox': [0, 297, 101, 343], 'score': 0.48}}})\n",
      "img_name:  148_n186491\n",
      "boxes_filt-- tensor([[ 45.7678,  47.2337, 640.0000, 426.7947]])\n",
      "pred_phrase-- ['table(0.81)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'table': {'table_1': {'number': 1, 'bbox': [45, 47, 640, 426], 'score': 0.81}}})\n",
      "img_name:  149_n335542\n",
      "boxes_filt-- tensor([[427.6571,   9.3147, 618.9865, 391.4342]])\n",
      "pred_phrase-- ['bear(0.80)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'bear': {'bear_1': {'number': 1, 'bbox': [427, 9, 618, 391], 'score': 0.8}}})\n",
      "img_name:  150_n86120\n",
      "boxes_filt-- tensor([[205.2092, 208.2790, 538.8881, 427.0000],\n",
      "        [ 88.2974, 118.8605, 254.2205, 359.1002]])\n",
      "pred_phrase-- ['animals(0.42)', 'animals(0.31)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[215.1019,   0.0000, 535.9595, 402.5982]])\n",
      "pred_phrase-- ['she(0.42)']\n",
      "=======================\n",
      "after sort [(208, 205, 0), (118, 88, 1), (0, 215, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[205.2092, 208.0000, 538.8881, 427.0000],\n",
      "        [ 88.2974, 118.0000, 254.2205, 359.1002]]), tensor([[215.1019,   0.0000, 535.9595, 402.5982]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'animals': {'animals_1': {'number': 1, 'bbox': [205, 208, 538, 427], 'score': 0.42}, 'animals_2': {'number': 2, 'bbox': [88, 118, 254, 359], 'score': 0.31}}, 'she': {'she_1': {'number': 3, 'bbox': [215, 0, 535, 402], 'score': 0.42}}})\n",
      "img_name:  151_n86120\n",
      "boxes_filt-- tensor([[204.9324, 208.4985, 538.9280, 427.0000],\n",
      "        [ 88.3323, 118.9279, 254.2120, 359.2454]])\n",
      "pred_phrase-- ['goats(0.43)', 'goats(0.28)']\n",
      "=======================\n",
      "after sort [(208, 204, 0), (118, 88, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[204.9324, 208.0000, 538.9280, 427.0000],\n",
      "        [ 88.3323, 118.0000, 254.2120, 359.2454]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'goats': {'goats_1': {'number': 1, 'bbox': [204, 208, 538, 427], 'score': 0.43}, 'goats_2': {'number': 2, 'bbox': [88, 118, 254, 359], 'score': 0.28}}})\n",
      "img_name:  152_n379991\n",
      "boxes_filt-- tensor([[187.4835,  36.3444, 456.3344, 266.4649],\n",
      "        [467.8926,  45.4440, 547.8536, 205.1181],\n",
      "        [567.6376,  75.2855, 611.5358, 209.0952]])\n",
      "pred_phrase-- ['cooking utensil(0.31)', 'cooking utensil(0.27)', 'cooking utensil(0.25)']\n",
      "=======================\n",
      "after sort [(36, 187, 0), (45, 467, 1), (75, 567, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[187.4835,  36.0000, 456.3344, 266.4649],\n",
      "        [467.8926,  45.0000, 547.8536, 205.1181],\n",
      "        [567.6376,  75.0000, 611.5358, 209.0952]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'cooking utensil': {'cooking utensil_1': {'number': 1, 'bbox': [187, 36, 456, 266], 'score': 0.31}, 'cooking utensil_2': {'number': 2, 'bbox': [467, 45, 547, 205], 'score': 0.27}, 'cooking utensil_3': {'number': 3, 'bbox': [567, 75, 611, 209], 'score': 0.25}}})\n",
      "img_name:  153_n329479\n",
      "boxes_filt-- tensor([[123.6365,  55.9539, 415.8448, 226.3587]])\n",
      "pred_phrase-- ['man(0.90)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'man': {'man_1': {'number': 1, 'bbox': [123, 55, 415, 226], 'score': 0.9}}})\n",
      "img_name:  154_n433532\n",
      "boxes_filt-- tensor([[370.2620,  90.2455, 609.7057, 179.3551],\n",
      "        [356.5793,  91.1060, 640.0000, 426.8457]])\n",
      "pred_phrase-- ['hair(0.39)', 'hair(0.35)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'hair': {'hair_1': {'number': 1, 'bbox': [370, 90, 609, 179], 'score': 0.39}}})\n",
      "img_name:  155_n58220\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000,  78.1040, 427.0000]])\n",
      "pred_phrase-- ['flag(0.46)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'flag': {'flag_1': {'number': 1, 'bbox': [0, 0, 78, 427], 'score': 0.46}}})\n",
      "img_name:  156_n313060\n",
      "boxes_filt-- tensor([[  0.,   0., 640., 427.]])\n",
      "pred_phrase-- ['building(0.53)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  0.0000, 256.5634, 208.5383, 425.0595],\n",
      "        [  0.0000,   0.0000, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['fence(0.30)', 'fence(0.29)']\n",
      "=======================\n",
      "after sort [(0, 0, 0), (256, 0, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[  0.,   0., 640., 427.]]), tensor([[  0.0000, 256.0000, 208.5383, 425.0595]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'building': {'building_1': {'number': 1, 'bbox': [0, 0, 640, 427], 'score': 0.53}}, 'fence': {'fence_1': {'number': 2, 'bbox': [0, 256, 208, 425], 'score': 0.3}}})\n",
      "img_name:  157_n259002\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  frisbees\n",
      "boxes_filt-- tensor([[  0.0000, 196.7265, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['grass(0.58)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'grass': {'grass_1': {'number': 1, 'bbox': [0, 196, 640, 427], 'score': 0.58}}})\n",
      "img_name:  158_n16378\n",
      "boxes_filt-- tensor([[207.3929,  42.2781, 469.2309, 426.2268]])\n",
      "pred_phrase-- ['children(0.72)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'children': {'children_1': {'number': 1, 'bbox': [207, 42, 469, 426], 'score': 0.72}}})\n",
      "img_name:  159_n317260\n",
      "boxes_filt-- tensor([[  0.,   0., 640., 427.]])\n",
      "pred_phrase-- ['fence(0.81)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[452.3740, 144.8201, 621.3828, 242.9714]])\n",
      "pred_phrase-- ['bike(0.36)']\n",
      "=======================\n",
      "after sort [(0, 0, 0), (144, 452, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[  0.,   0., 640., 427.]]), tensor([[452.3740, 144.0000, 621.3828, 242.9714]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'fence': {'fence_1': {'number': 1, 'bbox': [0, 0, 640, 427], 'score': 0.81}}, 'bike': {'bike_1': {'number': 2, 'bbox': [452, 144, 621, 242], 'score': 0.36}}})\n",
      "img_name:  160_n324644\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000, 640.0000, 169.4062]])\n",
      "pred_phrase-- ['cables(0.33)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'cables': {'cables_1': {'number': 1, 'bbox': [0, 0, 640, 169], 'score': 0.33}}})\n",
      "img_name:  161_n413761\n",
      "obj_dict {}\n",
      "img_name:  162_n160664\n",
      "obj_dict {}\n",
      "img_name:  163_n331357\n",
      "boxes_filt-- tensor([[4.2938e+02, 2.5898e+02, 5.7470e+02, 4.0703e+02],\n",
      "        [8.2667e+01, 2.7089e+02, 1.7076e+02, 3.3955e+02],\n",
      "        [3.9131e-02, 2.1548e+02, 6.7547e+01, 2.6801e+02],\n",
      "        [5.3184e+01, 2.2179e+02, 9.0492e+01, 2.6490e+02],\n",
      "        [1.5124e+02, 2.1314e+02, 2.1492e+02, 2.8061e+02],\n",
      "        [3.0283e+02, 2.1220e+02, 3.6673e+02, 2.6685e+02],\n",
      "        [4.2380e+01, 1.9558e+02, 7.4237e+01, 2.3100e+02],\n",
      "        [4.9311e+02, 1.7030e+02, 5.2544e+02, 2.0087e+02],\n",
      "        [7.8885e+01, 2.6164e+02, 1.4506e+02, 2.9680e+02],\n",
      "        [3.5884e+02, 1.9187e+02, 3.8872e+02, 2.2376e+02],\n",
      "        [1.7482e+02, 2.3436e+02, 2.1775e+02, 2.7838e+02],\n",
      "        [1.3731e+02, 2.2459e+02, 1.6711e+02, 2.7164e+02],\n",
      "        [2.7261e+02, 1.8441e+02, 2.9136e+02, 2.1323e+02],\n",
      "        [3.1690e+02, 1.9467e+02, 3.4505e+02, 2.2123e+02],\n",
      "        [2.8449e+02, 1.9106e+02, 3.0819e+02, 2.2408e+02],\n",
      "        [1.6267e+02, 2.0219e+02, 1.9216e+02, 2.2674e+02],\n",
      "        [4.0841e+02, 1.7751e+02, 4.3943e+02, 2.1568e+02],\n",
      "        [3.4025e+02, 1.9767e+02, 3.6694e+02, 2.2772e+02],\n",
      "        [4.5218e+02, 1.8882e+02, 4.8746e+02, 2.1335e+02],\n",
      "        [3.7139e+02, 1.7959e+02, 4.0703e+02, 2.1482e+02],\n",
      "        [3.9399e+02, 1.7676e+02, 4.1925e+02, 2.1242e+02]])\n",
      "pred_phrase-- ['animal(0.53)', 'animal(0.50)', 'animal(0.47)', 'animal(0.47)', 'animal(0.45)', 'animal(0.43)', 'animal(0.42)', 'animal(0.41)', 'animal(0.41)', 'animal(0.39)', 'animal(0.39)', 'animal(0.38)', 'animal(0.37)', 'animal(0.37)', 'animal(0.36)', 'animal(0.36)', 'animal(0.35)', 'animal(0.35)', 'animal(0.33)', 'animal(0.32)', 'animal(0.25)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[429.9629, 259.5572, 574.6603, 407.1548]])\n",
      "pred_phrase-- ['mother(0.43)']\n",
      "=======================\n",
      "after sort [(211, 429, 0), (270, 82, 1), (125, 0, 2), (174, 53, 3), (212, 302, 4), (126, 42, 5), (115, 493, 6), (222, 78, 7), (53, 358, 8), (234, 174, 9), (154, 137, 10), (164, 272, 11), (88, 316, 12), (68, 284, 13), (186, 162, 14), (47, 408, 15), (136, 340, 16), (163, 452, 17), (50, 371, 18), (47, 393, 19), (259, 429, 20)]\n",
      "Adjusted collect_boxes_filt [tensor([[4.2938e+02, 2.1100e+02, 5.7470e+02, 4.0703e+02],\n",
      "        [8.2667e+01, 2.7000e+02, 1.7076e+02, 3.3955e+02],\n",
      "        [3.9131e-02, 1.2500e+02, 6.7547e+01, 2.6801e+02],\n",
      "        [5.3184e+01, 1.7400e+02, 9.0492e+01, 2.6490e+02],\n",
      "        [3.0283e+02, 2.1200e+02, 3.6673e+02, 2.6685e+02],\n",
      "        [4.2380e+01, 1.2600e+02, 7.4237e+01, 2.3100e+02],\n",
      "        [4.9311e+02, 1.1500e+02, 5.2544e+02, 2.0087e+02],\n",
      "        [7.8885e+01, 2.2200e+02, 1.4506e+02, 2.9680e+02],\n",
      "        [3.5884e+02, 5.3000e+01, 3.8872e+02, 2.2376e+02],\n",
      "        [1.7482e+02, 2.3400e+02, 2.1775e+02, 2.7838e+02],\n",
      "        [1.3731e+02, 1.5400e+02, 1.6711e+02, 2.7164e+02],\n",
      "        [2.7261e+02, 1.6400e+02, 2.9136e+02, 2.1323e+02],\n",
      "        [3.1690e+02, 8.8000e+01, 3.4505e+02, 2.2123e+02],\n",
      "        [2.8449e+02, 6.8000e+01, 3.0819e+02, 2.2408e+02],\n",
      "        [1.6267e+02, 1.8600e+02, 1.9216e+02, 2.2674e+02],\n",
      "        [4.0841e+02, 4.7000e+01, 4.3943e+02, 2.1568e+02],\n",
      "        [3.4025e+02, 1.3600e+02, 3.6694e+02, 2.2772e+02],\n",
      "        [4.5218e+02, 1.6300e+02, 4.8746e+02, 2.1335e+02],\n",
      "        [3.7139e+02, 5.0000e+01, 4.0703e+02, 2.1482e+02],\n",
      "        [3.9399e+02, 4.7000e+01, 4.1925e+02, 2.1242e+02]]), tensor([[429.9629, 259.0000, 574.6603, 407.1548]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'animal': {'animal_1': {'number': 1, 'bbox': [429, 211, 574, 407], 'score': 0.53}, 'animal_2': {'number': 2, 'bbox': [82, 270, 170, 339], 'score': 0.5}, 'animal_3': {'number': 3, 'bbox': [0, 125, 67, 268], 'score': 0.47}, 'animal_4': {'number': 4, 'bbox': [53, 174, 90, 264], 'score': 0.47}, 'animal_5': {'number': 5, 'bbox': [302, 212, 366, 266], 'score': 0.43}}, 'mother': {'mother_1': {'number': 6, 'bbox': [429, 259, 574, 407], 'score': 0.43}}})\n",
      "img_name:  164_n488874\n",
      "boxes_filt-- tensor([[387.0212, 198.0681, 640.0000, 270.3704],\n",
      "        [  0.0000, 166.0827, 640.0000, 270.7106]])\n",
      "pred_phrase-- ['colorful watercraft(0.43)', 'colorful watercraft(0.25)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'colorful watercraft': {'colorful watercraft_1': {'number': 1, 'bbox': [387, 198, 640, 270], 'score': 0.43}}})\n",
      "img_name:  165_n157375\n",
      "boxes_filt-- tensor([[ 73.0042, 195.8869, 435.2386, 293.8025]])\n",
      "pred_phrase-- ['airplanes(0.92)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 73.3570, 196.3032, 433.9364, 293.5247]])\n",
      "pred_phrase-- ['trucks(0.65)']\n",
      "=======================\n",
      "after sort [(148, 73, 0), (196, 73, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 73.0042, 148.0000, 435.2386, 293.8025]]), tensor([[ 73.3570, 196.0000, 433.9364, 293.5247]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'airplanes': {'airplanes_1': {'number': 1, 'bbox': [73, 148, 435, 293], 'score': 0.92}}, 'trucks': {'trucks_1': {'number': 2, 'bbox': [73, 196, 433, 293], 'score': 0.65}}})\n",
      "img_name:  166_n548534\n",
      "boxes_filt-- tensor([[ 30.9966, 248.4036, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['floor(0.39)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'floor': {'floor_1': {'number': 1, 'bbox': [30, 248, 640, 427], 'score': 0.39}}})\n",
      "img_name:  167_n489699\n",
      "boxes_filt-- tensor([[312.1147, 177.3562, 397.9766, 355.6372],\n",
      "        [395.1929, 191.5643, 484.5418, 357.1594]])\n",
      "pred_phrase-- ['child(0.48)', 'child(0.29)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[425.9677, 192.2066, 467.2874, 228.6767],\n",
      "        [200.7077,  64.7327, 252.6798, 105.3672],\n",
      "        [339.4218, 177.4911, 380.5782, 225.5139],\n",
      "        [335.1060,  70.6510, 380.0423, 112.6913],\n",
      "        [271.2646, 139.5460, 316.8455, 185.2891]])\n",
      "pred_phrase-- ['helmet(0.50)', 'helmet(0.43)', 'helmet(0.40)', 'helmet(0.37)', 'helmet(0.36)']\n",
      "=======================\n",
      "after sort [(48, 312, 0), (144, 395, 1), (192, 425, 2), (64, 200, 3), (96, 339, 4), (33, 335, 5), (47, 271, 6)]\n",
      "Adjusted collect_boxes_filt [tensor([[312.1147,  48.0000, 397.9766, 355.6372],\n",
      "        [395.1929, 144.0000, 484.5418, 357.1594]]), tensor([[425.9677, 192.0000, 467.2874, 228.6767],\n",
      "        [200.7077,  64.0000, 252.6798, 105.3672],\n",
      "        [339.4218,  96.0000, 380.5782, 225.5139],\n",
      "        [335.1060,  33.0000, 380.0423, 112.6913],\n",
      "        [271.2646,  47.0000, 316.8455, 185.2891]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'child': {'child_1': {'number': 1, 'bbox': [312, 48, 397, 355], 'score': 0.48}, 'child_2': {'number': 2, 'bbox': [395, 144, 484, 357], 'score': 0.29}}, 'helmet': {'helmet_1': {'number': 3, 'bbox': [425, 192, 467, 228], 'score': 0.5}, 'helmet_2': {'number': 4, 'bbox': [200, 64, 252, 105], 'score': 0.43}, 'helmet_3': {'number': 5, 'bbox': [339, 96, 380, 225], 'score': 0.4}, 'helmet_4': {'number': 6, 'bbox': [335, 33, 380, 112], 'score': 0.37}, 'helmet_5': {'number': 7, 'bbox': [271, 47, 316, 185], 'score': 0.36}}})\n",
      "img_name:  168_n445353\n",
      "boxes_filt-- tensor([[132.9657, 296.0159, 531.7748, 427.0000]])\n",
      "pred_phrase-- ['table(0.62)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'table': {'table_1': {'number': 1, 'bbox': [132, 296, 531, 427], 'score': 0.62}}})\n",
      "img_name:  169_n522733\n",
      "boxes_filt-- tensor([[1.9987e+02, 0.0000e+00, 6.4000e+02, 3.3524e+02],\n",
      "        [7.4524e-02, 2.2823e+02, 2.0734e+02, 3.1372e+02]])\n",
      "pred_phrase-- ['building(0.67)', 'building(0.42)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[256.6262,  88.1173, 306.1996, 348.9707]])\n",
      "pred_phrase-- ['pole(0.51)']\n",
      "=======================\n",
      "after sort [(0, 199, 0), (228, 0, 1), (88, 256, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[1.9987e+02, 0.0000e+00, 6.4000e+02, 3.3524e+02],\n",
      "        [7.4524e-02, 2.2800e+02, 2.0734e+02, 3.1372e+02]]), tensor([[256.6262,  88.0000, 306.1996, 348.9707]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'building': {'building_1': {'number': 1, 'bbox': [199, 0, 640, 335], 'score': 0.67}, 'building_2': {'number': 2, 'bbox': [0, 228, 207, 313], 'score': 0.42}}, 'pole': {'pole_1': {'number': 3, 'bbox': [256, 88, 306, 348], 'score': 0.51}}})\n",
      "img_name:  170_n51002\n",
      "boxes_filt-- tensor([[314.9716, 198.5813, 378.5635, 304.0076],\n",
      "        [324.0737, 214.4901, 364.3238, 282.8743]])\n",
      "pred_phrase-- ['fireplace(0.39)', 'fireplace(0.28)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[559.1608, 247.5224, 611.7006, 390.1450],\n",
      "        [329.9651, 152.5983, 381.7184, 213.1831]])\n",
      "pred_phrase-- ['tv(0.34)', 'tv(0.27)']\n",
      "=======================\n",
      "after sort [(214, 324, 0), (247, 559, 1), (152, 329, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[324.0737, 214.0000, 364.3238, 282.8743]]), tensor([[559.1608, 247.0000, 611.7006, 390.1450],\n",
      "        [329.9651, 152.0000, 381.7184, 213.1831]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'fireplace': {'fireplace_1': {'number': 1, 'bbox': [324, 214, 364, 282], 'score': 0.28}}, 'tv': {'tv_1': {'number': 2, 'bbox': [559, 247, 611, 390], 'score': 0.34}, 'tv_2': {'number': 3, 'bbox': [329, 152, 381, 213], 'score': 0.27}}})\n",
      "img_name:  171_n500209\n",
      "boxes_filt-- tensor([[160.7330,  33.1348, 270.6235, 128.1476],\n",
      "        [101.0405,   0.0000, 505.7340, 422.8697]])\n",
      "pred_phrase-- ['vegetable(0.35)', 'vegetable(0.35)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[196.7329, 298.5550, 443.7938, 388.3315],\n",
      "        [100.1466,   3.8195, 504.2823, 417.1633]])\n",
      "pred_phrase-- ['green bowls(0.44)', 'green bowls(0.27)']\n",
      "=======================\n",
      "after sort [(33, 160, 0), (298, 196, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[160.7330,  33.0000, 270.6235, 128.1476]]), tensor([[196.7329, 298.0000, 443.7938, 388.3315]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'vegetable': {'vegetable_1': {'number': 1, 'bbox': [160, 33, 270, 128], 'score': 0.35}}, 'green bowls': {'green bowls_1': {'number': 2, 'bbox': [196, 298, 443, 388], 'score': 0.44}}})\n",
      "img_name:  172_n534106\n",
      "boxes_filt-- tensor([[ 87.6884,   0.0000, 394.1708, 426.7322]])\n",
      "pred_phrase-- ['top(0.37)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  0.0000, 331.2939, 360.5989, 427.0000],\n",
      "        [  0.0000,  94.4168, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['floor(0.40)', 'floor(0.25)']\n",
      "=======================\n",
      "after sort [(0, 87, 0), (331, 0, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 87.6884,   0.0000, 394.1708, 426.7322]]), tensor([[  0.0000, 331.0000, 360.5989, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'top': {'top_1': {'number': 1, 'bbox': [87, 0, 394, 426], 'score': 0.37}}, 'floor': {'floor_1': {'number': 2, 'bbox': [0, 331, 360, 427], 'score': 0.4}}})\n",
      "img_name:  173_n546884\n",
      "boxes_filt-- tensor([[351.6456, 299.6377, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['receipt(0.50)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  0.0000,   0.0000, 328.4193, 426.6418],\n",
      "        [ 62.9076, 153.7583, 257.1465, 300.9429],\n",
      "        [368.9139,  32.2723, 640.0000, 306.2271]])\n",
      "pred_phrase-- ['ottoman(0.35)', 'ottoman(0.29)', 'ottoman(0.28)']\n",
      "=======================\n",
      "after sort [(299, 351, 0), (153, 62, 1), (32, 368, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[351.6456, 299.0000, 640.0000, 427.0000]]), tensor([[ 62.9076, 153.0000, 257.1465, 300.9429],\n",
      "        [368.9139,  32.0000, 640.0000, 306.2271]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'receipt': {'receipt_1': {'number': 1, 'bbox': [351, 299, 640, 427], 'score': 0.5}}, 'ottoman': {'ottoman_1': {'number': 2, 'bbox': [62, 153, 257, 300], 'score': 0.29}, 'ottoman_2': {'number': 3, 'bbox': [368, 32, 640, 306], 'score': 0.28}}})\n",
      "img_name:  174_n51658\n",
      "boxes_filt-- tensor([[187.4460, 153.0972, 514.7137, 401.4892],\n",
      "        [551.4777, 247.2584, 640.0000, 342.1560],\n",
      "        [401.9371, 156.9830, 491.5562, 224.7633],\n",
      "        [180.8286, 144.9342, 278.7136, 209.0565],\n",
      "        [289.6382,  35.3956, 383.4185,  96.8281],\n",
      "        [384.8551,  94.8782, 469.0815, 176.9033],\n",
      "        [ 47.9240,  59.7328, 132.2785, 151.0724],\n",
      "        [107.9715,   0.0000, 225.3611,  73.7058],\n",
      "        [ 95.1236, 126.6643, 181.0128, 187.0008],\n",
      "        [124.1363,  49.3922, 216.9600, 132.4697],\n",
      "        [368.3651, 183.9866, 432.5609, 226.2460],\n",
      "        [ 40.7838,   8.4133, 129.2541,  80.0690],\n",
      "        [163.9954, 117.9979, 239.8840, 185.1251],\n",
      "        [197.7260,  27.1835, 270.6111,  92.6768],\n",
      "        [  0.0000,   0.0000,  62.8499,  66.5199],\n",
      "        [181.8735,  58.1057, 262.6971, 128.1785],\n",
      "        [311.4869, 117.7700, 409.8160, 201.7708],\n",
      "        [329.1951,  62.1189, 413.3632, 111.1673],\n",
      "        [  0.0000,  49.7239,  58.2278, 164.7900],\n",
      "        [330.0428,  82.7105, 419.1388, 129.5157],\n",
      "        [ 72.5932,   0.0000, 143.8934,  30.5841],\n",
      "        [246.3915, 118.8933, 340.7601, 188.6618],\n",
      "        [ 33.9588,   0.0000,  84.8546,  28.4360],\n",
      "        [251.2219,   0.0000, 333.3638,  37.8231],\n",
      "        [  0.0000,   0.0000, 485.6848, 224.0347]])\n",
      "pred_phrase-- ['person(0.69)', 'person(0.59)', 'person(0.47)', 'person(0.47)', 'person(0.46)', 'person(0.44)', 'person(0.42)', 'person(0.42)', 'person(0.40)', 'person(0.39)', 'person(0.39)', 'person(0.38)', 'person(0.37)', 'person(0.37)', 'person(0.37)', 'person(0.36)', 'person(0.36)', 'person(0.34)', 'person(0.34)', 'person(0.32)', 'person(0.30)', 'person(0.29)', 'person(0.28)', 'person(0.27)', 'person(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[187.7831, 153.0873, 516.0460, 402.1060],\n",
      "        [476.7583,  17.9269, 500.5410,  33.7183]])\n",
      "pred_phrase-- ['tennis ball(0.38)', 'tennis ball(0.35)']\n",
      "=======================\n",
      "after sort [(247, 551, 0), (135, 401, 1), (30, 180, 2), (0, 289, 3), (0, 384, 4), (59, 47, 5), (0, 107, 6), (126, 95, 7), (10, 124, 8), (183, 368, 9), (0, 40, 10), (78, 163, 11), (0, 197, 12), (0, 0, 13), (21, 181, 14), (9, 311, 15), (0, 329, 16), (11, 0, 17), (0, 330, 18), (0, 72, 19), (10, 246, 20), (0, 33, 21), (0, 251, 22), (153, 187, 23), (17, 476, 24)]\n",
      "Adjusted collect_boxes_filt [tensor([[551.4777, 247.0000, 640.0000, 342.1560],\n",
      "        [401.9371, 135.0000, 491.5562, 224.7633],\n",
      "        [180.8286,  30.0000, 278.7136, 209.0565],\n",
      "        [289.6382,   0.0000, 383.4185,  96.8281],\n",
      "        [384.8551,   0.0000, 469.0815, 176.9033],\n",
      "        [ 47.9240,  59.0000, 132.2785, 151.0724],\n",
      "        [107.9715,   0.0000, 225.3611,  73.7058],\n",
      "        [ 95.1236, 126.0000, 181.0128, 187.0008],\n",
      "        [124.1363,  10.0000, 216.9600, 132.4697],\n",
      "        [368.3651, 183.0000, 432.5609, 226.2460],\n",
      "        [ 40.7838,   0.0000, 129.2541,  80.0690],\n",
      "        [163.9954,  78.0000, 239.8840, 185.1251],\n",
      "        [197.7260,   0.0000, 270.6111,  92.6768],\n",
      "        [  0.0000,   0.0000,  62.8499,  66.5199],\n",
      "        [181.8735,  21.0000, 262.6971, 128.1785],\n",
      "        [311.4869,   9.0000, 409.8160, 201.7708],\n",
      "        [329.1951,   0.0000, 413.3632, 111.1673],\n",
      "        [  0.0000,  11.0000,  58.2278, 164.7900],\n",
      "        [330.0428,   0.0000, 419.1388, 129.5157],\n",
      "        [ 72.5932,   0.0000, 143.8934,  30.5841],\n",
      "        [246.3915,  10.0000, 340.7601, 188.6618],\n",
      "        [ 33.9588,   0.0000,  84.8546,  28.4360],\n",
      "        [251.2219,   0.0000, 333.3638,  37.8231]]), tensor([[187.7831, 153.0000, 516.0460, 402.1060],\n",
      "        [476.7583,  17.0000, 500.5410,  33.7183]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'person': {'person_1': {'number': 1, 'bbox': [551, 247, 640, 342], 'score': 0.59}, 'person_2': {'number': 2, 'bbox': [401, 135, 491, 224], 'score': 0.47}, 'person_3': {'number': 3, 'bbox': [180, 30, 278, 209], 'score': 0.47}, 'person_4': {'number': 4, 'bbox': [289, 0, 383, 96], 'score': 0.46}, 'person_5': {'number': 5, 'bbox': [384, 0, 469, 176], 'score': 0.44}}, 'tennis ball': {'tennis ball_1': {'number': 6, 'bbox': [187, 153, 516, 402], 'score': 0.38}, 'tennis ball_2': {'number': 7, 'bbox': [476, 17, 500, 33], 'score': 0.35}}})\n",
      "img_name:  175_n258500\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  rackets\n",
      "obj_dict defaultdict(<class 'dict'>, {})\n",
      "img_name:  176_n55058\n",
      "boxes_filt-- tensor([[145.4496,   0.0000, 299.5474,  71.1816]])\n",
      "pred_phrase-- ['cups(0.52)']\n",
      "=======================\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  glasses\n",
      "obj_dict defaultdict(<class 'dict'>, {'cups': {'cups_1': {'number': 1, 'bbox': [145, 0, 299, 71], 'score': 0.52}}})\n",
      "img_name:  177_n238266\n",
      "boxes_filt-- tensor([[348.4012,  12.5082, 556.8041, 305.3509]])\n",
      "pred_phrase-- ['cake stand(0.48)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[509.9721, 149.1162, 640.0000, 316.0150],\n",
      "        [351.9832,  11.7432, 555.0745, 303.2308]])\n",
      "pred_phrase-- ['metal(0.36)', 'metal(0.25)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[349.9911,  11.0302, 556.5463, 300.8238]])\n",
      "pred_phrase-- ['glass(0.29)']\n",
      "=======================\n",
      "after sort [(12, 348, 0), (149, 509, 1), (0, 351, 2), (0, 349, 3)]\n",
      "Adjusted collect_boxes_filt [tensor([[348.4012,  12.0000, 556.8041, 305.3509]]), tensor([[509.9721, 149.0000, 640.0000, 316.0150],\n",
      "        [351.9832,   0.0000, 555.0745, 303.2308]]), tensor([[349.9911,   0.0000, 556.5463, 300.8238]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'cake stand': {'cake stand_1': {'number': 1, 'bbox': [348, 12, 556, 305], 'score': 0.48}}, 'metal': {'metal_1': {'number': 2, 'bbox': [509, 149, 640, 316], 'score': 0.36}, 'metal_2': {'number': 3, 'bbox': [351, 0, 555, 303], 'score': 0.25}}, 'glass': {'glass_1': {'number': 4, 'bbox': [349, 0, 556, 300], 'score': 0.29}}})\n",
      "img_name:  178_n355567\n",
      "boxes_filt-- tensor([[361.7351,  85.0242, 429.7606, 133.8048]])\n",
      "pred_phrase-- ['stainless steel clocks(0.37)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[385.1947, 205.7983, 487.4668, 424.8924]])\n",
      "pred_phrase-- ['faucets(0.26)']\n",
      "=======================\n",
      "after sort [(85, 361, 0), (205, 385, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[361.7351,  85.0000, 429.7606, 133.8048]]), tensor([[385.1947, 205.0000, 487.4668, 424.8924]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'stainless steel clocks': {'stainless steel clocks_1': {'number': 1, 'bbox': [361, 85, 429, 133], 'score': 0.37}}, 'faucets': {'faucets_1': {'number': 2, 'bbox': [385, 205, 487, 424], 'score': 0.26}}})\n",
      "img_name:  179_n285391\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  pillow\n",
      "boxes_filt-- tensor([[ 69.8151, 286.5843, 640.0000, 423.5063]])\n",
      "pred_phrase-- ['other pillow(0.29)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'other pillow': {'other pillow_1': {'number': 1, 'bbox': [69, 286, 640, 423], 'score': 0.29}}})\n",
      "img_name:  180_n39114\n",
      "boxes_filt-- tensor([[173.5405,  66.0083, 473.8196, 417.5334]])\n",
      "pred_phrase-- ['he(0.89)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[173.4940,  66.4832, 472.9464, 415.6158]])\n",
      "pred_phrase-- ['jersey(0.49)']\n",
      "=======================\n",
      "after sort [(18, 173, 0), (66, 173, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[173.5405,  18.0000, 473.8196, 417.5334]]), tensor([[173.4940,  66.0000, 472.9464, 415.6158]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'he': {'he_1': {'number': 1, 'bbox': [173, 18, 473, 417], 'score': 0.89}}, 'jersey': {'jersey_1': {'number': 2, 'bbox': [173, 66, 472, 415], 'score': 0.49}}})\n",
      "img_name:  181_n172618\n",
      "boxes_filt-- tensor([[392.9271,  57.4732, 533.3147, 365.9091]])\n",
      "pred_phrase-- ['long hair(0.51)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'long hair': {'long hair_1': {'number': 1, 'bbox': [392, 57, 533, 365], 'score': 0.51}}})\n",
      "img_name:  182_n159802\n",
      "boxes_filt-- tensor([[ 53.6175,   0.0000, 504.0639, 427.0000],\n",
      "        [373.9528,   0.0000, 640.0000, 422.2175],\n",
      "        [  0.0000,   0.0000, 218.4523, 396.7131]])\n",
      "pred_phrase-- ['person(0.56)', 'person(0.56)', 'person(0.47)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[495.4466, 179.5211, 640.0000, 427.0000]])\n",
      "pred_phrase-- ['water bottle(0.36)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 53.7038,   0.0000, 505.8406, 426.1704]])\n",
      "pred_phrase-- ['young(0.92)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 53.4244,   0.0000, 505.6033, 426.6428]])\n",
      "pred_phrase-- ['asian(0.70)']\n",
      "=======================\n",
      "after sort [(0, 53, 0), (0, 373, 1), (0, 0, 2), (179, 495, 3), (0, 53, 4), (0, 53, 5)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 53.6175,   0.0000, 504.0639, 427.0000],\n",
      "        [373.9528,   0.0000, 640.0000, 422.2175],\n",
      "        [  0.0000,   0.0000, 218.4523, 396.7131]]), tensor([[495.4466, 179.0000, 640.0000, 427.0000]]), tensor([[ 53.7038,   0.0000, 505.8406, 426.1704]]), tensor([[ 53.4244,   0.0000, 505.6033, 426.6428]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'person': {'person_1': {'number': 1, 'bbox': [53, 0, 504, 427], 'score': 0.56}, 'person_2': {'number': 2, 'bbox': [373, 0, 640, 422], 'score': 0.56}, 'person_3': {'number': 3, 'bbox': [0, 0, 218, 396], 'score': 0.47}}, 'water bottle': {'water bottle_1': {'number': 4, 'bbox': [495, 179, 640, 427], 'score': 0.36}}, 'young': {'young_1': {'number': 5, 'bbox': [53, 0, 505, 426], 'score': 0.92}}, 'asian': {'asian_1': {'number': 6, 'bbox': [53, 0, 505, 426], 'score': 0.7}}})\n",
      "img_name:  183_n513100\n",
      "boxes_filt-- tensor([[412.3176, 116.2154, 576.1014, 329.6456],\n",
      "        [207.2533, 118.9703, 371.0464, 318.9368],\n",
      "        [292.9712, 142.5640, 551.8173, 402.4666]])\n",
      "pred_phrase-- ['furniture(0.47)', 'furniture(0.46)', 'furniture(0.46)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  0.0000, 109.5961, 640.0000, 282.2243]])\n",
      "pred_phrase-- ['fence(0.56)']\n",
      "=======================\n",
      "after sort [(116, 412, 0), (118, 207, 1), (142, 292, 2), (109, 0, 3)]\n",
      "Adjusted collect_boxes_filt [tensor([[412.3176, 116.0000, 576.1014, 329.6456],\n",
      "        [207.2533, 118.0000, 371.0464, 318.9368],\n",
      "        [292.9712, 142.0000, 551.8173, 402.4666]]), tensor([[  0.0000, 109.0000, 640.0000, 282.2243]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'furniture': {'furniture_1': {'number': 1, 'bbox': [412, 116, 576, 329], 'score': 0.47}, 'furniture_2': {'number': 2, 'bbox': [207, 118, 371, 318], 'score': 0.46}, 'furniture_3': {'number': 3, 'bbox': [292, 142, 551, 402], 'score': 0.46}}, 'fence': {'fence_1': {'number': 4, 'bbox': [0, 109, 640, 282], 'score': 0.56}}})\n",
      "img_name:  184_n51658\n",
      "boxes_filt-- tensor([[187.4460, 153.0972, 514.7137, 401.4892],\n",
      "        [551.4777, 247.2584, 640.0000, 342.1560],\n",
      "        [401.9371, 156.9830, 491.5562, 224.7633],\n",
      "        [180.8286, 144.9342, 278.7136, 209.0565],\n",
      "        [289.6382,  35.3956, 383.4185,  96.8281],\n",
      "        [384.8551,  94.8782, 469.0815, 176.9033],\n",
      "        [ 47.9240,  59.7328, 132.2785, 151.0724],\n",
      "        [107.9715,   0.0000, 225.3611,  73.7058],\n",
      "        [ 95.1236, 126.6643, 181.0128, 187.0008],\n",
      "        [124.1363,  49.3922, 216.9600, 132.4697],\n",
      "        [368.3651, 183.9866, 432.5609, 226.2460],\n",
      "        [ 40.7838,   8.4133, 129.2541,  80.0690],\n",
      "        [163.9954, 117.9979, 239.8840, 185.1251],\n",
      "        [197.7260,  27.1835, 270.6111,  92.6768],\n",
      "        [  0.0000,   0.0000,  62.8499,  66.5199],\n",
      "        [181.8735,  58.1057, 262.6971, 128.1785],\n",
      "        [311.4869, 117.7700, 409.8160, 201.7708],\n",
      "        [329.1951,  62.1189, 413.3632, 111.1673],\n",
      "        [  0.0000,  49.7239,  58.2278, 164.7900],\n",
      "        [330.0428,  82.7105, 419.1388, 129.5157],\n",
      "        [ 72.5932,   0.0000, 143.8934,  30.5841],\n",
      "        [246.3915, 118.8933, 340.7601, 188.6618],\n",
      "        [ 33.9588,   0.0000,  84.8546,  28.4360],\n",
      "        [251.2219,   0.0000, 333.3638,  37.8231],\n",
      "        [  0.0000,   0.0000, 485.6848, 224.0347]])\n",
      "pred_phrase-- ['person(0.69)', 'person(0.59)', 'person(0.47)', 'person(0.47)', 'person(0.46)', 'person(0.44)', 'person(0.42)', 'person(0.42)', 'person(0.40)', 'person(0.39)', 'person(0.39)', 'person(0.38)', 'person(0.37)', 'person(0.37)', 'person(0.37)', 'person(0.36)', 'person(0.36)', 'person(0.34)', 'person(0.34)', 'person(0.32)', 'person(0.30)', 'person(0.29)', 'person(0.28)', 'person(0.27)', 'person(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[499.5769, 231.0295, 546.1133, 272.4113],\n",
      "        [186.5307, 152.0869, 516.3757, 402.7733]])\n",
      "pred_phrase-- ['tennis racket(0.50)', 'tennis racket(0.37)']\n",
      "=======================\n",
      "after sort [(247, 551, 0), (135, 401, 1), (30, 180, 2), (0, 289, 3), (0, 384, 4), (59, 47, 5), (0, 107, 6), (126, 95, 7), (10, 124, 8), (183, 368, 9), (0, 40, 10), (78, 163, 11), (0, 197, 12), (0, 0, 13), (21, 181, 14), (8, 311, 15), (0, 329, 16), (11, 0, 17), (0, 330, 18), (0, 72, 19), (10, 246, 20), (0, 33, 21), (0, 251, 22), (199, 499, 23), (152, 186, 24)]\n",
      "Adjusted collect_boxes_filt [tensor([[551.4777, 247.0000, 640.0000, 342.1560],\n",
      "        [401.9371, 135.0000, 491.5562, 224.7633],\n",
      "        [180.8286,  30.0000, 278.7136, 209.0565],\n",
      "        [289.6382,   0.0000, 383.4185,  96.8281],\n",
      "        [384.8551,   0.0000, 469.0815, 176.9033],\n",
      "        [ 47.9240,  59.0000, 132.2785, 151.0724],\n",
      "        [107.9715,   0.0000, 225.3611,  73.7058],\n",
      "        [ 95.1236, 126.0000, 181.0128, 187.0008],\n",
      "        [124.1363,  10.0000, 216.9600, 132.4697],\n",
      "        [368.3651, 183.0000, 432.5609, 226.2460],\n",
      "        [ 40.7838,   0.0000, 129.2541,  80.0690],\n",
      "        [163.9954,  78.0000, 239.8840, 185.1251],\n",
      "        [197.7260,   0.0000, 270.6111,  92.6768],\n",
      "        [  0.0000,   0.0000,  62.8499,  66.5199],\n",
      "        [181.8735,  21.0000, 262.6971, 128.1785],\n",
      "        [311.4869,   8.0000, 409.8160, 201.7708],\n",
      "        [329.1951,   0.0000, 413.3632, 111.1673],\n",
      "        [  0.0000,  11.0000,  58.2278, 164.7900],\n",
      "        [330.0428,   0.0000, 419.1388, 129.5157],\n",
      "        [ 72.5932,   0.0000, 143.8934,  30.5841],\n",
      "        [246.3915,  10.0000, 340.7601, 188.6618],\n",
      "        [ 33.9588,   0.0000,  84.8546,  28.4360],\n",
      "        [251.2219,   0.0000, 333.3638,  37.8231]]), tensor([[499.5769, 199.0000, 546.1133, 272.4113],\n",
      "        [186.5307, 152.0000, 516.3757, 402.7733]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'person': {'person_1': {'number': 1, 'bbox': [551, 247, 640, 342], 'score': 0.59}, 'person_2': {'number': 2, 'bbox': [401, 135, 491, 224], 'score': 0.47}, 'person_3': {'number': 3, 'bbox': [180, 30, 278, 209], 'score': 0.47}, 'person_4': {'number': 4, 'bbox': [289, 0, 383, 96], 'score': 0.46}, 'person_5': {'number': 5, 'bbox': [384, 0, 469, 176], 'score': 0.44}}, 'tennis racket': {'tennis racket_1': {'number': 6, 'bbox': [499, 199, 546, 272], 'score': 0.5}, 'tennis racket_2': {'number': 7, 'bbox': [186, 152, 516, 402], 'score': 0.37}}})\n",
      "img_name:  185_n433692\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  pot\n",
      "obj_dict defaultdict(<class 'dict'>, {})\n",
      "img_name:  186_n28792\n",
      "boxes_filt-- tensor([[262.7032, 118.6439, 367.1615, 389.1144]])\n",
      "pred_phrase-- ['bald man(0.80)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'bald man': {'bald man_1': {'number': 1, 'bbox': [262, 118, 367, 389], 'score': 0.8}}})\n",
      "img_name:  187_n173807\n",
      "boxes_filt-- tensor([[175.9440,  89.0556, 474.1591, 294.9454],\n",
      "        [127.3833, 214.6221, 154.1566, 234.2340]])\n",
      "pred_phrase-- ['vehicle(0.89)', 'vehicle(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[177.0987,  90.3290, 473.7603, 294.2874]])\n",
      "pred_phrase-- ['metal(0.75)']\n",
      "=======================\n",
      "after sort [(42, 175, 0), (214, 127, 1), (90, 177, 2)]\n",
      "Adjusted collect_boxes_filt [tensor([[175.9440,  42.0000, 474.1591, 294.9454],\n",
      "        [127.3833, 214.0000, 154.1566, 234.2340]]), tensor([[177.0987,  90.0000, 473.7603, 294.2874]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'vehicle': {'vehicle_1': {'number': 1, 'bbox': [175, 42, 474, 294], 'score': 0.89}, 'vehicle_2': {'number': 2, 'bbox': [127, 214, 154, 234], 'score': 0.26}}, 'metal': {'metal_1': {'number': 3, 'bbox': [177, 90, 473, 294], 'score': 0.75}}})\n",
      "img_name:  188_n508641\n",
      "boxes_filt-- tensor([[294.7340, 116.1393, 431.3054, 370.7770]])\n",
      "pred_phrase-- ['pants(0.51)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'pants': {'pants_1': {'number': 1, 'bbox': [294, 116, 431, 370], 'score': 0.51}}})\n",
      "img_name:  189_n98544\n",
      "boxes_filt-- tensor([[  0.,   0., 640., 427.]])\n",
      "pred_phrase-- ['bathroom(0.67)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'bathroom': {'bathroom_1': {'number': 1, 'bbox': [0, 0, 640, 427], 'score': 0.67}}})\n",
      "img_name:  190_n151768\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  boxes\n",
      "boxes_filt-- tensor([[135.9474, 255.0453, 434.9849, 427.0000],\n",
      "        [136.5274, 372.4900, 433.9387, 427.0000]])\n",
      "pred_phrase-- ['steps(0.26)', 'steps(0.25)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'steps': {'steps_1': {'number': 1, 'bbox': [136, 372, 433, 427], 'score': 0.25}}})\n",
      "img_name:  191_n565418\n",
      "boxes_filt-- tensor([[131.7640, 130.6428, 640.0000, 426.5561]])\n",
      "pred_phrase-- ['parking lot(0.40)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'parking lot': {'parking lot_1': {'number': 1, 'bbox': [131, 130, 640, 426], 'score': 0.4}}})\n",
      "img_name:  192_n367944\n",
      "boxes_filt-- tensor([[ 85.1579, 191.3738, 175.5153, 263.0319]])\n",
      "pred_phrase-- ['calculator(0.32)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[395.1562,   0.0000, 640.0000, 293.9536]])\n",
      "pred_phrase-- ['laptop(0.75)']\n",
      "=======================\n",
      "after sort [(191, 85, 0), (0, 395, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 85.1579, 191.0000, 175.5153, 263.0319]]), tensor([[395.1562,   0.0000, 640.0000, 293.9536]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'calculator': {'calculator_1': {'number': 1, 'bbox': [85, 191, 175, 263], 'score': 0.32}}, 'laptop': {'laptop_1': {'number': 2, 'bbox': [395, 0, 640, 293], 'score': 0.75}}})\n",
      "img_name:  193_n479092\n",
      "boxes_filt-- tensor([[  0.0000,  29.7724, 625.9277, 393.9863]])\n",
      "pred_phrase-- ['napkin(0.28)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'napkin': {'napkin_1': {'number': 1, 'bbox': [0, 29, 625, 393], 'score': 0.28}}})\n",
      "img_name:  194_n16936\n",
      "boxes_filt-- tensor([[ 97.5606,   0.0000, 171.8701, 247.4331]])\n",
      "pred_phrase-- ['skateboarder(0.69)']\n",
      "=======================\n",
      "obj_dict defaultdict(<class 'dict'>, {'skateboarder': {'skateboarder_1': {'number': 1, 'bbox': [97, 0, 171, 247], 'score': 0.69}}})\n",
      "img_name:  195_n12404\n",
      "boxes_filt-- tensor([[208.6653,  65.9401, 318.1740, 364.3097]])\n",
      "pred_phrase-- ['person(0.86)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[329.1655, 135.8799, 355.2142, 198.1747],\n",
      "        [ 65.5296, 154.6288, 115.6726, 247.9921],\n",
      "        [460.8991, 123.2930, 488.1555, 172.6674],\n",
      "        [543.2146, 106.9473, 569.0704, 157.3016],\n",
      "        [593.6914, 111.6812, 617.8965, 148.0319]])\n",
      "pred_phrase-- ['traffic cone(0.64)', 'traffic cone(0.64)', 'traffic cone(0.64)', 'traffic cone(0.63)', 'traffic cone(0.55)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[270.0775, 256.2208, 515.0627, 356.7466]])\n",
      "pred_phrase-- ['dirt(0.34)']\n",
      "=======================\n",
      "after sort [(65, 208, 0), (135, 329, 1), (154, 65, 2), (123, 460, 3), (63, 543, 4), (111, 593, 5), (256, 270, 6)]\n",
      "Adjusted collect_boxes_filt [tensor([[208.6653,  65.0000, 318.1740, 364.3097]]), tensor([[329.1655, 135.0000, 355.2142, 198.1747],\n",
      "        [ 65.5296, 154.0000, 115.6726, 247.9921],\n",
      "        [460.8991, 123.0000, 488.1555, 172.6674],\n",
      "        [543.2146,  63.0000, 569.0704, 157.3016],\n",
      "        [593.6914, 111.0000, 617.8965, 148.0319]]), tensor([[270.0775, 256.0000, 515.0627, 356.7466]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'person': {'person_1': {'number': 1, 'bbox': [208, 65, 318, 364], 'score': 0.86}}, 'traffic cone': {'traffic cone_1': {'number': 2, 'bbox': [329, 135, 355, 198], 'score': 0.64}, 'traffic cone_2': {'number': 3, 'bbox': [65, 154, 115, 247], 'score': 0.64}, 'traffic cone_3': {'number': 4, 'bbox': [460, 123, 488, 172], 'score': 0.64}, 'traffic cone_4': {'number': 5, 'bbox': [543, 63, 569, 157], 'score': 0.63}, 'traffic cone_5': {'number': 6, 'bbox': [593, 111, 617, 148], 'score': 0.55}}, 'dirt': {'dirt_1': {'number': 7, 'bbox': [270, 256, 515, 356], 'score': 0.34}}})\n",
      "img_name:  196_n531359\n",
      "boxes_filt-- tensor([[  0.0000, 287.9405, 127.6622, 427.0000],\n",
      "        [  0.0000, 181.9835, 123.7079, 277.0997]])\n",
      "pred_phrase-- ['tables(0.45)', 'tables(0.34)']\n",
      "=======================\n",
      "after sort [(287, 0, 0), (181, 0, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[  0.0000, 287.0000, 127.6622, 427.0000],\n",
      "        [  0.0000, 181.0000, 123.7079, 277.0997]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'tables': {'tables_1': {'number': 1, 'bbox': [0, 287, 127, 427], 'score': 0.45}, 'tables_2': {'number': 2, 'bbox': [0, 181, 123, 277], 'score': 0.34}}})\n",
      "img_name:  197_n470131\n",
      "boxes_filt-- tensor([[ 86.8752, 175.3077, 536.1259, 368.8600]])\n",
      "pred_phrase-- ['sweet dessert(0.40)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[442.6218, 229.8844, 535.7057, 310.2768],\n",
      "        [429.4776, 308.4914, 529.5562, 364.3207],\n",
      "        [195.4451, 234.2771, 288.4285, 318.7173],\n",
      "        [408.3696, 174.9531, 492.5826, 247.2067],\n",
      "        [ 87.2853, 175.1249, 535.9044, 368.4792],\n",
      "        [103.4808, 249.5455, 167.6324, 331.5909],\n",
      "        [319.9370, 264.3535, 425.4973, 342.4319],\n",
      "        [296.9533, 180.5720, 389.2796, 285.5331],\n",
      "        [211.3508, 182.3017, 304.3365, 250.8775],\n",
      "        [331.7695, 333.6829, 418.2365, 366.2748],\n",
      "        [131.2664, 199.3266, 222.0834, 273.2861],\n",
      "        [180.5009, 309.0564, 296.3222, 366.0194],\n",
      "        [403.4654, 247.8310, 477.1591, 327.3085]])\n",
      "pred_phrase-- ['food(0.39)', 'food(0.39)', 'food(0.37)', 'food(0.37)', 'food(0.36)', 'food(0.35)', 'food(0.33)', 'food(0.32)', 'food(0.31)', 'food(0.31)', 'food(0.30)', 'food(0.28)', 'food(0.28)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[  0.0000,  56.1906, 623.0656, 427.0000]])\n",
      "pred_phrase-- ['tape(0.31)']\n",
      "=======================\n",
      "after sort [(138, 86, 0), (199, 442, 1), (308, 429, 2), (234, 195, 3), (151, 408, 4), (249, 103, 5), (264, 319, 6), (180, 296, 7), (182, 211, 8), (333, 331, 9), (186, 131, 10), (309, 180, 11), (247, 403, 12), (56, 0, 13)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 86.8752, 138.0000, 536.1259, 368.8600]]), tensor([[442.6218, 199.0000, 535.7057, 310.2768],\n",
      "        [429.4776, 308.0000, 529.5562, 364.3207],\n",
      "        [195.4451, 234.0000, 288.4285, 318.7173],\n",
      "        [408.3696, 151.0000, 492.5826, 247.2067],\n",
      "        [103.4808, 249.0000, 167.6324, 331.5909],\n",
      "        [319.9370, 264.0000, 425.4973, 342.4319],\n",
      "        [296.9533, 180.0000, 389.2796, 285.5331],\n",
      "        [211.3508, 182.0000, 304.3365, 250.8775],\n",
      "        [331.7695, 333.0000, 418.2365, 366.2748],\n",
      "        [131.2664, 186.0000, 222.0834, 273.2861],\n",
      "        [180.5009, 309.0000, 296.3222, 366.0194],\n",
      "        [403.4654, 247.0000, 477.1591, 327.3085]]), tensor([[  0.0000,  56.0000, 623.0656, 427.0000]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'sweet dessert': {'sweet dessert_1': {'number': 1, 'bbox': [86, 138, 536, 368], 'score': 0.4}}, 'food': {'food_1': {'number': 2, 'bbox': [442, 199, 535, 310], 'score': 0.39}, 'food_2': {'number': 3, 'bbox': [429, 308, 529, 364], 'score': 0.39}, 'food_3': {'number': 4, 'bbox': [195, 234, 288, 318], 'score': 0.37}, 'food_4': {'number': 5, 'bbox': [408, 151, 492, 247], 'score': 0.37}, 'food_5': {'number': 6, 'bbox': [103, 249, 167, 331], 'score': 0.35}}, 'tape': {'tape_1': {'number': 7, 'bbox': [0, 56, 623, 427], 'score': 0.31}}})\n",
      "img_name:  198_n259002\n",
      "boxes_filt-- tensor([[170.1451, 112.7608, 199.5011, 184.1099],\n",
      "        [  0.0000,  84.0801, 141.4673, 412.5709]])\n",
      "pred_phrase-- ['trash cans(0.32)', 'trash cans(0.28)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[328.7402,  73.8684, 466.6059, 357.0383],\n",
      "        [459.3792,  59.1505, 640.0000, 374.3670],\n",
      "        [  0.0000,  84.8080, 141.6563, 412.9009]])\n",
      "pred_phrase-- ['soccer player(0.41)', 'soccer player(0.34)', 'soccer player(0.26)']\n",
      "=======================\n",
      "boxes_filt-- tensor([], size=(0, 4))\n",
      "pred_phrase-- []\n",
      "Did not find:  ball\n",
      "after sort [(112, 170, 0), (36, 0, 1), (73, 328, 2), (59, 459, 3), (84, 0, 4)]\n",
      "Adjusted collect_boxes_filt [tensor([[170.1451, 112.0000, 199.5011, 184.1099],\n",
      "        [  0.0000,  36.0000, 141.4673, 412.5709]]), tensor([[328.7402,  73.0000, 466.6059, 357.0383],\n",
      "        [459.3792,  59.0000, 640.0000, 374.3670],\n",
      "        [  0.0000,  84.0000, 141.6563, 412.9009]]), tensor([], size=(0, 4))]\n",
      "obj_dict defaultdict(<class 'dict'>, {'trash cans': {'trash cans_1': {'number': 1, 'bbox': [170, 112, 199, 184], 'score': 0.32}, 'trash cans_2': {'number': 2, 'bbox': [0, 36, 141, 412], 'score': 0.28}}, 'soccer player': {'soccer player_1': {'number': 3, 'bbox': [328, 73, 466, 357], 'score': 0.41}, 'soccer player_2': {'number': 4, 'bbox': [459, 59, 640, 374], 'score': 0.34}, 'soccer player_3': {'number': 5, 'bbox': [0, 84, 141, 412], 'score': 0.26}}})\n",
      "img_name:  199_n511913\n",
      "boxes_filt-- tensor([[ 22.3105,  35.3504, 403.7485, 427.0000],\n",
      "        [ 28.9144, 239.6962, 110.5591, 310.2569]])\n",
      "pred_phrase-- ['device(0.27)', 'device(0.27)']\n",
      "=======================\n",
      "boxes_filt-- tensor([[ 18.9922,  34.8870, 403.7306, 426.7502]])\n",
      "pred_phrase-- ['woman(0.95)']\n",
      "=======================\n",
      "after sort [(239, 28, 0), (34, 18, 1)]\n",
      "Adjusted collect_boxes_filt [tensor([[ 28.9144, 239.0000, 110.5591, 310.2569]]), tensor([[ 18.9922,  34.0000, 403.7306, 426.7502]])]\n",
      "obj_dict defaultdict(<class 'dict'>, {'device': {'device_1': {'number': 1, 'bbox': [28, 239, 110, 310], 'score': 0.27}}, 'woman': {'woman_1': {'number': 2, 'bbox': [18, 34, 403, 426], 'score': 0.95}}})\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/root/projects/code_lihan/')\n",
    "sys.path.append('/root/projects/code_lihan/GroundingDINO')\n",
    "import importlib\n",
    "import objects_graph\n",
    "import groundingdino.util.inference \n",
    "importlib.reload(groundingdino.util.inference)\n",
    "importlib.reload(objects_graph)\n",
    "from objects_graph import ObjModel\n",
    "import os \n",
    "# config_file = '/root/projects/mmcot/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'  # change the path of the model config file\n",
    "\n",
    "# grounded_checkpoint = '/root/projects/mmcot/GroundingDINO/weights/groundingdino_swint_ogc.pth'  # change the path of the model\n",
    "config_file = '/root/projects/mmcot/GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py'  # change the path of the model config file\n",
    "# # ram_checkpoint = '/root/autodl-tmp/eva_weights/ram_plus_swin_large_14m.pth'  # change the path of the model\n",
    "grounded_checkpoint = '/root/projects/mmcot/GroundingDINO/weights/groundingdino_swinb_cogcoor.pth'  # change the path of the model\n",
    "model = ObjModel(grounded_checkpoint, config_file)\n",
    "obj_dicts=[]\n",
    "axis_dicts=[]\n",
    "obj_not_founds=[]\n",
    "obj_filt_dicts=[]\n",
    "# path_out='./test_morethan2'\n",
    "\n",
    "os.makedirs(f'./test_{num}-{num+200}', exist_ok=True)\n",
    "image_processed_Ids=[]\n",
    "for i in range(len(nodes)):\n",
    "# \n",
    "    # if i < 88: continue\n",
    "    objlist = nodes[i]\n",
    "\n",
    "    # if img_name in os.listdir(path_out):\n",
    "    #     img_id_out=imageIds[i]\n",
    "    # else:\n",
    "    img_id_out=str(i)+'_'+imageIds[i]\n",
    "    \n",
    "    print(\"img_name: \",img_id_out)\n",
    "    image_processed_Ids.append(img_id_out)\n",
    "    obj_dict,obj_not_found ,obj_filt_dict= model.find_obj( objlist, img_name, img_id_out, path,path_out,need_obj_boxes=False)\n",
    "    print(\"obj_dict\",obj_dict)\n",
    "\n",
    "    dict_ori=obj_dict\n",
    "    obj_dict={}\n",
    "    axis_dict={}\n",
    "    for key,val in dict_ori.items():\n",
    "        num_=[]\n",
    "        for key_obj,val_obj in val.items():\n",
    "            num_.append(val_obj['number'])\n",
    "            axis_dict[val_obj['number']]=val_obj['bbox']\n",
    "        obj_dict[key]=num_\n",
    "    obj_dicts.append(obj_dict)\n",
    "    axis_dicts.append(axis_dict)\n",
    "    obj_not_founds.append(obj_not_found)\n",
    "    obj_filt_dicts.append(obj_filt_dict)\n",
    "    # if i==20: break\n",
    "    # break\n",
    "import json\n",
    "with open(f'obj_dicts_{num}-{num+200}.json','w') as f:\n",
    "    json.dump(obj_dicts,f)\n",
    "with open(f'axis_dicts_{num}-{num+200}.json','w') as f:\n",
    "    json.dump(axis_dicts,f)\n",
    "with open(f'obj_not_founds_{num}-{num+200}.json','w') as f:\n",
    "    json.dump(obj_not_founds,f)\n",
    "with open(f'obj_filt_dicts_{num}-{num+200}.json','w') as f:\n",
    "    json.dump(obj_filt_dicts,f)\n",
    "with open(f'image_processed_Ids_{num}-{num+200}.txt','w') as f:\n",
    "    for imageId in image_processed_Ids:\n",
    "        f.write(str(imageId)+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start running~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "num=600\n",
    "import json\n",
    "with open(f'obj_dicts_{num}-{num+200}.json','r') as f:\n",
    "    obj_dicts=json.load(f)\n",
    "with open(f'axis_dicts_{num}-{num+200}.json','r') as f:\n",
    "    axis_dicts=json.load(f)\n",
    "with open(f'obj_not_founds_{num}-{num+200}.json','r') as f:\n",
    "    obj_not_founds=json.load(f)\n",
    "with open(f'{num}-{num+200}_Q.txt','r') as f:\n",
    "    questions=[]\n",
    "    for line in f:\n",
    "        questions.append(line[:-1])\n",
    "print(len(questions))\n",
    "with open(f'image_processed_Ids_{num}-{num+200}.txt','r') as f:\n",
    "    imageIds=[]\n",
    "    for line in f:\n",
    "        imageIds.append(line[:-1])\n",
    "print(len(imageIds))\n",
    "with open(f'{num}-{num+200}_A.txt','r') as f:\n",
    "    answers=[]\n",
    "    for line in f:\n",
    "        answers.append(line[:-1])\n",
    "print(len(answers))\n",
    "with open(f'obj_filt_dicts_{num}-{num+200}.json','r') as f:\n",
    "    obj_filt_dicts=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The 31 sample:\n",
      "30_n518912 Are the curtains white and short? no {'curtains': [1]}\n",
      "all_nodes [['curtains']]\n",
      "========== Node question Generation ===========\n",
      "False\n",
      "all_node ['curtains']\n",
      "node_question {\"curtains\":[\"Are the curtains white?\",\"Are the curtains short?\"]}\n",
      "node_question,posi_q {'curtains': ['Are the curtains white?', 'Are the curtains short?']} {'curtains': []}\n",
      "========== Dict Generation ===========\n",
      "['Are the curtains white and short?']\n",
      "extracted_lists [[]]\n",
      "=========generate edge============\n",
      "No edge_question for this question.\n",
      "all_edgequestions edge [['No edge_question']]\n",
      "all_relationship_dict:  [{}]\n",
      "combined_relation {}\n",
      "==========Generated graph==========\n",
      "[{'curtains': {'node_question': ['Are the curtains white?', 'Are the curtains short?']}}]\n",
      "Main_node: curtains\n",
      "  Main_node Question: Are the curtains white?\n",
      "\n",
      "obj_dict to update the question {'curtains': [1]}\n",
      "updated_question  Are the curtains [1] white?\n",
      "label_sentence In the image, there are one curtains marked by 1.\n",
      "{'agent': {'next': 'VQA_tool'}}\n",
      "-----------------\n",
      "Are the curtains [1] white in the image?\n",
      "Complete result from VQA tool:  Thought: I need to determine if the curtains labeled as [1] are white.\n",
      "Answer: Yes, the curtains [1] are white.\n",
      "---------------\n",
      "{'VQA_tool': {'Tool_return': [FunctionMessage(content='Yes, the curtains [1] are white.', name='VQA_tool')]}}\n",
      "-----------------\n",
      "ans Yes, the curtains [1] are white.\n",
      "obj_dict after update {'curtains': [1]}\n",
      "  Main_node Question: Are the curtains short?\n",
      "\n",
      "obj_dict to update the question {'curtains': [1]}\n",
      "updated_question  Are the curtains [1] short?\n",
      "label_sentence In the image, there are one curtains marked by 1.\n",
      "{'agent': {'next': 'VQA_tool'}}\n",
      "-----------------\n",
      "Are the curtains [1] short in the image?\n",
      "Complete result from VQA tool:  Thought: Based on the image, the curtains labeled as [1] appear to be long and hanging down, covering a significant portion of the wall. Therefore, the curtains [1] are not short.\n",
      "Answer: No, the curtains [1] are not short.\n",
      "---------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 83\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# if len(entity_list[0])>0:\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#     continue\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# print('obj_filt_dict',obj_filt_dict)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# print(response['choices'][0]['message']['content'])\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# print('answer',label)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m image_ori_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/root/projects/mmcot/gqa/images\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 83\u001b[0m logit\u001b[38;5;241m=\u001b[39m\u001b[43mtraverse_node_edge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQuestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43mImg_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mObj_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43mAxis_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43mentity_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mObj_not_found\u001b[49m\u001b[43m,\u001b[49m\u001b[43mObj_filt_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimage_ori_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# logit=graph_generation.main(Questions,entity_list)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m coll_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mcnt\n",
      "File \u001b[0;32m~/projects/code_lihan/traverse_node_edge.py:266\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(questions, img_dirs, obj_dicts, axis_dicts, entity_lists, obj_not_founds, obj_filt_dicts, image_ori_path)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdated_question \u001b[39m\u001b[38;5;124m\"\u001b[39m,question)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# print(\"obj_dict\",type((obj_dict)))\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# print('obj_filt_dict',obj_filt_dict)\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# print('obj_dict',obj_dict)\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m ans,tool_used\u001b[38;5;241m=\u001b[39m\u001b[43mlang_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnodeQ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlabel_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_filt_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mans\u001b[39m\u001b[38;5;124m'\u001b[39m,ans)\n\u001b[1;32m    268\u001b[0m if_tool_used\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/code_lihan/langgraph_process.py:49\u001b[0m, in \u001b[0;36mlang_graph\u001b[0;34m(Image, node, question, context, question_type, label_sentence, axis_dict, obj_dict, obj_filt_dict)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m,label_sentence)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# if context==\"None\":\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#      input={\"Entity\": [HumanMessage(content=node)],\"Subquestions\": [HumanMessage(content=question)]},\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# input={\"Entity\": [HumanMessage(content=node)],\"Subquestions\": [HumanMessage(content=question)],\"Context\": [HumanMessage(content=context)]},\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m     50\u001b[0m         \n\u001b[1;32m     51\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39mImage)],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntity\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39mnode)],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubquestions\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39mquestion)],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39mcontext)],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_type\u001b[39m\u001b[38;5;124m\"\u001b[39m:[HumanMessage(content\u001b[38;5;241m=\u001b[39mquestion_type)],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m:[HumanMessage(content\u001b[38;5;241m=\u001b[39mlabel_sentence)],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m:[HumanMessage(content\u001b[38;5;241m=\u001b[39maxis_dict)],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m:[HumanMessage(content\u001b[38;5;241m=\u001b[39mobj_dict)],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_filt_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m:[HumanMessage(content\u001b[38;5;241m=\u001b[39mobj_filt_dict)]},\n\u001b[1;32m     52\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m150\u001b[39m}\n\u001b[1;32m     53\u001b[0m         ):\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__end__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m s:\n\u001b[1;32m     55\u001b[0m                 \u001b[38;5;66;03m# print((llm))\u001b[39;00m\n\u001b[1;32m     56\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(s, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-----------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/ovd/lib/python3.10/site-packages/langgraph/pregel/__init__.py:955\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    953\u001b[0m     done, inflight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(), \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m futures:\n\u001b[0;32m--> 955\u001b[0m     done, inflight \u001b[38;5;241m=\u001b[39m \u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIRST_COMPLETED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# timed out\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/ovd/lib/python3.10/concurrent/futures/_base.py:307\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[1;32m    305\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
      "File \u001b[0;32m/opt/conda/envs/ovd/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/conda/envs/ovd/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/root/projects/code_lihan/')\n",
    "import importlib\n",
    "import json\n",
    "import tools\n",
    "import traverse_node_edge\n",
    "import graph_generation\n",
    "import langgraph_process\n",
    "# import gpt4_img\n",
    "import gpt4_img_qwen\n",
    "import re\n",
    "import string\n",
    "# importlib.reload(gpt4_img)\n",
    "importlib.reload(gpt4_img_qwen)\n",
    "importlib.reload(traverse_node_edge)\n",
    "importlib.reload(graph_generation)\n",
    "importlib.reload(tools)\n",
    "importlib.reload(langgraph_process)\n",
    "import os\n",
    "os.environ[\"DASHSCOPE_API_KEY\"]= 'sk-ac7aca0206ae4da9a517628e5fa2170f'\n",
    "\n",
    "\n",
    "# questions=['Are there napkins under the utensil to the left of the rice?']\n",
    "img_dirs=[]\n",
    "for imageId in imageIds:\n",
    "    img_dirs.append(f'/root/projects/code_lihan/experiment_questions_data/test_{num}-{num+200}/'+imageId+'.jpg')\n",
    "# obj_dicts=[{'napkins': [1], 'utensil': [2, 3], 'rice': [4]}]\n",
    "cnt=0\n",
    "# collection=[]\n",
    "for imageId,question,img_dir,label,obj_dict,axis_dict,obj_not_found,obj_filt_dict in zip(imageIds,questions,img_dirs,answers,obj_dicts,axis_dicts,obj_not_founds,obj_filt_dicts):\n",
    "    if cnt<30:\n",
    "        cnt+=1\n",
    "        continue \n",
    "    if cnt==120:\n",
    "        break\n",
    "    question=question[0]+question[1:].lower()\n",
    "    question_words=question[:-1].split(' ')\n",
    "    # if 'Who' in question_words :\n",
    "    #     # cnt+=1\n",
    "    #     continue\n",
    "    # if 'big' not in question_words :\n",
    "    #     # cnt+=1\n",
    "    #     continue\n",
    "    question = re.sub(r'walking\\b', 'standing', question)\n",
    "    question = re.sub(r'walk\\b', 'stand', question)\n",
    "    question = re.sub(r'Where\\b', 'What place', question)\n",
    "    question = re.sub(r'name\\b', 'type', question)\n",
    "    \n",
    "    cnt+=1\n",
    "    to_remove=[]\n",
    "    for k,v in obj_filt_dict.items():\n",
    "        if k not in obj_dict:\n",
    "            to_remove.append(k)\n",
    "    for k in to_remove:\n",
    "        del obj_filt_dict[k]\n",
    "    # if 'left' not in question and 'right' not in question:\n",
    "    #     continue\n",
    "    print(\"\\n\")\n",
    "    print(f\"The {cnt} sample:\")\n",
    "    print(imageId,question,label,obj_dict)\n",
    "    Questions=[question]\n",
    "    Img_dirs=[img_dir]\n",
    "    Obj_dicts=[obj_dict]\n",
    "    Axis_dicts=[axis_dict]\n",
    "    Obj_not_found=[obj_not_founds]\n",
    "    Obj_filt_dict=[obj_filt_dict]\n",
    "    coll_dict={}\n",
    "\n",
    "    entity_list=[]\n",
    "    for obj_dict in Obj_dicts:\n",
    "        entity_list.append(list(obj_dict.keys()))\n",
    "    # if len(entity_list[0])>0:\n",
    "    #     continue\n",
    "    # print('obj_filt_dict',obj_filt_dict)\n",
    "    # print('obj_dict',obj_dict)\n",
    "    # payload = create_payload_final_ans( img_name, question,'','')\n",
    "    # response = query_openai(payload)\n",
    "    # # print(question)\n",
    "    # logit=response['choices'][0]['message']['content']\n",
    "    # print(response['choices'][0]['message']['content'])\n",
    "    # print('answer',label)\n",
    "    image_ori_path='/root/projects/mmcot/gqa/images'\n",
    "    logit=traverse_node_edge.main(Questions,Img_dirs,Obj_dicts,Axis_dicts,entity_list,Obj_not_found,Obj_filt_dict,image_ori_path)\n",
    "\n",
    "    # logit=graph_generation.main(Questions,entity_list)\n",
    "    coll_dict['id']=cnt\n",
    "    coll_dict['Label']=label\n",
    "    coll_dict['Logit']=logit\n",
    "    coll_dict['Q']=question\n",
    "    coll_dict['img_name']=imageId\n",
    "    collection.append(coll_dict)\n",
    "    if cnt==1:\n",
    "        print(collection)\n",
    "    # if cnt%80==0:\n",
    "    #     with open(\"all_1000-1200_collection.json\",\"a\") as f:\n",
    "    #         json.dump(collection,f)\n",
    "    # if cnt==6:\n",
    "    #     break\n",
    "# with open('obj_filt_dicts_who.json','r') as f:\n",
    "#     obj_filt_dicts=json.load(f)\n",
    "# with open('obj_dicts_who.json','r') as f:\n",
    "#     obj_dicts=json.load(f)\n",
    "\n",
    "    # if cnt==18:\n",
    "        # break\n",
    "# with open('obj_dicts_100-200.json','r') as f:\n",
    "# with open('obj_dicts_0-100.json','r') as f:\n",
    "#     obj_dicts=json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TypeError: Conversation.call() missing 1 required positional argument: 'self'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/root/projects/code_lihan/')\n",
    "import importlib\n",
    "import json\n",
    "import tools\n",
    "import traverse_node_edge\n",
    "import graph_generation\n",
    "import langgraph_process\n",
    "import gpt4_img\n",
    "import re\n",
    "import string\n",
    "importlib.reload(gpt4_img)\n",
    "importlib.reload(traverse_node_edge)\n",
    "importlib.reload(graph_generation)\n",
    "importlib.reload(tools)\n",
    "importlib.reload(langgraph_process)\n",
    "\n",
    "\n",
    "\n",
    "# questions=['Are there napkins under the utensil to the left of the rice?']\n",
    "img_names=[]\n",
    "for imageId in imageIds:\n",
    "    img_names.append(f'/root/projects/code_lihan/experiment_questions_data/test_{num}-{num+200}/'+imageId+'.jpg')\n",
    "# obj_dicts=[{'napkins': [1], 'utensil': [2, 3], 'rice': [4]}]\n",
    "cnt=0\n",
    "collection=[]\n",
    "for imageId,question,img_name,label,obj_dict,axis_dict,obj_not_found,obj_filt_dict in zip(imageIds,questions,img_names,answers,obj_dicts,axis_dicts,obj_not_founds,obj_filt_dicts):\n",
    "    if cnt<43:\n",
    "        cnt+=1\n",
    "        continue \n",
    "    question=question[0]+question[1:].lower()\n",
    "    question_words=question[:-1].split(' ')\n",
    "    # if 'Who' in question_words :\n",
    "    #     # cnt+=1\n",
    "    #     continue\n",
    "    # if 'big' not in question_words :\n",
    "    #     # cnt+=1\n",
    "    #     continue\n",
    "    question = re.sub(r'walking\\b', 'standing', question)\n",
    "    question = re.sub(r'walk\\b', 'stand', question)\n",
    "    question = re.sub(r'Where\\b', 'What place', question)\n",
    "    question = re.sub(r'name\\b', 'type', question)\n",
    "    \n",
    "    cnt+=1\n",
    "    to_remove=[]\n",
    "    for k,v in obj_filt_dict.items():\n",
    "        if k not in obj_dict:\n",
    "            to_remove.append(k)\n",
    "    for k in to_remove:\n",
    "        del obj_filt_dict[k]\n",
    "    # if 'left' not in question and 'right' not in question:\n",
    "    #     continue\n",
    "    print(\"\\n\")\n",
    "    print(f\"The {cnt} sample:\")\n",
    "    print(imageId,question,label,obj_dict)\n",
    "    Questions=[question]\n",
    "    Img_names=[img_name]\n",
    "    Obj_dicts=[obj_dict]\n",
    "    Axis_dicts=[axis_dict]\n",
    "    Obj_not_found=[obj_not_founds]\n",
    "    Obj_filt_dict=[obj_filt_dict]\n",
    "    coll_dict={}\n",
    "\n",
    "    entity_list=[]\n",
    "    for obj_dict in Obj_dicts:\n",
    "        entity_list.append(list(obj_dict.keys()))\n",
    "    # print('obj_filt_dict',obj_filt_dict)\n",
    "    # print('obj_dict',obj_dict)\n",
    "    # payload = create_payload_final_ans( img_name, question,'','')\n",
    "    # response = query_openai(payload)\n",
    "    # # print(question)\n",
    "    # logit=response['choices'][0]['message']['content']\n",
    "    # print(response['choices'][0]['message']['content'])\n",
    "    # print('answer',label)\n",
    "    logit=traverse_node_edge.main(Questions,Img_names,Obj_dicts,Axis_dicts,entity_list,Obj_not_found,Obj_filt_dict)\n",
    "    # logit=graph_generation.main(Questions,entity_list)\n",
    "    coll_dict['id']=cnt\n",
    "    coll_dict['Label']=label\n",
    "    coll_dict['Logit']=logit\n",
    "    coll_dict['Q']=question\n",
    "    coll_dict['img_name']=img_name\n",
    "    collection.append(coll_dict)\n",
    "    if cnt==1:\n",
    "        print(collection)\n",
    "    # if cnt%80==0:\n",
    "    #     with open(\"all_1000-1200_collection.json\",\"a\") as f:\n",
    "    #         json.dump(collection,f)\n",
    "    break\n",
    "# with open('obj_filt_dicts_who.json','r') as f:\n",
    "#     obj_filt_dicts=json.load(f)\n",
    "# with open('obj_dicts_who.json','r') as f:\n",
    "#     obj_dicts=json.load(f)\n",
    "\n",
    "    # if cnt==18:\n",
    "        # break\n",
    "# with open('obj_dicts_100-200.json','r') as f:\n",
    "# with open('obj_dicts_0-100.json','r') as f:\n",
    "#     obj_dicts=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"qwen_test_600-720collection.json\",\"w\") as f:\n",
    "    json.dump(collection,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "with open(\"qwen_test_600-720collection.json\",\"rb\") as f:\n",
    "    collections=json.load(f)\n",
    "cnt=0\n",
    "nums=[]\n",
    "cnt1=0\n",
    "for c in collections:\n",
    "    # if 'what' in c['Q'] or 'What' in c['Q']: continue\n",
    "    # cnt1+=1\n",
    "    if c['Logit']==c['Label']:cnt+=1\n",
    "    else:\n",
    "        if bool(re.search(r'\\d', c['Logit'])): nums.append(c['id'])\n",
    "        # print('Logit: ',c['Logit'])\n",
    "        # print('Label: ',c['Label'])\n",
    "print(cnt)\n",
    "print(cnt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "189\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "with open(\"test_0-200_collection.json\",\"rb\") as f:\n",
    "    collections=json.load(f)\n",
    "cnt=0\n",
    "nums=[]\n",
    "cnt1=0\n",
    "for c in collections:\n",
    "    # if 'what' in c['Q'] or 'What' in c['Q']: \n",
    "    #     print(c['Q'])\n",
    "    #     continue\n",
    "    cnt1+=1\n",
    "    if c['Logit']==c['Label']:cnt+=1\n",
    "    else:\n",
    "        if bool(re.search(r'\\d', c['Logit'])): nums.append(c['id'])\n",
    "        # print('Logit: ',c['Logit'])\n",
    "        # print('Label: ',c['Label'])\n",
    "print(cnt)\n",
    "print(cnt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_dir /root/projects/code_lihan/experiment_questions_data/test_300-600_exWho/125_2363769.jpg\n",
      "What is the size of the book above the teddy bear, compared with a usual book? Answer with large or small in the image?\n",
      "Contexto_to_gpt4: My VQA question is: What is the size of the book above the teddy bear, compared with a usual book? Answer with large or small in the image?\n",
      "Small\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/root/projects/code_lihan/')\n",
    "import importlib\n",
    "import gpt4_img\n",
    "importlib.reload(gpt4_img)\n",
    "\n",
    "from gpt4_img import create_payload_normal,query_openai,create_payload_final_ans,create_payload_edge,create_payload_node\n",
    "# for i in range(len(imageIds)):\n",
    "    # imageId=str(imageIds[i])\n",
    "    # question=questions[i]\n",
    "    # answer=answers[i]\n",
    "# The white lamp is marked as 1, and the white curtain is marked as 2.\n",
    "\n",
    "payload = create_payload_final_ans( \"/root/projects/code_lihan/experiment_questions_data/test_300-600_exWho/125_2363769.jpg\", \"What is the size of the book above the teddy bear, compared with a usual book? Answer with large or small.\",'','')\n",
    "response = query_openai(payload)\n",
    "# print(question)\n",
    "print(response['choices'][0]['message']['content'])\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for imageId,question,img_name,label,obj_dict,axis_dict,obj_not_found,obj_filt_dict in zip(imageIds,questions,img_names,answers,obj_dicts,axis_dicts,obj_not_founds,obj_filt_dicts):\n",
    "#     if cnt<0: \n",
    "#         cnt+=1\n",
    "#         continue \n",
    "#     question_words=question[:-1].split(' ')\n",
    "#     if 'there' not in question_words and 'see' not in question_words:\n",
    "#         cnt+=1\n",
    "#         continue\n",
    "#     # if cnt==34: \n",
    "#     #     cnt+=1\n",
    "#     #     continue\n",
    "#     cnt+=1\n",
    "#     imageId=imageId.split('_')[1]\n",
    "#     print(f\"/root/projects/mmcot/gqa/images/{imageId}.jpg\")\n",
    "#     payload = create_payload_final_ans( f\"/root/projects/mmcot/gqa/images/{imageId}.jpg\", question,'','')\n",
    "#     response = query_openai(payload)\n",
    "#     # print(question)\n",
    "#     print(response['choices'][0]['message']['content'])\n",
    "#     print('answer',label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
