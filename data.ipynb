{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "with open(\"/root/projects/code_lihan/experiment_questions_data/QAI.pkl\",'rb') as f:\n",
    "    data=pickle.load(f)\n",
    "print(len(data))\n",
    "questions=[]\n",
    "imageIds=[]\n",
    "answers=[]\n",
    "for i in range(len(data)):\n",
    "    d=data[i]\n",
    "    questions.append(d['question'])\n",
    "    imageIds.append(d['imageId']) \n",
    "    answers.append(d['answer']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate nodes according to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Entity and Question ===========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is playing?\n",
      "['person']\n",
      "0\n",
      "The open umbrella is of what color?\n",
      "['umbrella']\n",
      "Which item of furniture is large, the bookcase or the desk?\n",
      "['bookcase','desk']\n",
      "What is the name of the piece of furniture that is made of the same material as the spoon to the left of her?\n",
      "['furniture','spoon','her']\n",
      "Who is wearing the wristband?\n",
      "['wristband']\n",
      "Is the basket behind a train?\n",
      "['basket','train']\n",
      "What kind of animal is above the grass?\n",
      "['animal','grass']\n",
      "Does the person next to the woman appear to be sleeping or reading?\n",
      "['person','woman']\n",
      "Which kind of furniture is tall?\n",
      "['furniture']\n",
      "How big is the vehicle on the right?\n",
      "['vehicle']\n",
      "Which color is the curtain to the right of the picture?\n",
      "['curtain','picture']\n",
      "What are the stairs in front of?\n",
      "['stairs']\n",
      "Is that woman to the left of a bag?\n",
      "['woman','bag']\n",
      "What are the beans inside of?\n",
      "['beans']\n",
      "Do you see any lamps or mirrors there?\n",
      "['lamps','mirrors']\n",
      "What type of device is to the right of the speaker?\n",
      "['device','speaker']\n",
      "Is the sidewalk made of concrete dry or wet?\n",
      "['sidewalk','concrete']\n",
      "Who is in front of the person that is crouching?\n",
      "['person']\n",
      "Is the color of the jeans the same as the sneakers?\n",
      "['jeans','sneakers']\n",
      "Which kind of animal is it?\n",
      "['animal']\n",
      "Is the building different in color than the sign?\n",
      "['building','sign']\n",
      "Is the old woman to the right or to the left of the man near the giraffe?\n",
      "['old woman','man','giraffe']\n",
      "Which side is the man on?\n",
      "['man']\n",
      "Are the pants blue?\n",
      "['pants']\n",
      "What is on the mountains?\n",
      "['mountains']\n",
      "Is the helmet made of the same material as the bench?\n",
      "['helmet','bench']\n",
      "Is the person that is bending resting or reading?\n",
      "['person']\n",
      "What is standing against the red bricks?\n",
      "['red bricks']\n",
      "What is the bottle cap made of?\n",
      "['bottle cap']\n",
      "What drink is the cup full of?\n",
      "['drink','cup']\n",
      "What vehicle is the telephone pole behind of, a train or a bus?\n",
      "['vehicle','telephone pole','train','bus']\n",
      "What color do you think these curtains are?\n",
      "['curtains']\n",
      "What is sitting beside the appliance that is beside the knives?\n",
      "['appliance','knives']\n",
      "Which color does the lamp made of metal have?\n",
      "['lamp','metal']\n",
      "Who is driving the motorcycle?\n",
      "['person','motorcycle']\n",
      "Is the ground dry or wet?\n",
      "['ground']\n",
      "Is the soccer player in front of the goal watching the ball?\n",
      "['soccer player','goal','ball']\n",
      "Which kind of fast food is to the right of the mug?\n",
      "['fast food','mug']\n",
      "What is the vegetable that is the same shape as the stove top called?\n",
      "['vegetable','stove top']\n",
      "What appliance is below the oil near the wine?\n",
      "['appliance','oil','wine']\n",
      "Which is larger, the pasture or the horse?\n",
      "['pasture','horse']\n",
      "What is the pattern of the towel?\n",
      "['towel']\n",
      "What is the glass made of glass sitting on?\n",
      "['glass']\n",
      "What is in front of the couch made of cloth?\n",
      "['couch']\n",
      "Who is skiing?\n",
      "['person','skiing']\n",
      "Who wears a shirt?\n",
      "['person','shirt']\n",
      "Does the person that is sitting appear to be posing or looking down?\n",
      "['person']\n",
      "Does the oven have a different color than the stove?\n",
      "['oven','stove']\n",
      "What is leaning against the glass window?\n",
      "['glass window']\n",
      "What do you think is the crispy meat?\n",
      "['crispy meat']\n",
      "Who is holding the baseball mitt?\n",
      "['person','baseball mitt']\n",
      "Are there both notebooks and cans in the scene?\n",
      "['notebooks','cans']\n",
      "What is the man to the left of the woman sitting in front of?\n",
      "['man','woman']\n",
      "Is the black speaker both little and narrow?\n",
      "['black speaker']\n",
      "Are the trousers short and dark?\n",
      "['trousers']\n",
      "Are there red tables or ottomen?\n",
      "['tables','ottomen']\n",
      "Is the doll to the right of the ladder made of plastic?\n",
      "['doll','ladder']\n",
      "Does the jersey have the same color as the cap?\n",
      "['jersey','cap']\n",
      "In which part are the glasses, the top or the bottom?\n",
      "['glasses','top','bottom']\n",
      "Do the shelves look brown and short?\n",
      "['shelves']\n",
      "Is the coat white and long sleeved?\n",
      "['coat']\n",
      "Are the trees near the rock short and green?\n",
      "['trees','rock']\n",
      "Is the sky clear?\n",
      "['sky']\n",
      "Is the curtain to the left of a pillow?\n",
      "['curtain','pillow']\n",
      "Does the polo shirt look white and short sleeved?\n",
      "['polo shirt']\n",
      "Which material is the laptop near the glass made of?\n",
      "['laptop','glass']\n",
      "What is the soap dispenser sitting on?\n",
      "['soap dispenser']\n",
      "Does the toy above the girl look small and white?\n",
      "['toy','girl']\n",
      "What do the faucet and the light fixture have in common?\n",
      "['faucet','light fixture']\n",
      "What is located on top of the woman in the middle?\n",
      "['woman']\n",
      "Are there beds or lamps in the picture?\n",
      "['beds','lamps']\n",
      "Is she to the left or to the right of the pillow that looks soft?\n",
      "['she','pillow']\n",
      "What item of furniture is not small?\n",
      "['furniture']\n",
      "How big is the flower on the surfboard?\n",
      "['flower','surfboard']\n",
      "Is the mouse to the left of a keyboard?\n",
      "['mouse','keyboard']\n",
      "Are there any men to the left of the vegetable that is not rotten?\n",
      "['men','vegetable']\n",
      "What are the flowers in front of?\n",
      "['flowers']\n",
      "Is the color of the ceiling different than the plate?\n",
      "['ceiling','plate']\n",
      "Does the notebook which is to the left of the napkin look black and closed?\n",
      "['notebook','napkin']\n",
      "What is the fence made of?\n",
      "['fence']\n",
      "Is there a computer or a Wii controller in this photo?\n",
      "['computer','Wii controller']\n",
      "What kind of furniture is long?\n",
      "['furniture']\n",
      "What is the gender of the skateboarder to the right of the lamp?\n",
      "['skateboarder','lamp']\n",
      "What animal stands at the grass?\n",
      "['animal','grass']\n",
      "What is the man in front of?\n",
      "['man']\n",
      "Who is standing behind the trash can?\n",
      "['person','trash can']\n",
      "Is the road near the building dark and narrow?\n",
      "['road','building']\n",
      "The whipped cream is in front of what?\n",
      "['whipped cream']\n",
      "Which kind of device is the keyboard sitting next to?\n",
      "['device','keyboard']\n",
      "Who is standing?\n",
      "['person']\n",
      "Do you see fire hydrants on top of the sidewalk that is not dry?\n",
      "['fire hydrants','sidewalk']\n",
      "What is the driver driving?\n",
      "['driver','vehicle']\n",
      "Is the bat that looks gray and red made of wood?\n",
      "['bat']\n",
      "Are these pillows or towels?\n",
      "['pillows','towels']\n",
      "Do you think the person that looks young is blond or brunette?\n",
      "['person']\n",
      "Is the light fixture made of the same material as the stop sign?\n",
      "['light fixture','stop sign']\n",
      "How big is the bath tub?\n",
      "['bath tub']\n",
      "What material is the road, asphalt or cobblestone?\n",
      "['road']\n",
      "Is the towel in the top or in the bottom of the image?\n",
      "['towel']\n",
      "What is the color of the appliance made of plastic?\n",
      "['appliance','plastic']\n",
      "['person'] Who is playing? n126891 boy\n",
      "['umbrella'] The open umbrella is of what color? n184551 blue\n",
      "['bookcase', 'desk'] Which item of furniture is large, the bookcase or the desk? n520071 desk\n",
      "['furniture', 'spoon', 'her'] What is the name of the piece of furniture that is made of the same material as the spoon to the left of her? n433532 shelf\n",
      "['wristband'] Who is wearing the wristband? n496803 athlete\n",
      "['basket', 'train'] Is the basket behind a train? n290409 no\n",
      "['animal', 'grass'] What kind of animal is above the grass? n196058 zebras\n",
      "['person', 'woman'] Does the person next to the woman appear to be sleeping or reading? n184551 reading\n",
      "['furniture'] Which kind of furniture is tall? n283587 chairs\n",
      "['vehicle'] How big is the vehicle on the right? n206358 large\n",
      "['curtain'] Which color is the curtain to the right of the picture? n192021 yellow\n",
      "['stairs'] What are the stairs in front of? n346247 doors\n",
      "['woman', 'bag'] Is that woman to the left of a bag? n160664 no\n",
      "['beans'] What are the beans inside of? n296467 bowl\n",
      "['lamps', 'mirrors'] Do you see any lamps or mirrors there? n446242 yes\n",
      "['device', 'speaker'] What type of device is to the right of the speaker? n77818 television\n",
      "['sidewalk', 'concrete'] Is the sidewalk made of concrete dry or wet? n514467 wet\n",
      "['person'] Who is in front of the person that is crouching? n130638 batter\n",
      "['jeans', 'sneakers'] Is the color of the jeans the same as the sneakers? n65202 no\n",
      "['animal'] Which kind of animal is it? n59627 elephant\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/root/projects/code_lihan/')\n",
    "import graph_generation\n",
    "import importlib\n",
    "importlib.reload(graph_generation)\n",
    "from graph_generation import entity_generation\n",
    "nodes,_=entity_generation(questions)\n",
    "tmpN=[]\n",
    "tmpQ=[]\n",
    "tmpI=[]\n",
    "tmpA=[]\n",
    "for i in range(len(nodes)):\n",
    "    node=nodes[i]\n",
    "    question=questions[i]\n",
    "    imageId=imageIds[i]\n",
    "    answer=answers[i]\n",
    "    if len(node)>=0:\n",
    "        tmpN.append(node)\n",
    "        tmpQ.append(question)\n",
    "        tmpI.append(imageId)\n",
    "        tmpA.append(answer)\n",
    "nodes=tmpN\n",
    "questions=tmpQ\n",
    "answers=tmpA\n",
    "imageIds=tmpI\n",
    "# for i in range(len(nodes)):\n",
    "for i in range(20):\n",
    "    node=nodes[i]\n",
    "    question=questions[i]\n",
    "    imageId=imageIds[i]\n",
    "    answer=answers[i]\n",
    "    print(node,question,imageId,answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=400\n",
    "with open(f'{num}-{num+200}_Q.txt','w') as f:\n",
    "    for q in questions:\n",
    "        f.write(q+'\\n')\n",
    "# with open('MoreThanTwoGoodImg.txt','w') as f:\n",
    "with open(f'{num}-{num+200}_Img.txt','w') as f:\n",
    "    for q in imageIds:\n",
    "        f.write(q+'\\n')\n",
    "with open(f'{num}-{num+200}_A.txt','w') as f:\n",
    "    for a in answers:\n",
    "        f.write(a+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'{num}-{num+200}_nodes.json',\"w\") as f:\n",
    "    json.dump(nodes,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processQ.txt','w') as f:\n",
    "    for q in questions:\n",
    "        f.write(q+'\\n')\n",
    "with open('processImg.txt','w') as f:\n",
    "    for imageId in imageIds:\n",
    "        f.write(str(imageId)+'\\n')\n",
    "with open('processA.txt','w') as f:\n",
    "    for a in answers:\n",
    "        f.write(a+'\\n')\n",
    "import json\n",
    "with open(\"processNodes.json\",\"w\") as f:\n",
    "    json.dump(nodes,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate images with object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "num=400\n",
    "with open(f'{num}-{num+200}_nodes.json', 'r') as file:\n",
    "    nodes = json.load(file)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "with open(f'{num}-{num+200}_Q.txt','r') as f:\n",
    "# with open('MoreThanTwoGoodQ.txt','r') as f:\n",
    "    questions=[]\n",
    "    for line in f:\n",
    "        questions.append(line[:-1])\n",
    "print(len(questions))\n",
    "# with open('MoreThanTwoGoodImg.txt','r') as f:\n",
    "with open(f'{num}-{num+200}_Img.txt','r') as f:\n",
    "    imageIds=[]\n",
    "    for line in f:\n",
    "        imageIds.append(line[:-1])\n",
    "print(len(imageIds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/root/projects/code_lihan/')\n",
    "import importlib\n",
    "import objects_graph\n",
    "import groundingdino.util.inference \n",
    "importlib.reload(groundingdino.util.inference)\n",
    "importlib.reload(objects_graph)\n",
    "from objects_graph import ObjModel\n",
    "import os \n",
    "# config_file = '/root/projects/mmcot/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py'  # change the path of the model config file\n",
    "\n",
    "# grounded_checkpoint = '/root/projects/mmcot/GroundingDINO/weights/groundingdino_swint_ogc.pth'  # change the path of the model\n",
    "config_file = '/root/projects/mmcot/GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py'  # change the path of the model config file\n",
    "# # ram_checkpoint = '/root/autodl-tmp/eva_weights/ram_plus_swin_large_14m.pth'  # change the path of the model\n",
    "grounded_checkpoint = '/root/projects/mmcot/GroundingDINO/weights/groundingdino_swinb_cogcoor.pth'  # change the path of the model\n",
    "model = ObjModel(grounded_checkpoint, config_file)\n",
    "obj_dicts=[]\n",
    "axis_dicts=[]\n",
    "obj_not_founds=[]\n",
    "obj_filt_dicts=[]\n",
    "# path_out='./test_morethan2'\n",
    "\n",
    "os.makedirs(f'./test_{num}-{num+200}', exist_ok=True)\n",
    "image_processed_Ids=[]\n",
    "for i in range(len(nodes)):\n",
    "# 调用方法\n",
    "    # if i < 88: continue\n",
    "    objlist = nodes[i]\n",
    "    path='/root/projects/mmcot/gqa/images'\n",
    "    path_out=f'./test_{num}-{num+200}'\n",
    "    img_name = imageIds[i]+'.jpg'\n",
    "    # if img_name in os.listdir(path_out):\n",
    "    #     img_id_out=imageIds[i]\n",
    "    # else:\n",
    "    img_id_out=str(i)+'_'+imageIds[i]\n",
    "    \n",
    "    print(\"img_name: \",img_id_out)\n",
    "    image_processed_Ids.append(img_id_out)\n",
    "    obj_dict,obj_not_found ,obj_filt_dict= model.find_obj( objlist, img_name, img_id_out, path,path_out,need_obj_boxes=False)\n",
    "    print(\"obj_dict\",obj_dict)\n",
    "\n",
    "    dict_ori=obj_dict\n",
    "    obj_dict={}\n",
    "    axis_dict={}\n",
    "    for key,val in dict_ori.items():\n",
    "        num_=[]\n",
    "        for key_obj,val_obj in val.items():\n",
    "            num_.append(val_obj['number'])\n",
    "            axis_dict[val_obj['number']]=val_obj['bbox']\n",
    "        obj_dict[key]=num_\n",
    "    obj_dicts.append(obj_dict)\n",
    "    axis_dicts.append(axis_dict)\n",
    "    obj_not_founds.append(obj_not_found)\n",
    "    obj_filt_dicts.append(obj_filt_dict)\n",
    "    # if i==20: break\n",
    "    # break\n",
    "import json\n",
    "with open(f'obj_dicts_{num}-{num+200}.json','w') as f:\n",
    "    json.dump(obj_dicts,f)\n",
    "with open(f'axis_dicts_{num}-{num+200}.json','w') as f:\n",
    "    json.dump(axis_dicts,f)\n",
    "with open(f'obj_not_founds_{num}-{num+200}.json','w') as f:\n",
    "    json.dump(obj_not_founds,f)\n",
    "with open(f'obj_filt_dicts_{num}-{num+200}.json','w') as f:\n",
    "    json.dump(obj_filt_dicts,f)\n",
    "with open(f'image_processed_Ids_{num}-{num+200}.txt','w') as f:\n",
    "    for imageId in image_processed_Ids:\n",
    "        f.write(str(imageId)+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start running~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "num=0\n",
    "import json\n",
    "with open(f'obj_dicts_{num}-{num+200}.json','r') as f:\n",
    "    obj_dicts=json.load(f)\n",
    "with open(f'axis_dicts_{num}-{num+200}.json','r') as f:\n",
    "    axis_dicts=json.load(f)\n",
    "with open(f'obj_not_founds_{num}-{num+200}.json','r') as f:\n",
    "    obj_not_founds=json.load(f)\n",
    "with open(f'{num}-{num+200}_Q.txt','r') as f:\n",
    "    questions=[]\n",
    "    for line in f:\n",
    "        questions.append(line[:-1])\n",
    "print(len(questions))\n",
    "with open(f'image_processed_Ids_{num}-{num+200}.txt','r') as f:\n",
    "    imageIds=[]\n",
    "    for line in f:\n",
    "        imageIds.append(line[:-1])\n",
    "print(len(imageIds))\n",
    "with open(f'{num}-{num+200}_A.txt','r') as f:\n",
    "    answers=[]\n",
    "    for line in f:\n",
    "        answers.append(line[:-1])\n",
    "print(len(answers))\n",
    "with open(f'obj_filt_dicts_{num}-{num+200}.json','r') as f:\n",
    "    obj_filt_dicts=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The 1 sample:\n",
      "0_n161313 Is it overcast? no {'sky': [1]}\n",
      "all_nodes [['sky']]\n",
      "========== Node question Generation ===========\n",
      "False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_question,posi_q {'sky': [\"None, because there is no characteristic about 'sky' itself.\"]} {'sky': ['Is sky in the image?']}\n",
      "========== Dict Generation ===========\n",
      "['Is it overcast?']\n",
      "=========generate edge============\n",
      "all_edgequestions edge [{}]\n",
      "all_relationship_dict:  [{}]\n",
      "combined_relation {}\n",
      "==========Generated graph==========\n",
      "[{'sky': {'node_question': ['Is sky in the image?']}}]\n",
      "Main_node: sky\n",
      "  Main_node Question: Is sky in the image?\n",
      "\n",
      "obj_dict to update the question {'sky': [1]}\n",
      "updated_question  Is sky [1] in the image?\n",
      "label_sentence In the image, there are one sky marked by 1.\n",
      "{'agent': {'next': 'judge_obj_tool'}}\n",
      "-----------------\n",
      "question\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/0_n161313_sky_merged_image.jpg\n",
      "{'judge_obj_tool': {'Tool_return': [FunctionMessage(content='{\"1\": [\"Yes\", \"large\"]}', name='judge_obj_tool')]}}\n",
      "-----------------\n",
      "ans {\"1\": [\"Yes\", \"large\"]}\n",
      "ans_dict {'1': ['Yes', 'large']}\n",
      "obj_dict after update {'sky': [1]}\n",
      "obj_filt_dict after update {'sky': [[0.0, 0.0, 640.0, 427.0]]}\n",
      "image_ori_dir /root/projects/mmcot/gqa/images/n161313.jpg\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/0_n161313_new.jpg\n",
      "img_dir /root/projects/code_lihan/experiment_questions_data/test_0-200/0_n161313_new.jpg\n",
      "\n",
      "======ans_graph_entity===========\n",
      "{'sky': {'node_question': ['{\"1\": [\"Yes\", \"large\"]}']}}\n",
      "\n",
      "=======All-answers===========\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "=======The Final Answer===========\n",
      "The question is : Is it overcast?\n",
      "\n",
      "The context is:  \n",
      "image_dir /root/projects/code_lihan/experiment_questions_data/test_0-200/0_n161313_new.jpg\n",
      "Is it overcast in the image?\n",
      "Contexto_to_gpt4: My VQA question is: Is it overcast in the image?\n",
      "{\"status_code\": 200, \"request_id\": \"e74a1a03-149a-9c7d-9fa6-07da71c61886\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": [{\"text\": \"No\"}]}}]}, \"usage\": {\"input_tokens\": 654, \"output_tokens\": 2, \"image_tokens\": 345}}\n",
      "\n",
      "The final answer is:  No\n",
      "logit no\n",
      "modify ans no\n",
      "[{'id': 1, 'Label': 'no', 'Logit': 'no', 'Q': 'Is it overcast?', 'img_name': '/root/projects/code_lihan/experiment_questions_data/test_0-200/0_n161313.jpg'}]\n",
      "\n",
      "\n",
      "The 2 sample:\n",
      "1_n414992 What is the airplane flying above? ocean {'airplane': [1]}\n",
      "all_nodes [['airplane']]\n",
      "========== Node question Generation ===========\n",
      "False\n",
      "node_question,posi_q {'airplane': ['What is the airplane flying above?']} {'airplane': ['Is airplane in the image?', 'What is the airplane flying above?']}\n",
      "========== Dict Generation ===========\n",
      "['What is the airplane flying above?']\n",
      "=========generate edge============\n",
      "all_edgequestions edge [{}]\n",
      "all_relationship_dict:  [{}]\n",
      "combined_relation {}\n",
      "==========Generated graph==========\n",
      "[{'airplane': {'node_question': ['Is airplane in the image?', 'What is the airplane flying above?', 'What is the airplane flying above?']}}]\n",
      "Main_node: airplane\n",
      "  Main_node Question: Is airplane in the image?\n",
      "\n",
      "obj_dict to update the question {'airplane': [1]}\n",
      "updated_question  Is airplane [1] in the image?\n",
      "label_sentence In the image, there are one airplane marked by 1.\n",
      "{'agent': {'next': 'judge_obj_tool'}}\n",
      "-----------------\n",
      "question\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/1_n414992_airplane_merged_image.jpg\n",
      "{'judge_obj_tool': {'Tool_return': [FunctionMessage(content='{\"1\": [\"Yes\", \"large\"]}', name='judge_obj_tool')]}}\n",
      "-----------------\n",
      "ans {\"1\": [\"Yes\", \"large\"]}\n",
      "ans_dict {'1': ['Yes', 'large']}\n",
      "obj_dict after update {'airplane': [1]}\n",
      "obj_filt_dict after update {'airplane': [[236.38934326171875, 167.7982635498047, 446.868408203125, 237.82940673828125]]}\n",
      "image_ori_dir /root/projects/mmcot/gqa/images/n414992.jpg\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/1_n414992_new.jpg\n",
      "img_dir /root/projects/code_lihan/experiment_questions_data/test_0-200/1_n414992_new.jpg\n",
      "  Main_node Question: What is the airplane flying above?\n",
      "\n",
      "obj_dict to update the question {'airplane': [1]}\n",
      "updated_question  What is the airplane [1] flying above?\n",
      "label_sentence In the image, there are one airplane marked by 1.\n",
      "{'agent': {'next': 'VQA_tool'}}\n",
      "-----------------\n",
      "What is the airplane [1] flying above in the image?\n",
      "Complete result from VQA tool:  Thought: The airplane [1] is flying above the sky.\n",
      "Answer: The airplane [1] is flying above the sky.\n",
      "---------------\n",
      "{'VQA_tool': {'Tool_return': [FunctionMessage(content='The airplane [1] is flying above the sky.', name='VQA_tool')]}}\n",
      "-----------------\n",
      "ans The airplane [1] is flying above the sky.\n",
      "obj_dict after update {'airplane': [1]}\n",
      "  Main_node Question: What is the airplane flying above?\n",
      "\n",
      "obj_dict to update the question {'airplane': [1]}\n",
      "updated_question  What is the airplane [1] flying above?\n",
      "label_sentence In the image, there are one airplane marked by 1.\n",
      "{'agent': {'next': 'VQA_tool'}}\n",
      "-----------------\n",
      "What is the airplane [1] flying above in the image?\n",
      "Complete result from VQA tool:  Thought: The airplane [1] is flying above the sky.\n",
      "Answer: The airplane [1] is flying above the sky.\n",
      "---------------\n",
      "{'VQA_tool': {'Tool_return': [FunctionMessage(content='The airplane [1] is flying above the sky.', name='VQA_tool')]}}\n",
      "-----------------\n",
      "ans The airplane [1] is flying above the sky.\n",
      "obj_dict after update {'airplane': [1]}\n",
      "\n",
      "======ans_graph_entity===========\n",
      "{'airplane': {'node_question': ['{\"1\": [\"Yes\", \"large\"]}', 'The airplane [1] is flying above the sky.', 'The airplane [1] is flying above the sky.']}}\n",
      "\n",
      "=======All-answers===========\n",
      "[]\n",
      "The airplane [1] is flying above the sky. The airplane [1] is flying above the sky.\n",
      "The airplane [1] is flying above the sky. The airplane [1] is flying above the sky.\n",
      "\n",
      "=======The Final Answer===========\n",
      "The question is : What is the airplane flying above?\n",
      "\n",
      "The context is:  The airplane [1] is flying above the sky. The airplane [1] is flying above the sky.\n",
      "image_dir /root/projects/code_lihan/experiment_questions_data/test_0-200/1_n414992_new.jpg\n",
      "What is the airplane flying above in the image?\n",
      "Contexto_to_gpt4: The context information is as follows: <The airplane [1] is flying above the sky. The airplane [1] is flying above the sky.>. My VQA question is: What is the airplane flying above in the image?\n",
      "{\"status_code\": 200, \"request_id\": \"d78f8543-81ca-9698-a73a-50108c881875\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": [{\"text\": \"sky\"}]}}]}, \"usage\": {\"input_tokens\": 687, \"output_tokens\": 2, \"image_tokens\": 345}}\n",
      "\n",
      "The final answer is:  sky\n",
      "logit sky\n",
      "modify ans sky\n",
      "\n",
      "\n",
      "The 3 sample:\n",
      "2_n23181 Are there drapes to the right of the bed? yes {'drapes': [1], 'bed': [2]}\n",
      "all_nodes [['drapes', 'bed']]\n",
      "========== Node question Generation ===========\n",
      "False\n",
      "node_question,posi_q {'drapes': [\"None, because there is no characteristic about 'drapes' itself.\"], 'bed': [\"None, because there is no characteristic about 'bed' itself.\"]} {'drapes': ['Is drapes in the image?'], 'bed': ['Is bed in the image?']}\n",
      "========== Dict Generation ===========\n",
      "['Are there drapes to the right of the bed?']\n",
      "new_obj_dict Thought: Since it says if the 'drapes' are to the right of the 'bed': ('drapes','bed'). Answer: [('drapes','bed')]\n",
      "[('drapes','bed')]\n",
      "extracted_tuples [('drapes', 'bed')]\n",
      "=========generate edge============\n",
      "all_edgequestions edge [{('drapes', 'bed'): ['Is the bed to the right of the drapes?']}]\n",
      "all_relationship_dict:  [{'drapes': ['bed']}]\n",
      "combined_relation {'drapes': ['bed']}\n",
      "==========Generated graph==========\n",
      "[{'drapes': {'node_question': ['Is drapes in the image?'], 'edge': [{'bed': {'edge_question': ['Is the bed to the right of the drapes?'], 'node_question': ['Is bed in the image?']}}]}}]\n",
      "Main_node: drapes\n",
      "  Main_node Question: Is drapes in the image?\n",
      "\n",
      "obj_dict to update the question {'drapes': [1], 'bed': [2]}\n",
      "updated_question  Is drapes [1] in the image?\n",
      "label_sentence In the image, there are one drapes marked by 1, and one bed marked by 2.\n",
      "{'agent': {'next': 'judge_obj_tool'}}\n",
      "-----------------\n",
      "question\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/2_n23181_drapes_merged_image.jpg\n",
      "{'judge_obj_tool': {'Tool_return': [FunctionMessage(content='{\"1\": [\"Yes\", \"large\"]}', name='judge_obj_tool')]}}\n",
      "-----------------\n",
      "ans {\"1\": [\"Yes\", \"large\"]}\n",
      "ans_dict {'1': ['Yes', 'large']}\n",
      "obj_dict after update {'drapes': [1], 'bed': [2]}\n",
      "obj_filt_dict after update {'drapes': [[568.12744140625, 0.0, 640.0, 342.74432373046875]], 'bed': [[274.9132995605469, 207.0, 627.51025390625, 427.0]]}\n",
      "image_ori_dir /root/projects/mmcot/gqa/images/n23181.jpg\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/2_n23181_new.jpg\n",
      "img_dir /root/projects/code_lihan/experiment_questions_data/test_0-200/2_n23181_new.jpg\n",
      "  Edge to bed\n",
      "    Edge Question: Is the bed to the right of the drapes?\n",
      "\n",
      "obj_dict to update the question {'drapes': [1], 'bed': [2]}\n",
      "updated_question  Is the bed [2] to the right of the drapes [1]?\n",
      "label_sentence In the image, there are one drapes marked by 1, and one bed marked by 2.\n",
      "{'agent': {'next': 'left_right_tool'}}\n",
      "-----------------\n",
      "obj_dict {'drapes': [1], 'bed': [2]}\n",
      "axis_dict {1: [568, 0, 640, 342], 2: [274, 207, 627, 427]}\n",
      "number_list:  ['2', '1']\n",
      " The result of left right tool:  drapes [1] is not to the right of bed [2]. \n",
      "---------------\n",
      "{'left_right_tool': {'Tool_return': [FunctionMessage(content='drapes [1] is not to the right of bed [2]. ', name='left_right_tool')]}}\n",
      "-----------------\n",
      "obj_dict_outedge {'drapes': [], 'bed': []}\n",
      "obj_dict after update {'drapes': [1], 'bed': [2]}\n",
      "    Node Question at bed: Is bed in the image?\n",
      "\n",
      "obj_dict to update the question {'drapes': [1], 'bed': [2]}\n",
      "updated_question  Is bed [2] in the image?\n",
      "label_sentence In the image, there are one drapes marked by 1, and one bed marked by 2.\n",
      "{'agent': {'next': 'judge_obj_tool'}}\n",
      "-----------------\n",
      "question\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/2_n23181_new_bed_merged_image.jpg\n",
      "{'judge_obj_tool': {'Tool_return': [FunctionMessage(content='{\"2\": [\"Yes\", \"large\"]}', name='judge_obj_tool')]}}\n",
      "-----------------\n",
      "ans_subnode {\"2\": [\"Yes\", \"large\"]}\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/2_n23181_new_new.jpg\n",
      "\n",
      "======ans_graph_entity===========\n",
      "{'drapes': {'node_question': ['{\"1\": [\"Yes\", \"large\"]}'], 'edge': [{'bed': {'edge_question': 'drapes [1] is not to the right of bed [2]. ', 'node_question': []}}]}}\n",
      "\n",
      "=======All-answers===========\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "=======The Final Answer===========\n",
      "The question is : Are there drapes to the right of the bed?\n",
      "\n",
      "The context is:  \n",
      "image_dir /root/projects/code_lihan/experiment_questions_data/test_0-200/2_n23181_new_new.jpg\n",
      "Are there drapes to the right of the bed in the image?\n",
      "Contexto_to_gpt4: My VQA question is: Are there drapes to the right of the bed in the image?\n",
      "{\"status_code\": 200, \"request_id\": \"0ce920c6-fc7d-918a-a0a4-cc8c759df579\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": [{\"text\": \"No\"}]}}]}, \"usage\": {\"input_tokens\": 660, \"output_tokens\": 2, \"image_tokens\": 345}}\n",
      "\n",
      "The final answer is:  No\n",
      "logit no\n",
      "modify ans no\n",
      "\n",
      "\n",
      "The 4 sample:\n",
      "3_n151768 Is the jacket long sleeved and black? yes {'jacket': [1]}\n",
      "all_nodes [['jacket']]\n",
      "========== Node question Generation ===========\n",
      "False\n",
      "node_question,posi_q {'jacket': ['Is the jacket long-sleeved?', 'Is the jacket black?']} {'jacket': ['Is jacket in the image?']}\n",
      "========== Dict Generation ===========\n",
      "['Is the jacket long sleeved and black?']\n",
      "=========generate edge============\n",
      "未找到 'Answer' 的内容\n",
      "all_edgequestions edge ['No edge_question']\n",
      "all_relationship_dict:  [{}]\n",
      "combined_relation {}\n",
      "==========Generated graph==========\n",
      "[{'jacket': {'node_question': ['Is jacket in the image?', 'Is the jacket long-sleeved?', 'Is the jacket black?']}}]\n",
      "Main_node: jacket\n",
      "  Main_node Question: Is jacket in the image?\n",
      "\n",
      "obj_dict to update the question {'jacket': [1]}\n",
      "updated_question  Is jacket [1] in the image?\n",
      "label_sentence In the image, there are one jacket marked by 1.\n",
      "{'agent': {'next': 'judge_obj_tool'}}\n",
      "-----------------\n",
      "question\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/3_n151768_jacket_merged_image.jpg\n",
      "{'judge_obj_tool': {'Tool_return': [FunctionMessage(content='{\"1\": [\"Yes\", \"large\"]}', name='judge_obj_tool')]}}\n",
      "-----------------\n",
      "ans {\"1\": [\"Yes\", \"large\"]}\n",
      "ans_dict {'1': ['Yes', 'large']}\n",
      "obj_dict after update {'jacket': [1]}\n",
      "obj_filt_dict after update {'jacket': [[304.31353759765625, 249.66712951660156, 373.80780029296875, 316.93902587890625]]}\n",
      "image_ori_dir /root/projects/mmcot/gqa/images/n151768.jpg\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/3_n151768_new.jpg\n",
      "img_dir /root/projects/code_lihan/experiment_questions_data/test_0-200/3_n151768_new.jpg\n",
      "  Main_node Question: Is the jacket long-sleeved?\n",
      "\n",
      "obj_dict to update the question {'jacket': [1]}\n",
      "updated_question  Is the jacket [1] long-sleeved?\n",
      "label_sentence In the image, there are one jacket marked by 1.\n",
      "{'agent': {'next': 'VQA_tool'}}\n",
      "-----------------\n",
      "Is the jacket [1] long-sleeved in the image?\n",
      "Complete result from VQA tool:  Thought: I need to determine if the jacket labeled as 1 has long sleeves.\n",
      "Answer: Yes, the jacket [1] is long-sleeved in the image.\n",
      "---------------\n",
      "{'VQA_tool': {'Tool_return': [FunctionMessage(content='Yes, the jacket [1] is long-sleeved in the image.', name='VQA_tool')]}}\n",
      "-----------------\n",
      "ans Yes, the jacket [1] is long-sleeved in the image.\n",
      "obj_dict after update {'jacket': [1]}\n",
      "  Main_node Question: Is the jacket black?\n",
      "\n",
      "obj_dict to update the question {'jacket': [1]}\n",
      "updated_question  Is the jacket [1] black?\n",
      "label_sentence In the image, there are one jacket marked by 1.\n",
      "{'agent': {'next': 'VQA_tool'}}\n",
      "-----------------\n",
      "Is the jacket [1] black in the image?\n",
      "Complete result from VQA tool:  Thought: I need to identify the color of the jacket marked by 1. Answer: Yes, the jacket [1] is black.\n",
      "---------------\n",
      "{'VQA_tool': {'Tool_return': [FunctionMessage(content='Yes, the jacket [1] is black.', name='VQA_tool')]}}\n",
      "-----------------\n",
      "ans Yes, the jacket [1] is black.\n",
      "obj_dict after update {'jacket': [1]}\n",
      "\n",
      "======ans_graph_entity===========\n",
      "{'jacket': {'node_question': ['{\"1\": [\"Yes\", \"large\"]}', 'Yes, the jacket [1] is long-sleeved in the image.', 'Yes, the jacket [1] is black.']}}\n",
      "\n",
      "=======All-answers===========\n",
      "[]\n",
      "Yes, the jacket [1] is long-sleeved in the image. Yes, the jacket [1] is black.\n",
      "Yes, the jacket [1] is long-sleeved in the image. Yes, the jacket [1] is black.\n",
      "\n",
      "=======The Final Answer===========\n",
      "The question is : Is the jacket long sleeved and black?\n",
      "\n",
      "The context is:  Yes, the jacket [1] is long-sleeved in the image. Yes, the jacket [1] is black.\n",
      "image_dir /root/projects/code_lihan/experiment_questions_data/test_0-200/3_n151768_new.jpg\n",
      "Is the jacket long sleeved and black in the image?\n",
      "Contexto_to_gpt4: The context information is as follows: <Yes, the jacket [1] is long-sleeved in the image. Yes, the jacket [1] is black.>. My VQA question is: Is the jacket long sleeved and black in the image?\n",
      "{\"status_code\": 200, \"request_id\": \"e3dfc66f-92fb-96d8-b0ae-42da940dc414\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": [{\"text\": \"Yes\"}]}}]}, \"usage\": {\"input_tokens\": 693, \"output_tokens\": 2, \"image_tokens\": 345}}\n",
      "\n",
      "The final answer is:  Yes\n",
      "logit yes\n",
      "modify ans yes\n",
      "\n",
      "\n",
      "The 5 sample:\n",
      "4_n52544 What place is the skinny person standing? table {'skinny person': [1]}\n",
      "all_nodes [['skinny person']]\n",
      "========== Node question Generation ===========\n",
      "False\n",
      "node_question,posi_q {'skinny person': ['What type is the skinny person?']} {'skinny person': ['Is skinny person in the image?', 'Where is the skinny person standing on the image?']}\n",
      "========== Dict Generation ===========\n",
      "['What place is the skinny person standing?']\n",
      "=========generate edge============\n",
      "未找到 'Answer' 的内容\n",
      "all_edgequestions edge ['No edge_question']\n",
      "all_relationship_dict:  [{}]\n",
      "combined_relation {}\n",
      "==========Generated graph==========\n",
      "[{'skinny person': {'node_question': ['Is skinny person in the image?', 'Where is the skinny person standing on the image?', 'What type is the skinny person?']}}]\n",
      "Main_node: skinny person\n",
      "  Main_node Question: Is skinny person in the image?\n",
      "\n",
      "obj_dict to update the question {'skinny person': [1]}\n",
      "updated_question  Is skinny person [1] in the image?\n",
      "label_sentence In the image, there are one skinny person marked by 1.\n",
      "{'agent': {'next': 'judge_obj_tool'}}\n",
      "-----------------\n",
      "question\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/4_n52544_skinnyperson_merged_image.jpg\n",
      "{'judge_obj_tool': {'Tool_return': [FunctionMessage(content='{\"1\": [\"No\", \"large\"]}', name='judge_obj_tool')]}}\n",
      "-----------------\n",
      "ans {\"1\": [\"No\", \"large\"]}\n",
      "ans_dict {'1': ['No', 'large']}\n",
      "obj_dict after update {'skinny person': []}\n",
      "obj_filt_dict after update {'skinny person': []}\n",
      "image_ori_dir /root/projects/mmcot/gqa/images/n52544.jpg\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/4_n52544_new.jpg\n",
      "img_dir /root/projects/code_lihan/experiment_questions_data/test_0-200/4_n52544_new.jpg\n",
      "  Main_node Question: Where is the skinny person standing on the image?\n",
      "\n",
      "obj_dict {'skinny person': []}\n",
      "  Main_node Question: What type is the skinny person?\n",
      "\n",
      "obj_dict {'skinny person': []}\n",
      "\n",
      "======ans_graph_entity===========\n",
      "{'skinny person': {}}\n",
      "\n",
      "=======All-answers===========\n",
      "[1]\n",
      "\n",
      "\n",
      "\n",
      "=======The Final Answer===========\n",
      "The question is : What place is the skinny person standing?\n",
      "\n",
      "The context is:  \n",
      "image_dir /root/projects/code_lihan/experiment_questions_data/test_0-200/4_n52544_new.jpg\n",
      "What place is the skinny person standing?\n",
      "Contexto_to_gpt4: My VQA question is: What place is the skinny person standing?\n",
      "{\"status_code\": 200, \"request_id\": \"f9c01308-6364-9537-8304-33046fb65e41\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": [{\"text\": \"party\"}]}}]}, \"usage\": {\"input_tokens\": 654, \"output_tokens\": 2, \"image_tokens\": 345}}\n",
      "\n",
      "The final answer is:  party\n",
      "logit party\n",
      "modify ans party\n",
      "\n",
      "\n",
      "The 6 sample:\n",
      "5_n315887 Are both the phone and the coffee cup the same color? yes {'phone': [1], 'coffee cup': [2]}\n",
      "all_nodes [['phone', 'coffee cup']]\n",
      "========== Node question Generation ===========\n",
      "False\n",
      "node_question,posi_q {'phone': ['What color is the phone?'], 'coffee cup': ['What color is the coffee cup?']} {'phone': ['Is phone in the image?'], 'coffee cup': ['Is coffee cup in the image?']}\n",
      "========== Dict Generation ===========\n",
      "['Are both the phone and the coffee cup the same color?']\n",
      "new_obj_dict Thought: Since it says 'both the phone and the coffee cup', it implies a relationship between 'phone' and 'coffee cup': ('phone','coffee cup'). Answer: [('phone','coffee cup')]\n",
      "[('phone','coffee cup')]\n",
      "extracted_tuples [('phone', 'coffee cup')]\n",
      "=========generate edge============\n",
      "all_edgequestions edge [{('phone', 'coffee cup'): ['Are the phone and the coffee cup the same color?']}]\n",
      "all_relationship_dict:  [{'phone': ['coffee cup']}]\n",
      "combined_relation {'phone': ['coffee cup']}\n",
      "==========Generated graph==========\n",
      "[{'phone': {'node_question': ['Is phone in the image?', 'What color is the phone?'], 'edge': [{'coffee cup': {'edge_question': ['Are the phone and the coffee cup the same color?'], 'node_question': ['Is coffee cup in the image?', 'What color is the coffee cup?']}}]}}]\n",
      "Main_node: phone\n",
      "  Main_node Question: Is phone in the image?\n",
      "\n",
      "obj_dict to update the question {'phone': [1], 'coffee cup': [2]}\n",
      "updated_question  Is phone [1] in the image?\n",
      "label_sentence In the image, there are one phone marked by 1, and one coffee cup marked by 2.\n",
      "{'agent': {'next': 'judge_obj_tool'}}\n",
      "-----------------\n",
      "question\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/5_n315887_phone_merged_image.jpg\n",
      "{'judge_obj_tool': {'Tool_return': [FunctionMessage(content='{\"1\": [\"Yes\", \"large\"]}', name='judge_obj_tool')]}}\n",
      "-----------------\n",
      "ans {\"1\": [\"Yes\", \"large\"]}\n",
      "ans_dict {'1': ['Yes', 'large']}\n",
      "obj_dict after update {'phone': [1], 'coffee cup': [2]}\n",
      "obj_filt_dict after update {'phone': [[565.6508178710938, 276.0, 640.0, 382.1591491699219]], 'coffee cup': [[486.5869140625, 161.0, 568.4815673828125, 266.8819885253906]]}\n",
      "image_ori_dir /root/projects/mmcot/gqa/images/n315887.jpg\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/5_n315887_new.jpg\n",
      "img_dir /root/projects/code_lihan/experiment_questions_data/test_0-200/5_n315887_new.jpg\n",
      "  Main_node Question: What color is the phone?\n",
      "\n",
      "obj_dict to update the question {'phone': [1], 'coffee cup': [2]}\n",
      "updated_question  What color is the phone [1]?\n",
      "label_sentence In the image, there are one phone marked by 1, and one coffee cup marked by 2.\n",
      "{'agent': {'next': 'VQA_tool'}}\n",
      "-----------------\n",
      "What color is the phone [1] in the image?\n",
      "Complete result from VQA tool:  Thought: I need to find the phone labeled with 1.\n",
      "Answer: The phone [1] is black.\n",
      "---------------\n",
      "{'VQA_tool': {'Tool_return': [FunctionMessage(content='The phone [1] is black.', name='VQA_tool')]}}\n",
      "-----------------\n",
      "ans The phone [1] is black.\n",
      "obj_dict after update {'phone': [1]}\n",
      "  Edge to coffee cup\n",
      "    Edge Question: Are the phone and the coffee cup the same color?\n",
      "\n",
      "obj_dict to update the question {'phone': [1], 'coffee cup': [2]}\n",
      "updated_question  Are the phone [1] and the coffee cup [2] the same color?\n",
      "label_sentence In the image, there are one phone marked by 1, and one coffee cup marked by 2.\n",
      "{'agent': {'next': 'VQA_tool'}}\n",
      "-----------------\n",
      "Are the phone [1] and the coffee cup [2] the same color in the image?\n",
      "Complete result from VQA tool:  Thought: There is no existence of colors in the image description.\n",
      "Answer: No.\n",
      "---------------\n",
      "{'VQA_tool': {'Tool_return': [FunctionMessage(content='No.', name='VQA_tool')]}}\n",
      "-----------------\n",
      "obj_dict_outedge {'phone': [], 'coffee cup': []}\n",
      "obj_dict after update {'phone': [1], 'coffee cup': [2]}\n",
      "    Node Question at coffee cup: Is coffee cup in the image?\n",
      "\n",
      "obj_dict to update the question {'phone': [1], 'coffee cup': [2]}\n",
      "updated_question  Is coffee cup [2] in the image?\n",
      "label_sentence In the image, there are one phone marked by 1, and one coffee cup marked by 2.\n",
      "{'agent': {'next': 'judge_obj_tool'}}\n",
      "-----------------\n",
      "question\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/5_n315887_new_coffeecup_merged_image.jpg\n",
      "{'judge_obj_tool': {'Tool_return': [FunctionMessage(content='{\"2\": [\"Yes\", \"large\"]}', name='judge_obj_tool')]}}\n",
      "-----------------\n",
      "ans_subnode {\"2\": [\"Yes\", \"large\"]}\n",
      "/root/projects/code_lihan/experiment_questions_data/test_0-200/5_n315887_new_new.jpg\n",
      "    Node Question at coffee cup: What color is the coffee cup?\n",
      "\n",
      "obj_dict to update the question {'phone': [1], 'coffee cup': [2]}\n",
      "updated_question  What color is the coffee cup [2]?\n",
      "label_sentence In the image, there are one phone marked by 1, and one coffee cup marked by 2.\n",
      "{'agent': {'next': 'VQA_tool'}}\n",
      "-----------------\n",
      "What color is the coffee cup [2] in the image?\n",
      "Complete result from VQA tool:  Thought: I need to find the coffee cup labeled 2.\n",
      "Answer: The coffee cup [2] is black.\n",
      "---------------\n",
      "{'VQA_tool': {'Tool_return': [FunctionMessage(content='The coffee cup [2] is black.', name='VQA_tool')]}}\n",
      "-----------------\n",
      "ans_subnode The coffee cup [2] is black.\n",
      "obj_dict after update {'coffee cup': [2]}\n",
      "\n",
      "======ans_graph_entity===========\n",
      "{'phone': {'node_question': ['{\"1\": [\"Yes\", \"large\"]}', 'The phone [1] is black.'], 'edge': [{'coffee cup': {'edge_question': 'No.', 'node_question': []}}]}}\n",
      "\n",
      "=======All-answers===========\n",
      "[]\n",
      "The phone [1] is black. The coffee cup [2] is black.\n",
      "The phone [1] is black. The coffee cup [2] is black.\n",
      "\n",
      "=======The Final Answer===========\n",
      "The question is : Are both the phone and the coffee cup the same color?\n",
      "\n",
      "The context is:  The phone [1] is black. The coffee cup [2] is black.\n",
      "image_dir /root/projects/code_lihan/experiment_questions_data/test_0-200/5_n315887_new_new.jpg\n",
      "Are both the phone and the coffee cup the same color in the image?\n",
      "Contexto_to_gpt4: The context information is as follows: <The phone [1] is black. The coffee cup [2] is black.>. My VQA question is: Are both the phone and the coffee cup the same color in the image?\n",
      "{\"status_code\": 200, \"request_id\": \"9666d237-b8c1-9a72-b8ff-f2c3c7bef93b\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": [{\"text\": \"yes\"}]}}]}, \"usage\": {\"input_tokens\": 687, \"output_tokens\": 2, \"image_tokens\": 345}}\n",
      "\n",
      "The final answer is:  yes\n",
      "logit yes\n",
      "modify ans yes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/root/projects/code_lihan/')\n",
    "import importlib\n",
    "import json\n",
    "import tools\n",
    "import traverse_node_edge\n",
    "import graph_generation\n",
    "import langgraph_process\n",
    "# import gpt4_img\n",
    "import gpt4_img_qwen\n",
    "import re\n",
    "import string\n",
    "# importlib.reload(gpt4_img)\n",
    "importlib.reload(gpt4_img_qwen)\n",
    "importlib.reload(traverse_node_edge)\n",
    "importlib.reload(graph_generation)\n",
    "importlib.reload(tools)\n",
    "importlib.reload(langgraph_process)\n",
    "import os\n",
    "os.environ[\"DASHSCOPE_API_KEY\"]= 'sk-ac7aca0206ae4da9a517628e5fa2170f'\n",
    "\n",
    "\n",
    "# questions=['Are there napkins under the utensil to the left of the rice?']\n",
    "img_names=[]\n",
    "for imageId in imageIds:\n",
    "    img_names.append(f'/root/projects/code_lihan/experiment_questions_data/test_{num}-{num+200}/'+imageId+'.jpg')\n",
    "# obj_dicts=[{'napkins': [1], 'utensil': [2, 3], 'rice': [4]}]\n",
    "cnt=0\n",
    "collection=[]\n",
    "for imageId,question,img_name,label,obj_dict,axis_dict,obj_not_found,obj_filt_dict in zip(imageIds,questions,img_names,answers,obj_dicts,axis_dicts,obj_not_founds,obj_filt_dicts):\n",
    "    if cnt<0:\n",
    "        cnt+=1\n",
    "        continue \n",
    "    question=question[0]+question[1:].lower()\n",
    "    question_words=question[:-1].split(' ')\n",
    "    # if 'Who' in question_words :\n",
    "    #     # cnt+=1\n",
    "    #     continue\n",
    "    # if 'big' not in question_words :\n",
    "    #     # cnt+=1\n",
    "    #     continue\n",
    "    question = re.sub(r'walking\\b', 'standing', question)\n",
    "    question = re.sub(r'walk\\b', 'stand', question)\n",
    "    question = re.sub(r'Where\\b', 'What place', question)\n",
    "    question = re.sub(r'name\\b', 'type', question)\n",
    "    \n",
    "    cnt+=1\n",
    "    to_remove=[]\n",
    "    for k,v in obj_filt_dict.items():\n",
    "        if k not in obj_dict:\n",
    "            to_remove.append(k)\n",
    "    for k in to_remove:\n",
    "        del obj_filt_dict[k]\n",
    "    # if 'left' not in question and 'right' not in question:\n",
    "    #     continue\n",
    "    print(\"\\n\")\n",
    "    print(f\"The {cnt} sample:\")\n",
    "    print(imageId,question,label,obj_dict)\n",
    "    Questions=[question]\n",
    "    Img_names=[img_name]\n",
    "    Obj_dicts=[obj_dict]\n",
    "    Axis_dicts=[axis_dict]\n",
    "    Obj_not_found=[obj_not_founds]\n",
    "    Obj_filt_dict=[obj_filt_dict]\n",
    "    coll_dict={}\n",
    "\n",
    "    entity_list=[]\n",
    "    for obj_dict in Obj_dicts:\n",
    "        entity_list.append(list(obj_dict.keys()))\n",
    "    # print('obj_filt_dict',obj_filt_dict)\n",
    "    # print('obj_dict',obj_dict)\n",
    "    # payload = create_payload_final_ans( img_name, question,'','')\n",
    "    # response = query_openai(payload)\n",
    "    # # print(question)\n",
    "    # logit=response['choices'][0]['message']['content']\n",
    "    # print(response['choices'][0]['message']['content'])\n",
    "    # print('answer',label)\n",
    "    logit=traverse_node_edge.main(Questions,Img_names,Obj_dicts,Axis_dicts,entity_list,Obj_not_found,Obj_filt_dict)\n",
    "    # logit=graph_generation.main(Questions,entity_list)\n",
    "    coll_dict['id']=cnt\n",
    "    coll_dict['Label']=label\n",
    "    coll_dict['Logit']=logit\n",
    "    coll_dict['Q']=question\n",
    "    coll_dict['img_name']=img_name\n",
    "    collection.append(coll_dict)\n",
    "    if cnt==1:\n",
    "        print(collection)\n",
    "    # if cnt%80==0:\n",
    "    #     with open(\"all_1000-1200_collection.json\",\"a\") as f:\n",
    "    #         json.dump(collection,f)\n",
    "    if cnt==6:\n",
    "        break\n",
    "# with open('obj_filt_dicts_who.json','r') as f:\n",
    "#     obj_filt_dicts=json.load(f)\n",
    "# with open('obj_dicts_who.json','r') as f:\n",
    "#     obj_dicts=json.load(f)\n",
    "\n",
    "    # if cnt==18:\n",
    "        # break\n",
    "# with open('obj_dicts_100-200.json','r') as f:\n",
    "# with open('obj_dicts_0-100.json','r') as f:\n",
    "#     obj_dicts=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/root/projects/code_lihan/')\n",
    "# import importlib\n",
    "# import json\n",
    "# import tools\n",
    "# import traverse_node_edge\n",
    "# import graph_generation\n",
    "# import langgraph_process\n",
    "# import gpt4_img\n",
    "# import re\n",
    "# import string\n",
    "# importlib.reload(gpt4_img)\n",
    "# importlib.reload(traverse_node_edge)\n",
    "# importlib.reload(graph_generation)\n",
    "# importlib.reload(tools)\n",
    "# importlib.reload(langgraph_process)\n",
    "\n",
    "\n",
    "\n",
    "# # questions=['Are there napkins under the utensil to the left of the rice?']\n",
    "# img_names=[]\n",
    "# for imageId in imageIds:\n",
    "#     img_names.append(f'/root/projects/code_lihan/experiment_questions_data/test_{num}-{num+200}/'+imageId+'.jpg')\n",
    "# # obj_dicts=[{'napkins': [1], 'utensil': [2, 3], 'rice': [4]}]\n",
    "# cnt=0\n",
    "# collection=[]\n",
    "# for imageId,question,img_name,label,obj_dict,axis_dict,obj_not_found,obj_filt_dict in zip(imageIds,questions,img_names,answers,obj_dicts,axis_dicts,obj_not_founds,obj_filt_dicts):\n",
    "#     if cnt<43:\n",
    "#         cnt+=1\n",
    "#         continue \n",
    "#     question=question[0]+question[1:].lower()\n",
    "#     question_words=question[:-1].split(' ')\n",
    "#     # if 'Who' in question_words :\n",
    "#     #     # cnt+=1\n",
    "#     #     continue\n",
    "#     # if 'big' not in question_words :\n",
    "#     #     # cnt+=1\n",
    "#     #     continue\n",
    "#     question = re.sub(r'walking\\b', 'standing', question)\n",
    "#     question = re.sub(r'walk\\b', 'stand', question)\n",
    "#     question = re.sub(r'Where\\b', 'What place', question)\n",
    "#     question = re.sub(r'name\\b', 'type', question)\n",
    "    \n",
    "#     cnt+=1\n",
    "#     to_remove=[]\n",
    "#     for k,v in obj_filt_dict.items():\n",
    "#         if k not in obj_dict:\n",
    "#             to_remove.append(k)\n",
    "#     for k in to_remove:\n",
    "#         del obj_filt_dict[k]\n",
    "#     # if 'left' not in question and 'right' not in question:\n",
    "#     #     continue\n",
    "#     print(\"\\n\")\n",
    "#     print(f\"The {cnt} sample:\")\n",
    "#     print(imageId,question,label,obj_dict)\n",
    "#     Questions=[question]\n",
    "#     Img_names=[img_name]\n",
    "#     Obj_dicts=[obj_dict]\n",
    "#     Axis_dicts=[axis_dict]\n",
    "#     Obj_not_found=[obj_not_founds]\n",
    "#     Obj_filt_dict=[obj_filt_dict]\n",
    "#     coll_dict={}\n",
    "\n",
    "#     entity_list=[]\n",
    "#     for obj_dict in Obj_dicts:\n",
    "#         entity_list.append(list(obj_dict.keys()))\n",
    "#     # print('obj_filt_dict',obj_filt_dict)\n",
    "#     # print('obj_dict',obj_dict)\n",
    "#     # payload = create_payload_final_ans( img_name, question,'','')\n",
    "#     # response = query_openai(payload)\n",
    "#     # # print(question)\n",
    "#     # logit=response['choices'][0]['message']['content']\n",
    "#     # print(response['choices'][0]['message']['content'])\n",
    "#     # print('answer',label)\n",
    "#     logit=traverse_node_edge.main(Questions,Img_names,Obj_dicts,Axis_dicts,entity_list,Obj_not_found,Obj_filt_dict)\n",
    "#     # logit=graph_generation.main(Questions,entity_list)\n",
    "#     coll_dict['id']=cnt\n",
    "#     coll_dict['Label']=label\n",
    "#     coll_dict['Logit']=logit\n",
    "#     coll_dict['Q']=question\n",
    "#     coll_dict['img_name']=img_name\n",
    "#     collection.append(coll_dict)\n",
    "#     if cnt==1:\n",
    "#         print(collection)\n",
    "#     # if cnt%80==0:\n",
    "#     #     with open(\"all_1000-1200_collection.json\",\"a\") as f:\n",
    "#     #         json.dump(collection,f)\n",
    "#     break\n",
    "# # with open('obj_filt_dicts_who.json','r') as f:\n",
    "# #     obj_filt_dicts=json.load(f)\n",
    "# # with open('obj_dicts_who.json','r') as f:\n",
    "# #     obj_dicts=json.load(f)\n",
    "\n",
    "#     # if cnt==18:\n",
    "#         # break\n",
    "# # with open('obj_dicts_100-200.json','r') as f:\n",
    "# # with open('obj_dicts_0-100.json','r') as f:\n",
    "# #     obj_dicts=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"qwen_collection.json\",\"a\") as f:\n",
    "    json.dump(collection,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "with open(\"qwen_collection.json\",\"rb\") as f:\n",
    "    collections=json.load(f)\n",
    "cnt=0\n",
    "nums=[]\n",
    "cnt1=0\n",
    "for c in collections:\n",
    "    # if 'what' in c['Q'] or 'What' in c['Q']: continue\n",
    "    # cnt1+=1\n",
    "    if c['Logit']==c['Label']:cnt+=1\n",
    "    else:\n",
    "        if bool(re.search(r'\\d', c['Logit'])): nums.append(c['id'])\n",
    "        # print('Logit: ',c['Logit'])\n",
    "        # print('Label: ',c['Label'])\n",
    "print(cnt)\n",
    "print(cnt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "189\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "with open(\"test_0-200_collection.json\",\"rb\") as f:\n",
    "    collections=json.load(f)\n",
    "cnt=0\n",
    "nums=[]\n",
    "cnt1=0\n",
    "for c in collections:\n",
    "    # if 'what' in c['Q'] or 'What' in c['Q']: \n",
    "    #     print(c['Q'])\n",
    "    #     continue\n",
    "    cnt1+=1\n",
    "    if c['Logit']==c['Label']:cnt+=1\n",
    "    else:\n",
    "        if bool(re.search(r'\\d', c['Logit'])): nums.append(c['id'])\n",
    "        # print('Logit: ',c['Logit'])\n",
    "        # print('Label: ',c['Label'])\n",
    "print(cnt)\n",
    "print(cnt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"exwho_300-600_collection.json\",\"a\") as f:\n",
    "    json.dump(collection,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/root/projects/code_lihan/')\n",
    "import importlib\n",
    "import gpt4_img\n",
    "importlib.reload(gpt4_img)\n",
    "\n",
    "from gpt4_img import create_payload_normal,query_openai,create_payload_final_ans,create_payload_edge,create_payload_node\n",
    "# for i in range(len(imageIds)):\n",
    "    # imageId=str(imageIds[i])\n",
    "    # question=questions[i]\n",
    "    # answer=answers[i]\n",
    "# The white lamp is marked as 1, and the white curtain is marked as 2.\n",
    "\n",
    "payload = create_payload_final_ans( \"/root/projects/code_lihan/experiment_questions_data/test_300-600_exWho/125_2363769.jpg\", \"What is the size of the book above the teddy bear, compared with a usual book? Answer with large or small.\",'','')\n",
    "response = query_openai(payload)\n",
    "# print(question)\n",
    "print(response['choices'][0]['message']['content'])\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for imageId,question,img_name,label,obj_dict,axis_dict,obj_not_found,obj_filt_dict in zip(imageIds,questions,img_names,answers,obj_dicts,axis_dicts,obj_not_founds,obj_filt_dicts):\n",
    "#     if cnt<0: \n",
    "#         cnt+=1\n",
    "#         continue \n",
    "#     question_words=question[:-1].split(' ')\n",
    "#     if 'there' not in question_words and 'see' not in question_words:\n",
    "#         cnt+=1\n",
    "#         continue\n",
    "#     # if cnt==34: \n",
    "#     #     cnt+=1\n",
    "#     #     continue\n",
    "#     cnt+=1\n",
    "#     imageId=imageId.split('_')[1]\n",
    "#     print(f\"/root/projects/mmcot/gqa/images/{imageId}.jpg\")\n",
    "#     payload = create_payload_final_ans( f\"/root/projects/mmcot/gqa/images/{imageId}.jpg\", question,'','')\n",
    "#     response = query_openai(payload)\n",
    "#     # print(question)\n",
    "#     print(response['choices'][0]['message']['content'])\n",
    "#     print('answer',label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
